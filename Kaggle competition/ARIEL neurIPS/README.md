# Ariel Exoplanet Atmospheric Retrieval

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Dataset on HuggingFace](https://img.shields.io/badge/ğŸ¤—%20Dataset-alexy--louis%2Fariel--exoplanet--2024-blue)](https://huggingface.co/datasets/alexy-louis/ariel-exoplanet-2024)
[![Kaggle Competition](https://img.shields.io/badge/Kaggle-NeurIPS%202024%20Ariel-20BEFF?logo=kaggle)](https://www.kaggle.com/competitions/ariel-data-challenge-2024)

End-to-end deep learning pipeline for the **NeurIPS 2024 Ariel Data Challenge** â€” extracting exoplanet atmospheric transmission spectra from simulated ESA Ariel telescope photometry.

---

## For ML Practitioners

### Quick Start

```bash
git clone https://github.com/Smooth-Cactus0/ariel-exoplanet-ml.git
cd ariel-exoplanet-ml
pip install -r requirements.txt
```

> **Data lives on Kaggle** (~180 GB). See [data/download.sh](data/download.sh) for download instructions, or work directly in a [Kaggle notebook](https://www.kaggle.com/competitions/ariel-data-challenge-2024) with the dataset pre-attached.

### Data Format

Each planet is stored as a directory of parquet files:

```
{split}/{planet_id}/
    AIRS-CH0_signal.parquet          (11250, 32Ã—356) uint16
    FGS1_signal.parquet              (135000, 32Ã—32) uint16
    AIRS-CH0_calibration/
        dark.parquet  flat.parquet  dead.parquet  read.parquet  linear_corr.parquet
    FGS1_calibration/
        dark.parquet  flat.parquet  dead.parquet  read.parquet  linear_corr.parquet
```

| Root-level file | Description | Shape |
|-----------------|-------------|-------|
| `train_labels.csv` | Ground-truth transmission spectra (means only) | `planet_id \| wl_1 \| ... \| wl_283` (~24% of planets) |
| `train_adc_info.csv` | 5 ADC calibration features per planet | `planet_id \| FGS1_adc_offset \| FGS1_adc_gain \| AIRS-CH0_adc_offset \| AIRS-CH0_adc_gain \| star` |
| `wavelengths.csv` | 283 output wavelength values in microns | `wl_1=0.705 Âµm (FGS1), wl_2â€“wl_283 = AIRS bins` |
| `axis_info.parquet` | Time/wavelength axis metadata | `(135000, 4)` |

### Model Architecture: TransitCNN

```
AIRS-CH0 (356, time) â”€â”€â–º TemporalEncoder â”€â”€â–º z_airs (128,)  â”
FGS1      (  1, time) â”€â”€â–º TemporalEncoder â”€â”€â–º z_fgs1 ( 32,)  â”œâ”€â”€â–º Fusion MLP â”€â”€â–º mean (283,)
aux        (       5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â–º log_var (283,)
```

- **TemporalEncoder**: 3-layer Conv1D â†’ GELU â†’ AdaptiveAvgPool1d(1)
- **Fusion MLP**: Linear(165â†’512â†’512â†’256) with GELU + dropout
- **Loss**: Gaussian NLL: `0.5 * (log_var + (y âˆ’ Î¼)Â² / exp(log_var))`
- **Output**: per-wavelength mean + calibrated uncertainty for 283 wavelength bins

### Preprocessing Pipeline

```
Raw AIRS (time Ã— 356)
  â”‚
  â–¼ baseline_normalize()     â€” divide each channel by its OOT median
  â–¼ common_mode_correction()  â€” subtract correlated stellar/instrumental variability
  â–¼ bin_time(bin_size=5)      â€” temporal binning: SNR âˆ âˆšbin_size
  â”‚
  â””â”€â”€â–º transit_depth per channel = 1 âˆ’ (mean in-transit / OOT) Â± noise
```

See [src/preprocessing.py](src/preprocessing.py) for implementation and [notebooks/02_preprocessing.ipynb](notebooks/02_preprocessing.ipynb) for a step-by-step visual walkthrough.

### Reproduction Steps (Kaggle)

1. **Clone repo inside Kaggle notebook** (Internet enabled):
   ```python
   import subprocess, sys
   subprocess.run(["git", "clone",
       "https://github.com/Smooth-Cactus0/ariel-exoplanet-ml.git",
       "/kaggle/working/ariel-exoplanet-ml"])
   sys.path.insert(0, "/kaggle/working/ariel-exoplanet-ml")
   ```

2. **Run EDA**: [notebooks/01_eda.ipynb](notebooks/01_eda.ipynb)

3. **Train baseline** (CPU, ~2 min): [notebooks/03_baseline.ipynb](notebooks/03_baseline.ipynb)

4. **Train TransitCNN** (GPU T4/P100, ~45 min): [notebooks/04_deep_learning.ipynb](notebooks/04_deep_learning.ipynb)

5. **Generate submission**: notebook 04 writes `submission.csv` automatically.

### Exploratory Data Analysis

Figures generated by [notebooks/01_eda.ipynb](notebooks/01_eda.ipynb) on the full competition dataset:

#### Calibrated Light Curves
AIRS-CH0 white-light curve (mean across 356 spectral channels) and FGS1 broadband photometry (12:1 downsampled to AIRS cadence). The transit dip is clearly visible in both instruments.

![Light Curves](figures/light_curves.png)

#### AIRS-CH0 Spectral Heatmap
2-D flux matrix (time Ã— 356 spectral channels) showing wavelength-dependent transit depth and instrumental systematics.

![AIRS Heatmap](figures/airs_heatmap.png)

#### ADC Feature Distributions
Histograms of the 5 ADC calibration features from `train_adc_info.csv` with median annotations.

![ADC Distributions](figures/adc_distributions.png)

#### Transit Depth Distribution
Distribution of median transit depth (across 283 wavelength bins) for all labelled planets. Annotated with median and Â±1Ïƒ.

![Transit Depth Distribution](figures/transit_depth_distribution.png)

#### Label Correlation Heatmap
Pearson correlation of mean transit depth across subsampled wavelength channels. Adjacent channels are highly correlated; decorrelation occurs across molecular absorption band boundaries.

![Label Correlation Heatmap](figures/label_correlation_heatmap.png)

### Results

| Method | Val GLL | Notes |
|--------|---------|-------|
| Constant median predictor | â€” | Run baseline notebook to populate |
| Ridge regression on aux | â€” | Run baseline notebook to populate |
| TransitCNN (this repo) | â€” | Run DL notebook to populate |

*GLL = Gaussian Log-Likelihood. Higher is better; 0 = perfect; competition normalizes against a reference baseline.*

### Repository Structure

```
ariel-exoplanet-ml/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py   # Signal processing pipeline (6 functions, 9 tests)
â”‚   â”œâ”€â”€ dataset.py          # PyTorch Dataset wrapping parquet + CSV data
â”‚   â”œâ”€â”€ model.py            # TransitCNN + gaussian_nll_loss
â”‚   â”œâ”€â”€ train.py            # Training loop (AdamW + CosineAnnealingLR)
â”‚   â””â”€â”€ evaluate.py         # Inference, GLL metric, submission CSV
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb                # Exploratory data analysis
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb      # Preprocessing walkthrough
â”‚   â”œâ”€â”€ 03_baseline.ipynb           # Statistical baselines
â”‚   â”œâ”€â”€ 04_deep_learning.ipynb      # TransitCNN training + submission
â”‚   â””â”€â”€ 05_huggingface_upload.ipynb # Preprocess + push to HF Hub
â”œâ”€â”€ figures/                 # EDA plots (generated on Kaggle)
â”œâ”€â”€ hf_dataset/
â”‚   â””â”€â”€ ariel_dataset.py   # HuggingFace Datasets loading script
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ explore_data.py    # One-shot data structure discovery
â”œâ”€â”€ tests/                 # 26 unit tests (pytest)
â”œâ”€â”€ data/download.sh       # Kaggle API download script
â””â”€â”€ requirements.txt
```

### Running Tests (Local)

```bash
# Requires Python 3.9+ with numpy, torch, pandas, scipy
pytest tests/ -v
```

All 26 tests pass without competition data â€” they use synthetic fixtures.

---

## For Astronomers

### The Science: What Is Atmospheric Retrieval?

When an exoplanet transits (passes in front of) its host star, the planet's atmosphere absorbs some starlight at specific wavelengths. By measuring how much the star dims as a function of wavelength, we can infer the composition of the planet's atmosphere â€” this is the **transmission spectrum**.

```
        Star
      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ
   â–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆ
   â–ˆâ–ˆ  â—‹ Planet  â–ˆâ–ˆ     â† Planet + atmosphere blocks star
   â–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆ       at wavelengths absorbed by Hâ‚‚O, COâ‚‚, CHâ‚„, etc.
    â–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ
      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

The **transit depth** at wavelength Î» is:

```
D(Î») = [R_p(Î») / R_*]Â²
```

where `R_p(Î»)` is the effective planetary radius including atmosphere. Larger depth = more absorption at that wavelength.

### The Ariel Mission

[ESA Ariel](https://www.ariel-spacemission.eu/) (Atmospheric Remote-sensing Infrared Exoplanet Large-survey), launching 2029, will characterize atmospheres of ~1000 exoplanets using:
- **AIRS-CH0**: IR spectrometer, 1.95â€“3.90 Âµm â€” sensitive to Hâ‚‚O, COâ‚‚, CHâ‚„, CO
- **FGS1**: Visible photometer, 0.60â€“0.80 Âµm â€” photometric reference channel

This competition uses simulated Ariel data to benchmark atmospheric retrieval algorithms ahead of launch.

### Systematic Effects We Correct For

Real space photometry is corrupted by:

1. **Baseline drift**: stellar variability, pointing jitter, thermal settling. We normalize each wavelength channel by its out-of-transit (OOT) median.

2. **Common-mode systematics**: correlated noise from the detector or stellar limb darkening variability. We compute the mean light curve across all wavelength channels and divide it out â€” but *only using OOT frames* to avoid erasing the transit signal.

3. **Photon noise**: random photon-counting noise. We bin consecutive time steps (5Ã— binning â‰ˆ âˆš5 â‰ˆ 2.2Ã— noise reduction).

### How to Apply This to Your Own Transit Data

The preprocessing pipeline is instrument-agnostic. For any transit photometry dataset:

```python
from src.preprocessing import preprocess_planet
import numpy as np

# airs: numpy array of shape (n_timesteps, n_wavelengths)
# fgs1: numpy array of shape (n_timesteps,) â€” photometric reference
airs = np.load("your_airs_data.npy")
fgs1 = np.load("your_fgs1_data.npy")

result = preprocess_planet(
    airs, fgs1,
    ingress=0.2,    # fraction of obs where transit starts
    egress=0.8,     # fraction of obs where transit ends
    bin_size=5,
)

print(result["transit_depth"])      # (n_wavelengths,) â€” transmission spectrum
print(result["transit_depth_err"])  # (n_wavelengths,) â€” 1Ïƒ uncertainty
```

The ingress/egress fractions can be derived from orbital parameters:
```
ingress â‰ˆ 0.5 - T14 / (2 * T_obs)
egress  â‰ˆ 0.5 + T14 / (2 * T_obs)
```
where `T14` is the transit duration (first to fourth contact) and `T_obs` is the total observation length.

### Preprocessed Dataset on HuggingFace

A preprocessed version of the competition data (normalized, binned, transit depths extracted) is available on HuggingFace Hub:

```python
from datasets import load_dataset
ds = load_dataset("alexy-louis/ariel-exoplanet-2024", split="train")

sample = ds[0]
print(sample["transit_depth"])     # 356-element float32 array
print(sample["aux"])               # [FGS1_adc_offset, FGS1_adc_gain, ...]
```

---

## Citation

If you use this code or dataset, please cite the original competition:

```bibtex
@misc{ariel2024,
  title  = {NeurIPS Ariel Data Challenge 2024},
  author = {Ariel Data Challenge Team},
  year   = {2024},
  url    = {https://www.kaggle.com/competitions/ariel-data-challenge-2024}
}
```

---

## License

MIT â€” see [LICENSE](LICENSE).

*Built by [Alexy Louis](https://github.com/Smooth-Cactus0) as part of an ML engineering portfolio.*
