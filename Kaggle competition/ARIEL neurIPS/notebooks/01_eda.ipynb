{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS 2024 Ariel Data Challenge — Exploratory Data Analysis\n",
    "\n",
    "**Goal**: Understand the structure, distributions, and quality of the Ariel exoplanet atmospheric spectra dataset.  \n",
    "**Dataset**: ~180 GB of simulated telescope photometry from AIRS-CH0 (IR spectrometer) and FGS1 (visible photometer).  \n",
    "**Task**: Extract exoplanet atmospheric transmission spectra (283 wavelength channels) from transit light curves.  \n",
    "**Scoring**: Gaussian Log-Likelihood over predicted mean and std per wavelength.\n",
    "\n",
    "**Data format**: Nested directories with parquet files per planet, plus CSV/parquet metadata.  \n",
    "Detector geometry: AIRS = 32 spatial rows x 356 spectral channels, FGS1 = 32x32, FGS1 runs at 12x AIRS cadence.\n",
    "\n",
    "> **Note**: This notebook is Kaggle-ready and requires the `ariel-data-challenge-2024` dataset attached to the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing packages\n",
    "import subprocess, sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package}: already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"{package}: not found — installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\"{package}: installed successfully\")\n",
    "\n",
    "install_if_missing(\"pyarrow\")\n",
    "install_if_missing(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport seaborn as sns\nimport pyarrow.parquet as pq\n\n# ── Data root ──────────────────────────────────────────────────────────────\nDATA_ROOT = Path(\"/kaggle/input/ariel-data-challenge-2024\")\n\n# ── Figure export directory ────────────────────────────────────────────────\n# Figures are saved here so they can be displayed in the GitHub README.\n# On Kaggle, /kaggle/working/ is the persistent output directory.\nFIG_DIR = Path(\"/kaggle/working/figures\")\nFIG_DIR.mkdir(parents=True, exist_ok=True)\nprint(f\"Figures will be saved to: {FIG_DIR}\")\n\n# ── Plot style ─────────────────────────────────────────────────────────────\nplt.rcParams.update({\n    \"figure.dpi\": 120,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"savefig.dpi\": 150,\n    \"savefig.bbox\": \"tight\",\n    \"savefig.facecolor\": \"white\",\n})\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\n\n# ── Version report ─────────────────────────────────────────────────────────\nprint(f\"Python      : {sys.version}\")\nprint(f\"NumPy       : {np.__version__}\")\nprint(f\"Pandas      : {pd.__version__}\")\nprint(f\"Matplotlib  : {matplotlib.__version__}\")\nprint(f\"Seaborn     : {sns.__version__}\")\nprint(f\"PyArrow     : {pq.lib.version()}\")\nprint(f\"\\nDATA_ROOT   : {DATA_ROOT}\")\nprint(f\"Exists      : {DATA_ROOT.exists()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk the data root and report file sizes\n",
    "total_bytes = 0\n",
    "file_records = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(DATA_ROOT):\n",
    "    # Skip hidden directories\n",
    "    dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n",
    "    for fname in sorted(filenames):\n",
    "        fpath = Path(dirpath) / fname\n",
    "        try:\n",
    "            size_bytes = fpath.stat().st_size\n",
    "        except OSError:\n",
    "            size_bytes = 0\n",
    "        total_bytes += size_bytes\n",
    "        rel_path = fpath.relative_to(DATA_ROOT)\n",
    "        file_records.append({\n",
    "            \"path\": str(rel_path),\n",
    "            \"size_MB\": round(size_bytes / 1_048_576, 2),\n",
    "        })\n",
    "\n",
    "df_files = pd.DataFrame(file_records).sort_values(\"size_MB\", ascending=False).reset_index(drop=True)\n",
    "print(f\"Total files : {len(df_files)}\")\n",
    "print(f\"Total size  : {total_bytes / 1_073_741_824:.2f} GB\\n\")\n",
    "print(df_files.head(30).to_string(index=False))\n",
    "\n",
    "print(f\"\\n[Summary] Found {len(df_files)} files totalling {total_bytes / 1_073_741_824:.2f} GB on disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CSV / Metadata Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── train_adc_info.csv ─────────────────────────────────────────────────────\n",
    "# Columns: planet_id | FGS1_adc_offset | FGS1_adc_gain | AIRS-CH0_adc_offset | AIRS-CH0_adc_gain | star\n",
    "adc_path = DATA_ROOT / \"train_adc_info.csv\"\n",
    "df_adc = pd.read_csv(adc_path)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"train_adc_info.csv\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df_adc.shape}  ({df_adc.shape[0]} planets, {df_adc.shape[1]} columns)\")\n",
    "print(f\"Columns: {df_adc.columns.tolist()}\")\n",
    "print(\"\\n--- Head ---\")\n",
    "display(df_adc.head())\n",
    "\n",
    "print(\"\\n--- Data types ---\")\n",
    "print(df_adc.dtypes.to_string())\n",
    "\n",
    "print(\"\\n--- Missing value counts ---\")\n",
    "missing = df_adc.isnull().sum()\n",
    "print(missing[missing > 0].to_string() if missing.any() else \"No missing values detected.\")\n",
    "\n",
    "print(\"\\n--- Descriptive statistics ---\")\n",
    "display(df_adc.describe())\n",
    "\n",
    "print(f\"\\n[Summary] train_adc_info.csv has {df_adc.shape[0]} planets and {df_adc.shape[1]} columns \"\n",
    "      f\"(5 ADC features + planet_id) with {df_adc.isnull().sum().sum()} total missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── train_labels.csv ───────────────────────────────────────────────────────\n",
    "# Columns: planet_id | wl_1 | wl_2 | ... | wl_283 (means only, no quartile/sigma columns)\n",
    "labels_path = DATA_ROOT / \"train_labels.csv\"\n",
    "df_labels = pd.read_csv(labels_path)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"train_labels.csv\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df_labels.shape}\")\n",
    "print(f\"\\n--- First 6 columns ---\")\n",
    "display(df_labels.iloc[:, :6].head(10))\n",
    "print(f\"\\n--- Column name pattern ---\")\n",
    "print(df_labels.columns[:10].tolist(), \"...\")\n",
    "\n",
    "# Labelled vs unlabelled planets\n",
    "labelled_ids = set(df_labels.iloc[:, 0].unique()) if df_labels.shape[0] > 0 else set()\n",
    "all_ids = set(df_adc.iloc[:, 0].unique())\n",
    "n_labelled = len(labelled_ids)\n",
    "n_total = len(all_ids)\n",
    "n_unlabelled = n_total - n_labelled\n",
    "\n",
    "print(\"\\n--- Labelled vs Unlabelled ---\")\n",
    "print(f\"Total planets (train_adc_info)  : {n_total:>6}\")\n",
    "print(f\"Labelled planets (train_labels) : {n_labelled:>6}  ({100 * n_labelled / n_total:.1f}%)\")\n",
    "print(f\"Unlabelled planets              : {n_unlabelled:>6}  ({100 * n_unlabelled / n_total:.1f}%)\")\n",
    "\n",
    "print(f\"\\n[Summary] {n_labelled} of {n_total} planets ({100 * n_labelled / n_total:.1f}%) have \"\n",
    "      f\"labels (wl_1...wl_283 means); {n_unlabelled} are unlabelled (test set).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── wavelengths.csv ────────────────────────────────────────────────────────\n",
    "# (1, 283) row: wl_1=0.705 um (FGS1), wl_2-wl_283 = 282 AIRS spectral bins\n",
    "wl_path = DATA_ROOT / \"wavelengths.csv\"\n",
    "df_wl = pd.read_csv(wl_path)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"wavelengths.csv\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df_wl.shape}\")\n",
    "display(df_wl)\n",
    "\n",
    "wl_values = df_wl.values.flatten()\n",
    "print(f\"\\nWavelength range: {wl_values.min():.4f} um to {wl_values.max():.4f} um\")\n",
    "print(f\"FGS1 channel (wl_1): {wl_values[0]:.4f} um\")\n",
    "print(f\"AIRS channels (wl_2 - wl_283): {wl_values[1]:.4f} um to {wl_values[-1]:.4f} um\")\n",
    "\n",
    "print(f\"\\n[Summary] 283 wavelength channels: 1 FGS1 ({wl_values[0]:.3f} um) + \"\n",
    "      f\"282 AIRS ({wl_values[1]:.3f} - {wl_values[-1]:.3f} um).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── axis_info.parquet ──────────────────────────────────────────────────────\n",
    "# (135000, 4) time/wavelength metadata\n",
    "axis_path = DATA_ROOT / \"axis_info.parquet\"\n",
    "if axis_path.exists():\n",
    "    df_axis = pd.read_parquet(axis_path)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"axis_info.parquet\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Shape: {df_axis.shape}\")\n",
    "    print(f\"Columns: {df_axis.columns.tolist()}\")\n",
    "    print(f\"Dtypes:\\n{df_axis.dtypes.to_string()}\")\n",
    "    print(\"\\n--- Head ---\")\n",
    "    display(df_axis.head(10))\n",
    "    print(\"\\n--- Descriptive statistics ---\")\n",
    "    display(df_axis.describe())\n",
    "    print(f\"\\n[Summary] axis_info.parquet has {df_axis.shape[0]} rows and {df_axis.shape[1]} columns.\")\n",
    "else:\n",
    "    print(\"axis_info.parquet not found at expected location.\")\n",
    "    df_axis = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Directory Structure\n",
    "\n",
    "Each planet has its own directory containing parquet files for signal data and calibration frames:\n",
    "```\n",
    "{data_root}/{split}/{planet_id}/\n",
    "    AIRS-CH0_signal.parquet      (11250, 32*356) uint16\n",
    "    FGS1_signal.parquet          (135000, 32*32) uint16\n",
    "    AIRS-CH0_calibration/\n",
    "        dark.parquet  flat.parquet  dead.parquet  read.parquet  linear_corr.parquet\n",
    "    FGS1_calibration/\n",
    "        dark.parquet  flat.parquet  dead.parquet  read.parquet  linear_corr.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── List planet directories ────────────────────────────────────────────────\n",
    "# Planets live under {DATA_ROOT}/train/ and {DATA_ROOT}/test/\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    split_dir = DATA_ROOT / split\n",
    "    if not split_dir.exists():\n",
    "        print(f\"{split}/ directory not found.\")\n",
    "        continue\n",
    "    planet_dirs = sorted([d.name for d in split_dir.iterdir() if d.is_dir()])\n",
    "    print(f\"{split}/ : {len(planet_dirs)} planet directories\")\n",
    "    print(f\"  First 5: {planet_dirs[:5]}\")\n",
    "    print(f\"  Last  5: {planet_dirs[-5:]}\")\n",
    "    print()\n",
    "\n",
    "# ── Show contents of one example planet directory ──────────────────────────\n",
    "train_dir = DATA_ROOT / \"train\"\n",
    "if train_dir.exists():\n",
    "    planet_dirs_all = sorted([d for d in train_dir.iterdir() if d.is_dir()])\n",
    "    example_planet_dir = planet_dirs_all[0]\n",
    "    example_planet_id = example_planet_dir.name\n",
    "\n",
    "    print(f\"Contents of example planet directory: {example_planet_id}/\")\n",
    "    print(\"-\" * 60)\n",
    "    for item in sorted(example_planet_dir.rglob(\"*\")):\n",
    "        rel = item.relative_to(example_planet_dir)\n",
    "        if item.is_file():\n",
    "            size_kb = item.stat().st_size / 1024\n",
    "            print(f\"  {rel}  ({size_kb:.1f} KB)\")\n",
    "        elif item.is_dir():\n",
    "            print(f\"  {rel}/\")\n",
    "\n",
    "    print(f\"\\n[Summary] Example planet '{example_planet_id}' directory listed above. \"\n",
    "          f\"Total planets in train/: {len(planet_dirs_all)}.\")\n",
    "else:\n",
    "    example_planet_id = None\n",
    "    example_planet_dir = None\n",
    "    print(\"WARNING: train/ directory not found. Adjust DATA_ROOT.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Single-Planet Light-Curve Plot\n",
    "\n",
    "Load one planet's signal and calibration parquet files, apply calibration (dark subtract, flat field, dead pixel mask), sum over spatial rows, and plot the resulting light curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Calibration helper ─────────────────────────────────────────────────────\n",
    "\n",
    "def calibrate(raw, dark, flat, dead):\n",
    "    \"\"\"Dark subtract, flat field, zero dead pixels.\"\"\"\n",
    "    cal = raw - dark[None]\n",
    "    flat_safe = np.where(flat == 0, 1.0, flat)\n",
    "    cal /= flat_safe[None]\n",
    "    cal[:, dead.astype(bool)] = 0.0\n",
    "    return cal\n",
    "\n",
    "print(\"calibrate() helper defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load and calibrate one planet ──────────────────────────────────────────\n",
    "# AIRS-CH0: raw shape (n_time, 32*356) uint16 → reshape to (n_time, 32, 356)\n",
    "#   calibration frames: dark (32, 356), flat (32, 356), dead (32, 356)\n",
    "#   after calibration, sum spatial rows → (n_time, 356)\n",
    "# FGS1: raw shape (n_time_fgs, 32*32) uint16 → reshape to (n_time_fgs, 32, 32)\n",
    "#   same calibration, sum spatial → (n_time_fgs,), then downsample 12:1\n",
    "\n",
    "airs_data_calibrated = None  # will be (n_time, 356) after calibration\n",
    "fgs1_data_calibrated = None  # will be (n_time_fgs,) after calibration\n",
    "fgs1_downsampled = None      # will be (n_time,) after 12:1 downsample\n",
    "\n",
    "if example_planet_dir is not None:\n",
    "    # ── AIRS-CH0 ───────────────────────────────────────────────────────────\n",
    "    airs_signal_path = example_planet_dir / \"AIRS-CH0_signal.parquet\"\n",
    "    airs_cal_dir = example_planet_dir / \"AIRS-CH0_calibration\"\n",
    "\n",
    "    airs_raw = pd.read_parquet(airs_signal_path).values.astype(np.float64)\n",
    "    n_time_airs = airs_raw.shape[0]\n",
    "    airs_raw = airs_raw.reshape(n_time_airs, 32, 356)  # (n_time, 32_spatial, 356_spectral)\n",
    "\n",
    "    airs_dark = pd.read_parquet(airs_cal_dir / \"dark.parquet\").values.astype(np.float64).reshape(32, 356)\n",
    "    airs_flat = pd.read_parquet(airs_cal_dir / \"flat.parquet\").values.astype(np.float64).reshape(32, 356)\n",
    "    airs_dead = pd.read_parquet(airs_cal_dir / \"dead.parquet\").values.astype(np.float64).reshape(32, 356)\n",
    "\n",
    "    # Reshape raw for calibration: (n_time, 32*356) to pass to calibrate\n",
    "    airs_raw_flat = airs_raw.reshape(n_time_airs, 32 * 356)\n",
    "    airs_cal_flat = calibrate(\n",
    "        airs_raw_flat,\n",
    "        airs_dark.ravel(),\n",
    "        airs_flat.ravel(),\n",
    "        airs_dead.ravel()\n",
    "    )\n",
    "    # Reshape back and sum over spatial rows\n",
    "    airs_cal_3d = airs_cal_flat.reshape(n_time_airs, 32, 356)\n",
    "    airs_data_calibrated = airs_cal_3d.sum(axis=1)  # (n_time, 356)\n",
    "\n",
    "    print(f\"AIRS-CH0 signal raw shape   : ({n_time_airs}, 32, 356)\")\n",
    "    print(f\"AIRS-CH0 calibrated + summed: {airs_data_calibrated.shape}\")\n",
    "    print(f\"  value range: [{airs_data_calibrated.min():.2f}, {airs_data_calibrated.max():.2f}]\")\n",
    "\n",
    "    # ── FGS1 ───────────────────────────────────────────────────────────────\n",
    "    fgs1_signal_path = example_planet_dir / \"FGS1_signal.parquet\"\n",
    "    fgs1_cal_dir = example_planet_dir / \"FGS1_calibration\"\n",
    "\n",
    "    fgs1_raw = pd.read_parquet(fgs1_signal_path).values.astype(np.float64)\n",
    "    n_time_fgs = fgs1_raw.shape[0]\n",
    "    fgs1_raw = fgs1_raw.reshape(n_time_fgs, 32, 32)  # (n_time_fgs, 32, 32)\n",
    "\n",
    "    fgs1_dark = pd.read_parquet(fgs1_cal_dir / \"dark.parquet\").values.astype(np.float64).reshape(32, 32)\n",
    "    fgs1_flat = pd.read_parquet(fgs1_cal_dir / \"flat.parquet\").values.astype(np.float64).reshape(32, 32)\n",
    "    fgs1_dead = pd.read_parquet(fgs1_cal_dir / \"dead.parquet\").values.astype(np.float64).reshape(32, 32)\n",
    "\n",
    "    fgs1_raw_flat = fgs1_raw.reshape(n_time_fgs, 32 * 32)\n",
    "    fgs1_cal_flat = calibrate(\n",
    "        fgs1_raw_flat,\n",
    "        fgs1_dark.ravel(),\n",
    "        fgs1_flat.ravel(),\n",
    "        fgs1_dead.ravel()\n",
    "    )\n",
    "    # Sum over all spatial pixels → (n_time_fgs,)\n",
    "    fgs1_data_calibrated = fgs1_cal_flat.reshape(n_time_fgs, 32, 32).sum(axis=(1, 2))\n",
    "\n",
    "    # Downsample FGS1 by factor of 12 to match AIRS cadence\n",
    "    n_downsample = n_time_fgs // 12\n",
    "    fgs1_downsampled = fgs1_data_calibrated[:n_downsample * 12].reshape(n_downsample, 12).mean(axis=1)\n",
    "\n",
    "    print(f\"\\nFGS1 signal raw shape       : ({n_time_fgs}, 32, 32)\")\n",
    "    print(f\"FGS1 calibrated + summed    : {fgs1_data_calibrated.shape}\")\n",
    "    print(f\"FGS1 downsampled (12:1)     : {fgs1_downsampled.shape}\")\n",
    "    print(f\"  value range: [{fgs1_downsampled.min():.2f}, {fgs1_downsampled.max():.2f}]\")\n",
    "\n",
    "    print(f\"\\n[Summary] Loaded and calibrated planet '{example_planet_id}': \"\n",
    "          f\"AIRS ({n_time_airs}, 356), FGS1 ({n_time_fgs},) → downsampled to ({n_downsample},).\")\n",
    "else:\n",
    "    print(\"Skipping planet loading — no example planet directory found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Plot light curves ──────────────────────────────────────────────────────\n# AIRS-CH0: white-light curve = mean across 356 spectral channels vs time\n# FGS1: downsampled broadband light curve\n\nplanet_label = example_planet_id if example_planet_id is not None else \"Unknown\"\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 7), sharex=False)\nfig.suptitle(f\"Planet {planet_label} — calibrated light curves\", fontsize=14, fontweight=\"bold\")\n\n# ── Top subplot: AIRS white-light curve ────────────────────────────────────\nax0 = axes[0]\nif airs_data_calibrated is not None:\n    white_light = airs_data_calibrated.mean(axis=1)  # mean over 356 spectral channels → (n_time,)\n    t_airs = np.arange(len(white_light))\n    ax0.plot(t_airs, white_light, lw=0.8, color=\"steelblue\", label=\"White-light flux\")\n    ax0.set_xlabel(\"Time index\")\n    ax0.set_ylabel(\"Flux (calibrated)\")\n    ax0.set_title(f\"AIRS-CH0 — white-light curve ({len(white_light)} time steps)\")\n    ax0.legend()\n\n    # Annotate transit depth\n    depth = white_light.max() - white_light.min()\n    ax0.annotate(\n        f\"delta flux = {depth:.2f}\",\n        xy=(t_airs[len(t_airs) // 2], white_light.min()),\n        xytext=(t_airs[len(t_airs) // 2], white_light.min() + depth * 0.3),\n        arrowprops=dict(arrowstyle=\"->\", color=\"gray\"),\n        fontsize=9, color=\"gray\"\n    )\nelse:\n    ax0.text(0.5, 0.5, \"AIRS-CH0 data not loaded\", ha=\"center\", va=\"center\",\n             transform=ax0.transAxes, fontsize=12, color=\"red\")\n    ax0.set_title(\"AIRS-CH0 — white-light curve (data unavailable)\")\n\n# ── Bottom subplot: FGS1 light curve ──────────────────────────────────────\nax1 = axes[1]\nif fgs1_downsampled is not None:\n    t_fgs = np.arange(len(fgs1_downsampled))\n    ax1.plot(t_fgs, fgs1_downsampled, lw=0.8, color=\"darkorange\", label=\"FGS1 flux (12:1 downsampled)\")\n    ax1.set_xlabel(\"Time index (AIRS cadence)\")\n    ax1.set_ylabel(\"Flux (calibrated)\")\n    ax1.set_title(f\"FGS1 — broadband light curve ({len(fgs1_downsampled)} time steps after 12:1 downsample)\")\n    ax1.legend()\nelse:\n    ax1.text(0.5, 0.5, \"FGS1 data not loaded\", ha=\"center\", va=\"center\",\n             transform=ax1.transAxes, fontsize=12, color=\"red\")\n    ax1.set_title(\"FGS1 — broadband light curve (data unavailable)\")\n\nplt.tight_layout()\nfig.savefig(FIG_DIR / \"light_curves.png\")\nprint(f\"Saved: {FIG_DIR / 'light_curves.png'}\")\nplt.show()\n\nprint(f\"[Summary] Light curves plotted for planet '{planet_label}'. \"\n      f\"AIRS white-light: {len(white_light) if airs_data_calibrated is not None else 'N/A'} time steps; \"\n      f\"FGS1 downsampled: {len(fgs1_downsampled) if fgs1_downsampled is not None else 'N/A'} time steps.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. AIRS 2-D Heatmap (Time x Wavelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot the calibrated 2-D flux matrix (time x 356 spectral channels) for the example planet\n\nfig, ax = plt.subplots(figsize=(14, 5))\n\nif airs_data_calibrated is not None:\n    flux_2d = airs_data_calibrated  # (n_time, 356)\n\n    # Clip to [1st, 99th] percentile for better contrast\n    vmin = np.percentile(flux_2d, 1)\n    vmax = np.percentile(flux_2d, 99)\n\n    im = ax.imshow(\n        flux_2d.T,  # plot as (wavelength, time) so wavelength is on y-axis\n        aspect=\"auto\",\n        origin=\"lower\",\n        vmin=vmin,\n        vmax=vmax,\n        cmap=\"RdYlBu_r\",\n        interpolation=\"nearest\",\n    )\n    cbar = plt.colorbar(im, ax=ax, pad=0.02)\n    cbar.set_label(\"Flux (calibrated)\", fontsize=10)\n\n    ax.set_xlabel(\"Time index\", fontsize=11)\n    ax.set_ylabel(\"Spectral channel (0-355)\", fontsize=11)\n    ax.set_title(\n        f\"Planet {planet_label} — AIRS-CH0 calibrated flux \"\n        f\"(shape: {flux_2d.shape[0]} time x {flux_2d.shape[1]} channels)  \"\n        f\"[clipped to p1={vmin:.1f}, p99={vmax:.1f}]\",\n        fontsize=11\n    )\n\n    print(f\"[Summary] AIRS calibrated flux matrix shape: {flux_2d.shape} (time x spectral). \"\n          f\"Value range after percentile clip: [{vmin:.1f}, {vmax:.1f}].\")\nelse:\n    ax.text(0.5, 0.5, \"AIRS-CH0 data not loaded — cannot render heatmap\",\n            ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=12, color=\"red\")\n    ax.set_title(\"AIRS-CH0 flux heatmap (data unavailable)\")\n    print(\"[Summary] Skipped heatmap — AIRS data not available.\")\n\nplt.tight_layout()\nfig.savefig(FIG_DIR / \"airs_heatmap.png\")\nprint(f\"Saved: {FIG_DIR / 'airs_heatmap.png'}\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ADC Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot histograms for all ADC feature columns from train_adc_info.csv\n# Columns: FGS1_adc_offset, FGS1_adc_gain, AIRS-CH0_adc_offset, AIRS-CH0_adc_gain, star\n\nid_col = df_adc.columns[0]\nfeature_cols = [c for c in df_adc.columns if c != id_col and pd.api.types.is_numeric_dtype(df_adc[c])]\n\nprint(f\"ADC feature columns ({len(feature_cols)}): {feature_cols}\")\n\nn_features = len(feature_cols)\nncols_grid = min(n_features, 3)\nnrows = (n_features + ncols_grid - 1) // ncols_grid\nfig, axes = plt.subplots(nrows, ncols_grid, figsize=(5 * ncols_grid, 4 * nrows))\nif n_features == 1:\n    axes_flat = [axes]\nelse:\n    axes_flat = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n\nfor idx, col in enumerate(feature_cols):\n    ax = axes_flat[idx]\n    vals = df_adc[col].dropna()\n\n    # Determine if log scale is appropriate\n    use_log = False\n    if vals.min() > 0:\n        ratio = vals.max() / vals.min()\n        if ratio > 100:\n            use_log = True\n\n    if use_log:\n        ax.hist(vals, bins=50, color=\"steelblue\", edgecolor=\"white\", linewidth=0.3)\n        ax.set_xscale(\"log\")\n        scale_note = \"(log x-axis)\"\n    else:\n        ax.hist(vals, bins=50, color=\"steelblue\", edgecolor=\"white\", linewidth=0.3)\n        scale_note = \"\"\n\n    ax.set_title(f\"{col} {scale_note}\", fontsize=10, fontweight=\"bold\")\n    ax.set_xlabel(col, fontsize=9)\n    ax.set_ylabel(\"Count\", fontsize=9)\n    ax.tick_params(labelsize=8)\n\n    # Annotate with median\n    median_val = vals.median()\n    ax.axvline(median_val, color=\"red\", linestyle=\"--\", lw=1.2, alpha=0.8)\n    ax.text(0.97, 0.95, f\"median={median_val:.3g}\",\n            transform=ax.transAxes, ha=\"right\", va=\"top\", fontsize=8, color=\"red\")\n\n# Hide unused subplots\nfor idx in range(n_features, len(axes_flat)):\n    axes_flat[idx].set_visible(False)\n\nfig.suptitle(\"ADC Feature Distributions (train_adc_info.csv)\", fontsize=14, fontweight=\"bold\", y=1.01)\nplt.tight_layout()\nfig.savefig(FIG_DIR / \"adc_distributions.png\")\nprint(f\"Saved: {FIG_DIR / 'adc_distributions.png'}\")\nplt.show()\n\n# Print summary stats\nprint(\"\\nSummary statistics for ADC features:\")\ndisplay(df_adc[feature_cols].describe().round(4))\n\nprint(f\"\\n[Summary] Plotted {len(feature_cols)} ADC features. \"\n      f\"Log x-scale applied to columns spanning >2 orders of magnitude.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Transit Depth Distribution (Labelled Planets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# For labelled planets, compute median transit depth across 283 wavelength channels\n# train_labels.csv has columns: planet_id, wl_1, wl_2, ..., wl_283 (means only, no quartiles)\n\nprint(\"train_labels.csv column names (first 10):\")\nprint(df_labels.columns[:10].tolist())\nprint(f\"\\ntrain_labels.csv column names (last 5):\")\nprint(df_labels.columns[-5:].tolist())\n\n# Identify wavelength columns (wl_1 ... wl_283)\nwl_cols = [c for c in df_labels.columns if c.startswith(\"wl_\")]\nprint(f\"\\nFound {len(wl_cols)} wavelength columns.\")\n\n# Compute median transit depth per planet (median across 283 wavelengths)\nwl_values_labels = df_labels[wl_cols].values.astype(float)\nmedian_depth_per_planet = np.nanmedian(wl_values_labels, axis=1)\n\noverall_median = np.nanmedian(median_depth_per_planet)\noverall_std = np.nanstd(median_depth_per_planet)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.hist(median_depth_per_planet, bins=60, color=\"steelblue\", edgecolor=\"white\",\n        linewidth=0.3, alpha=0.85, label=\"Median transit depth\")\n\nax.axvline(overall_median, color=\"red\", linestyle=\"--\", lw=2, label=f\"Median = {overall_median:.4f}\")\nax.axvline(overall_median - overall_std, color=\"orange\", linestyle=\":\", lw=1.5,\n           label=f\"+-1 std = {overall_std:.4f}\")\nax.axvline(overall_median + overall_std, color=\"orange\", linestyle=\":\", lw=1.5)\n\n# Shaded region\nax.axvspan(overall_median - overall_std, overall_median + overall_std,\n           alpha=0.1, color=\"orange\")\n\nax.set_xlabel(\"Median wl value (transit depth proxy) across 283 wavelengths\", fontsize=11)\nax.set_ylabel(\"Number of planets\", fontsize=11)\nax.set_title(\n    f\"Transit Depth Distribution — {len(median_depth_per_planet)} labelled planets\\n\"\n    f\"Annotated: median = {overall_median:.4f} +- std = {overall_std:.4f}\",\n    fontsize=12\n)\nax.legend(fontsize=9)\n\nplt.tight_layout()\nfig.savefig(FIG_DIR / \"transit_depth_distribution.png\")\nprint(f\"Saved: {FIG_DIR / 'transit_depth_distribution.png'}\")\nplt.show()\n\nprint(f\"Transit depth statistics across {len(median_depth_per_planet)} labelled planets:\")\nprint(f\"  Median  : {overall_median:.6f}\")\nprint(f\"  Std     : {overall_std:.6f}\")\nprint(f\"  Min     : {np.nanmin(median_depth_per_planet):.6f}\")\nprint(f\"  Max     : {np.nanmax(median_depth_per_planet):.6f}\")\nprint(f\"  p5      : {np.nanpercentile(median_depth_per_planet, 5):.6f}\")\nprint(f\"  p95     : {np.nanpercentile(median_depth_per_planet, 95):.6f}\")\nprint(f\"\\n[Summary] Median transit depth = {overall_median:.4f} +- {overall_std:.4f} across \"\n      f\"{len(median_depth_per_planet)} labelled planets; range \"\n      f\"[{np.nanmin(median_depth_per_planet):.4f}, {np.nanmax(median_depth_per_planet):.4f}].\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Label Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pearson correlation matrix of wl_* columns (283 wavelengths)\n# Subsample every 10th wavelength for readability → ~28 channels\n\nSUBSAMPLE_STEP = 10  # use every 10th wavelength\nwl_sub_cols = wl_cols[::SUBSAMPLE_STEP]\n\nprint(f\"Subsampling from {len(wl_cols)} to {len(wl_sub_cols)} wavelength channels \"\n      f\"(every {SUBSAMPLE_STEP}th channel).\")\n\n# Compute Pearson correlation\ndf_wl_sub = df_labels[wl_sub_cols].copy()\n\n# Create short labels: wavelength index\nshort_labels = [c.replace(\"wl_\", \"\") for c in wl_sub_cols]\ndf_wl_sub.columns = short_labels\n\ncorr_matrix = df_wl_sub.corr(method=\"pearson\")\n\nfig, ax = plt.subplots(figsize=(14, 12))\n\nsns.heatmap(\n    corr_matrix,\n    ax=ax,\n    cmap=\"RdBu_r\",\n    vmin=-1,\n    vmax=1,\n    center=0,\n    square=True,\n    linewidths=0.3,\n    cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"},\n    annot=False,\n    xticklabels=short_labels,\n    yticklabels=short_labels,\n)\n\nax.set_title(\n    f\"Label Correlation Heatmap — wl_* (mean transit depth) across subsampled wavelengths\\n\"\n    f\"Showing {len(wl_sub_cols)} of {len(wl_cols)} channels (every {SUBSAMPLE_STEP}th), \"\n    f\"{len(df_labels)} labelled planets\",\n    fontsize=11\n)\nax.set_xlabel(\"Wavelength channel index\", fontsize=10)\nax.set_ylabel(\"Wavelength channel index\", fontsize=10)\nax.tick_params(axis=\"x\", rotation=45, labelsize=7)\nax.tick_params(axis=\"y\", rotation=0, labelsize=7)\n\nplt.tight_layout()\nfig.savefig(FIG_DIR / \"label_correlation_heatmap.png\")\nprint(f\"Saved: {FIG_DIR / 'label_correlation_heatmap.png'}\")\nplt.show()\n\n# Find pairs with high correlation\ncorr_vals = corr_matrix.values.copy()\nnp.fill_diagonal(corr_vals, np.nan)\nmax_corr = np.nanmax(np.abs(corr_vals))\nmean_corr = np.nanmean(np.abs(corr_vals))\n\nprint(f\"Correlation statistics (off-diagonal):\")\nprint(f\"  Max |Pearson r| : {max_corr:.4f}\")\nprint(f\"  Mean |Pearson r|: {mean_corr:.4f}\")\nprint(f\"  Highly correlated pairs (|r| > 0.9): \"\n      f\"{(np.abs(corr_vals) > 0.9).sum() // 2}\")\n\nprint(f\"\\n[Summary] Label correlation heatmap: mean |r| = {mean_corr:.3f}, \"\n      f\"max |r| = {max_corr:.3f} among {len(wl_sub_cols)} subsampled wavelength channels.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "### Key findings from this EDA:\n",
    "\n",
    "- **Dataset scale**: The competition dataset is ~180 GB, consisting of nested directories with parquet files per planet, containing time-series photometry from two instruments (AIRS-CH0 and FGS1) plus CSV/parquet metadata tables.\n",
    "\n",
    "- **Data format**: Each planet directory contains `AIRS-CH0_signal.parquet` (11250 time steps, 32x356 detector pixels as uint16), `FGS1_signal.parquet` (135000 time steps at 12x cadence, 32x32 pixels as uint16), and calibration subdirectories with `dark.parquet`, `flat.parquet`, `dead.parquet`, `read.parquet`, and `linear_corr.parquet`.\n",
    "\n",
    "- **Calibration pipeline**: Raw uint16 detector frames require dark subtraction, flat fielding, and dead pixel masking before scientific use. Spatial rows are then summed to produce 1-D spectra per time step.\n",
    "\n",
    "- **Labels**: Only ~24% of planets are labelled with mean transit depths (wl_1...wl_283) across 283 wavelength bins in `train_labels.csv`. No quartile or sigma columns are provided — only means. The remaining ~76% form the test set.\n",
    "\n",
    "- **Wavelength grid**: 283 channels total: 1 FGS1 visible channel (0.705 um) + 282 AIRS infrared channels. Grid defined in `wavelengths.csv`.\n",
    "\n",
    "- **Typical transit depth range**: The median transit depth across labelled planets shows substantial planet-to-planet variation (see Section 8 histogram).\n",
    "\n",
    "- **ADC features**: Five features available in `train_adc_info.csv`: FGS1_adc_offset, FGS1_adc_gain, AIRS-CH0_adc_offset, AIRS-CH0_adc_gain, and star identifier. These describe detector gain/offset calibration parameters per planet.\n",
    "\n",
    "- **AIRS-CH0 structure**: After calibration and spatial summing, each planet's light curve is a 2-D matrix of shape (n_time x 356 spectral channels). The white-light curve shows a clear transit dip. The 2-D heatmap reveals wavelength-dependent flux variations.\n",
    "\n",
    "- **FGS1 structure**: 32x32 visible photometer at 12x higher cadence than AIRS. After calibration, spatial summing, and 12:1 downsampling, it provides a complementary broadband transit signal.\n",
    "\n",
    "- **Label correlation**: Adjacent wavelength channels are highly correlated (Pearson r typically > 0.9 for neighbouring channels), consistent with smooth atmospheric spectra. Distant channels decorrelate across molecular absorption band boundaries.\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "1. Implement full calibration pipeline including `read.parquet` and `linear_corr.parquet` corrections.\n",
    "2. Compute per-planet baseline features: white-light transit depth, ingress/egress timing.\n",
    "3. Extract AIRS-CH0 spectral features: per-channel transit depth, SNR per channel.\n",
    "4. Build a baseline model using ADC features + spectral features.\n",
    "5. Explore common-mode correction using FGS1 as a systematics reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}