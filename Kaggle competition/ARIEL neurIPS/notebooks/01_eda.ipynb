{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS 2024 Ariel Data Challenge — Exploratory Data Analysis\n",
    "\n",
    "**Goal**: Understand the structure, distributions, and quality of the Ariel exoplanet atmospheric spectra dataset.  \n",
    "**Dataset**: ~180 GB of simulated telescope photometry from AIRS-CH0 (IR spectrometer) and FGS1 (visible photometer).  \n",
    "**Task**: Extract exoplanet atmospheric transmission spectra (283 wavelength channels) from transit light curves.  \n",
    "**Scoring**: Gaussian Log-Likelihood over predicted mean and std per wavelength.\n",
    "\n",
    "> **Note**: This notebook is Kaggle-ready and requires the `ariel-data-challenge-2024` dataset attached to the kernel."
   ],
   "id": "cell-markdown-title",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ],
   "id": "cell-markdown-setup",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing packages\n",
    "import subprocess, sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package}: already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"{package}: not found — installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\"{package}: installed successfully\")\n",
    "\n",
    "install_if_missing(\"h5py\")\n",
    "install_if_missing(\"seaborn\")"
   ],
   "id": "cell-setup-install"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "\n",
    "# ── Data root ──────────────────────────────────────────────────────────────\n",
    "DATA_ROOT = Path(\"/kaggle/input/ariel-data-challenge-2024\")\n",
    "\n",
    "# ── Plot style ─────────────────────────────────────────────────────────────\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "# ── Version report ─────────────────────────────────────────────────────────\n",
    "print(f\"Python      : {sys.version}\")\n",
    "print(f\"NumPy       : {np.__version__}\")\n",
    "print(f\"Pandas      : {pd.__version__}\")\n",
    "print(f\"Matplotlib  : {matplotlib.__version__}\")\n",
    "print(f\"Seaborn     : {sns.__version__}\")\n",
    "print(f\"h5py        : {h5py.__version__}\")\n",
    "print(f\"\\nDATA_ROOT   : {DATA_ROOT}\")\n",
    "print(f\"Exists      : {DATA_ROOT.exists()}\")"
   ],
   "id": "cell-setup-imports"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Tree"
   ],
   "id": "cell-markdown-filetree",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk the data root and report file sizes\n",
    "total_bytes = 0\n",
    "file_records = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(DATA_ROOT):\n",
    "    # Skip hidden directories\n",
    "    dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n",
    "    for fname in sorted(filenames):\n",
    "        fpath = Path(dirpath) / fname\n",
    "        try:\n",
    "            size_bytes = fpath.stat().st_size\n",
    "        except OSError:\n",
    "            size_bytes = 0\n",
    "        total_bytes += size_bytes\n",
    "        rel_path = fpath.relative_to(DATA_ROOT)\n",
    "        file_records.append({\n",
    "            \"path\": str(rel_path),\n",
    "            \"size_MB\": round(size_bytes / 1_048_576, 2),\n",
    "        })\n",
    "\n",
    "df_files = pd.DataFrame(file_records).sort_values(\"size_MB\", ascending=False).reset_index(drop=True)\n",
    "print(f\"Total files : {len(df_files)}\")\n",
    "print(f\"Total size  : {total_bytes / 1_073_741_824:.2f} GB\\n\")\n",
    "print(df_files.to_string(index=False))\n",
    "\n",
    "print(f\"\\n[Summary] Found {len(df_files)} files totalling {total_bytes / 1_073_741_824:.2f} GB on disk.\")"
   ],
   "id": "cell-filetree"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CSV Inspection"
   ],
   "id": "cell-markdown-csv",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── AuxillaryTable.csv ─────────────────────────────────────────────────────\n",
    "aux_path = DATA_ROOT / \"AuxillaryTable.csv\"\n",
    "df_aux = pd.read_csv(aux_path)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AuxillaryTable.csv\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df_aux.shape}  ({df_aux.shape[0]} planets, {df_aux.shape[1]} columns)\")\n",
    "print(\"\\n--- Head ---\")\n",
    "display(df_aux.head())\n",
    "\n",
    "print(\"\\n--- Data types ---\")\n",
    "print(df_aux.dtypes.to_string())\n",
    "\n",
    "print(\"\\n--- Missing value counts ---\")\n",
    "missing = df_aux.isnull().sum()\n",
    "print(missing[missing > 0].to_string() if missing.any() else \"No missing values detected.\")\n",
    "\n",
    "print(\"\\n--- Descriptive statistics ---\")\n",
    "display(df_aux.describe())\n",
    "\n",
    "print(f\"\\n[Summary] AuxillaryTable has {df_aux.shape[0]} planets and {df_aux.shape[1]} columns \"\n",
    "      f\"with {df_aux.isnull().sum().sum()} total missing values.\")"
   ],
   "id": "cell-csv-aux"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── QuartilesTable.csv ─────────────────────────────────────────────────────\n",
    "q_path = DATA_ROOT / \"QuartilesTable.csv\"\n",
    "df_q = pd.read_csv(q_path)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"QuartilesTable.csv\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df_q.shape}\")\n",
    "print(\"\\n--- First 5 columns ---\")\n",
    "display(df_q.iloc[:, :5].head(10))\n",
    "print(\"\\n--- Column name pattern ---\")\n",
    "print(df_q.columns[:10].tolist(), \"...\")\n",
    "\n",
    "# Labelled vs unlabelled planets\n",
    "# QuartilesTable contains only labelled planets (training set)\n",
    "labelled_ids = set(df_q.iloc[:, 0].unique()) if df_q.shape[0] > 0 else set()\n",
    "all_ids = set(df_aux.iloc[:, 0].unique())\n",
    "n_labelled = len(labelled_ids)\n",
    "n_total = len(all_ids)\n",
    "n_unlabelled = n_total - n_labelled\n",
    "\n",
    "print(\"\\n--- Labelled vs Unlabelled ---\")\n",
    "print(f\"Total planets (AuxillaryTable) : {n_total:>6}\")\n",
    "print(f\"Labelled planets (QuartilesTable): {n_labelled:>6}  ({100 * n_labelled / n_total:.1f}%)\")\n",
    "print(f\"Unlabelled planets               : {n_unlabelled:>6}  ({100 * n_unlabelled / n_total:.1f}%)\")\n",
    "\n",
    "print(f\"\\n[Summary] {n_labelled} of {n_total} planets ({100 * n_labelled / n_total:.1f}%) have \"\n",
    "      f\"quartile labels; {n_unlabelled} are unlabelled (test set).\")"
   ],
   "id": "cell-csv-quartiles"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HDF5 Structure"
   ],
   "id": "cell-markdown-hdf5",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: verify key names after running explore_data.py on Kaggle\n",
    "# Expected HDF5 top-level keys: \"AIRS-CH0\" (IR spectrometer) and \"FGS1\" (visible photometer)\n",
    "\n",
    "# Locate the HDF5 file(s)\n",
    "hdf5_files = list(DATA_ROOT.rglob(\"*.h5\")) + list(DATA_ROOT.rglob(\"*.hdf5\")) + list(DATA_ROOT.rglob(\"*.hdf\"))\n",
    "print(f\"HDF5 files found: {len(hdf5_files)}\")\n",
    "for f in hdf5_files[:10]:\n",
    "    print(f\"  {f.relative_to(DATA_ROOT)}  ({f.stat().st_size / 1_073_741_824:.2f} GB)\")\n",
    "\n",
    "# Use the first HDF5 file for structure inspection\n",
    "hdf5_path = hdf5_files[0] if hdf5_files else None\n",
    "if hdf5_path is None:\n",
    "    print(\"WARNING: No HDF5 file found. Adjust the glob pattern to match the actual filename.\")"
   ],
   "id": "cell-hdf5-locate"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── HDF5 top-level structure ────────────────────────────────────────────────\n",
    "if hdf5_path is not None:\n",
    "    with h5py.File(hdf5_path, \"r\") as f:\n",
    "        print(f\"File: {hdf5_path.name}\")\n",
    "        print(f\"Top-level keys: {list(f.keys())}\")\n",
    "\n",
    "        # TODO: verify key names after running explore_data.py on Kaggle\n",
    "        AIRS_KEY = \"AIRS-CH0\"   # TODO: verify key names after running explore_data.py on Kaggle\n",
    "        FGS1_KEY = \"FGS1\"       # TODO: verify key names after running explore_data.py on Kaggle\n",
    "\n",
    "        try:\n",
    "            airs_group = f[AIRS_KEY]\n",
    "            print(f\"\\n'{AIRS_KEY}' group found.\")\n",
    "            planet_ids_airs = list(airs_group.keys())\n",
    "            print(f\"  Number of planets: {len(planet_ids_airs)}\")\n",
    "            print(f\"  First 5 planet IDs: {planet_ids_airs[:5]}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError accessing '{AIRS_KEY}': {e}. Check actual key names with list(f.keys()).\")\n",
    "            planet_ids_airs = []\n",
    "\n",
    "        try:\n",
    "            fgs1_group = f[FGS1_KEY]\n",
    "            print(f\"\\n'{FGS1_KEY}' group found.\")\n",
    "            planet_ids_fgs1 = list(fgs1_group.keys())\n",
    "            print(f\"  Number of planets: {len(planet_ids_fgs1)}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError accessing '{FGS1_KEY}': {e}. Check actual key names with list(f.keys()).\")\n",
    "            planet_ids_fgs1 = []\n",
    "\n",
    "        print(f\"\\n[Summary] HDF5 contains {len(planet_ids_airs)} planets under '{AIRS_KEY}' \"\n",
    "              f\"and {len(planet_ids_fgs1)} planets under '{FGS1_KEY}'.\")\n",
    "else:\n",
    "    print(\"Skipping HDF5 structure inspection — no file found.\")"
   ],
   "id": "cell-hdf5-structure"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Example planet: dataset shapes and dtypes ───────────────────────────────\n",
    "# TODO: verify key names after running explore_data.py on Kaggle\n",
    "AIRS_KEY = \"AIRS-CH0\"  # TODO: verify key names after running explore_data.py on Kaggle\n",
    "FGS1_KEY = \"FGS1\"      # TODO: verify key names after running explore_data.py on Kaggle\n",
    "\n",
    "example_planet_id = None\n",
    "airs_data_example = None\n",
    "fgs1_data_example = None\n",
    "\n",
    "if hdf5_path is not None:\n",
    "    with h5py.File(hdf5_path, \"r\") as f:\n",
    "        try:\n",
    "            planet_ids = list(f[AIRS_KEY].keys())\n",
    "            example_planet_id = planet_ids[0]  # index 0\n",
    "            print(f\"Example planet ID: {example_planet_id}\")\n",
    "\n",
    "            # AIRS-CH0 data\n",
    "            airs_ds = f[AIRS_KEY][example_planet_id]\n",
    "            print(f\"\\n{AIRS_KEY}/{example_planet_id}:\")\n",
    "            if isinstance(airs_ds, h5py.Dataset):\n",
    "                print(f\"  shape : {airs_ds.shape}\")\n",
    "                print(f\"  dtype : {airs_ds.dtype}\")\n",
    "                print(f\"  min   : {airs_ds[...].min():.6f}\")\n",
    "                print(f\"  max   : {airs_ds[...].max():.6f}\")\n",
    "                airs_data_example = airs_ds[...]  # load into memory\n",
    "            else:\n",
    "                # It may be a sub-group\n",
    "                print(f\"  sub-keys: {list(airs_ds.keys())}\")\n",
    "                for sk in list(airs_ds.keys()):\n",
    "                    ds = airs_ds[sk]\n",
    "                    print(f\"    {sk}: shape={ds.shape}, dtype={ds.dtype}\")\n",
    "                # Try to find the main flux array\n",
    "                for candidate in [\"flux\", \"signal\", \"data\", \"photometry\"]:\n",
    "                    if candidate in airs_ds:\n",
    "                        airs_data_example = airs_ds[candidate][...]\n",
    "                        print(f\"  Using sub-key '{candidate}' as flux array.\")\n",
    "                        break\n",
    "\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Error reading AIRS-CH0 example: {e}\")\n",
    "\n",
    "        try:\n",
    "            if example_planet_id and example_planet_id in f[FGS1_KEY]:\n",
    "                fgs1_ds = f[FGS1_KEY][example_planet_id]\n",
    "                print(f\"\\n{FGS1_KEY}/{example_planet_id}:\")\n",
    "                if isinstance(fgs1_ds, h5py.Dataset):\n",
    "                    print(f\"  shape : {fgs1_ds.shape}\")\n",
    "                    print(f\"  dtype : {fgs1_ds.dtype}\")\n",
    "                    print(f\"  min   : {fgs1_ds[...].min():.6f}\")\n",
    "                    print(f\"  max   : {fgs1_ds[...].max():.6f}\")\n",
    "                    fgs1_data_example = fgs1_ds[...]\n",
    "                else:\n",
    "                    print(f\"  sub-keys: {list(fgs1_ds.keys())}\")\n",
    "                    for sk in list(fgs1_ds.keys()):\n",
    "                        ds = fgs1_ds[sk]\n",
    "                        print(f\"    {sk}: shape={ds.shape}, dtype={ds.dtype}\")\n",
    "                    for candidate in [\"flux\", \"signal\", \"data\", \"photometry\"]:\n",
    "                        if candidate in fgs1_ds:\n",
    "                            fgs1_data_example = fgs1_ds[candidate][...]\n",
    "                            print(f\"  Using sub-key '{candidate}' as flux array.\")\n",
    "                            break\n",
    "        except (KeyError, TypeError) as e:\n",
    "            print(f\"Error reading FGS1 example: {e}\")\n",
    "\n",
    "        print(f\"\\n[Summary] Loaded example planet '{example_planet_id}': \"\n",
    "              f\"AIRS shape={getattr(airs_data_example, 'shape', 'N/A')}, \"\n",
    "              f\"FGS1 shape={getattr(fgs1_data_example, 'shape', 'N/A')}.\")\n",
    "else:\n",
    "    print(\"Skipping example planet inspection — no HDF5 file found.\")"
   ],
   "id": "cell-hdf5-example"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Single-Planet Light-Curve Plot"
   ],
   "id": "cell-markdown-lightcurve",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw light curves for planet index 0\n",
    "# AIRS-CH0: white-light curve = mean across all wavelength channels vs time\n",
    "# FGS1: single broadband channel vs time\n",
    "\n",
    "planet_label = example_planet_id if example_planet_id is not None else \"Unknown\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 7), sharex=False)\n",
    "fig.suptitle(f\"Planet {planet_label} — raw light curves\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# ── Top subplot: AIRS white-light curve ────────────────────────────────────\n",
    "ax0 = axes[0]\n",
    "if airs_data_example is not None:\n",
    "    arr = np.asarray(airs_data_example)\n",
    "    # Handle dimensionality: expect (time, wavelength) or (wavelength, time)\n",
    "    if arr.ndim == 2:\n",
    "        # Guess orientation: more wavelengths than time steps → (time, wavelength)\n",
    "        if arr.shape[1] > arr.shape[0]:\n",
    "            white_light = arr.mean(axis=1)  # mean over wavelength → (time,)\n",
    "        else:\n",
    "            white_light = arr.mean(axis=0)  # mean over wavelength → (time,)\n",
    "    elif arr.ndim == 1:\n",
    "        white_light = arr\n",
    "    else:\n",
    "        white_light = arr.reshape(arr.shape[0], -1).mean(axis=1)\n",
    "\n",
    "    t_airs = np.arange(len(white_light))\n",
    "    ax0.plot(t_airs, white_light, lw=0.8, color=\"steelblue\", label=\"White-light flux\")\n",
    "    ax0.set_xlabel(\"Time index\")\n",
    "    ax0.set_ylabel(\"Flux (normalised)\")\n",
    "    ax0.set_title(f\"AIRS-CH0 — white-light curve ({len(white_light)} time steps)\")\n",
    "    ax0.legend()\n",
    "\n",
    "    # Annotate transit depth\n",
    "    depth = white_light.max() - white_light.min()\n",
    "    ax0.annotate(\n",
    "        f\"Δflux ≈ {depth:.4f}\",\n",
    "        xy=(t_airs[len(t_airs) // 2], white_light.min()),\n",
    "        xytext=(t_airs[len(t_airs) // 2], white_light.min() + depth * 0.3),\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"gray\"),\n",
    "        fontsize=9, color=\"gray\"\n",
    "    )\n",
    "else:\n",
    "    ax0.text(0.5, 0.5, \"AIRS-CH0 data not loaded\", ha=\"center\", va=\"center\",\n",
    "             transform=ax0.transAxes, fontsize=12, color=\"red\")\n",
    "    ax0.set_title(\"AIRS-CH0 — white-light curve (data unavailable)\")\n",
    "\n",
    "# ── Bottom subplot: FGS1 light curve ──────────────────────────────────────\n",
    "ax1 = axes[1]\n",
    "if fgs1_data_example is not None:\n",
    "    arr_fgs = np.asarray(fgs1_data_example).ravel()\n",
    "    t_fgs = np.arange(len(arr_fgs))\n",
    "    ax1.plot(t_fgs, arr_fgs, lw=0.8, color=\"darkorange\", label=\"FGS1 flux\")\n",
    "    ax1.set_xlabel(\"Time index\")\n",
    "    ax1.set_ylabel(\"Flux (normalised)\")\n",
    "    ax1.set_title(f\"FGS1 — broadband light curve ({len(arr_fgs)} time steps)\")\n",
    "    ax1.legend()\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, \"FGS1 data not loaded\", ha=\"center\", va=\"center\",\n",
    "             transform=ax1.transAxes, fontsize=12, color=\"red\")\n",
    "    ax1.set_title(\"FGS1 — broadband light curve (data unavailable)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"[Summary] Light curves plotted for planet '{planet_label}'. \"\n",
    "      f\"AIRS white-light: {getattr(white_light if airs_data_example is not None else None, 'shape', 'N/A')} time steps; \"\n",
    "      f\"FGS1: {getattr(arr_fgs if fgs1_data_example is not None else None, 'shape', 'N/A')} time steps.\")"
   ],
   "id": "cell-lightcurve-plot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. AIRS 2-D Heatmap (Time × Wavelength)"
   ],
   "id": "cell-markdown-heatmap",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2-D flux matrix (time × wavelength) for planet index 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "if airs_data_example is not None:\n",
    "    arr = np.asarray(airs_data_example)\n",
    "\n",
    "    # Ensure shape is (time, wavelength)\n",
    "    if arr.ndim == 2:\n",
    "        if arr.shape[1] < arr.shape[0]:\n",
    "            # Looks like (wavelength, time) — transpose\n",
    "            arr = arr.T\n",
    "        # Now arr is (time, wavelength)\n",
    "        flux_2d = arr\n",
    "    elif arr.ndim == 3:\n",
    "        # E.g. (time, wavelength, something) — take first slice\n",
    "        flux_2d = arr[:, :, 0]\n",
    "    else:\n",
    "        flux_2d = arr.reshape(-1, 1)\n",
    "\n",
    "    # Clip to [1st, 99th] percentile for better contrast\n",
    "    vmin = np.percentile(flux_2d, 1)\n",
    "    vmax = np.percentile(flux_2d, 99)\n",
    "\n",
    "    im = ax.imshow(\n",
    "        flux_2d.T,  # plot as (wavelength, time) so wavelength is on y-axis\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cmap=\"RdYlBu_r\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    cbar = plt.colorbar(im, ax=ax, pad=0.02)\n",
    "    cbar.set_label(\"Flux (normalised)\", fontsize=10)\n",
    "\n",
    "    ax.set_xlabel(\"Time index\", fontsize=11)\n",
    "    ax.set_ylabel(\"Wavelength channel\", fontsize=11)\n",
    "    ax.set_title(\n",
    "        f\"Planet {planet_label} — AIRS-CH0 flux matrix \"\n",
    "        f\"(shape: {flux_2d.shape[0]} time × {flux_2d.shape[1]} wavelengths)  \"\n",
    "        f\"[clipped to p1={vmin:.4f}, p99={vmax:.4f}]\",\n",
    "        fontsize=11\n",
    "    )\n",
    "\n",
    "    print(f\"[Summary] AIRS flux matrix shape: {flux_2d.shape} (time × wavelength). \"\n",
    "          f\"Value range after percentile clip: [{vmin:.4f}, {vmax:.4f}].\")\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"AIRS-CH0 data not loaded — cannot render heatmap\",\n",
    "            ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=12, color=\"red\")\n",
    "    ax.set_title(\"AIRS-CH0 flux heatmap (data unavailable)\")\n",
    "    print(\"[Summary] Skipped heatmap — AIRS data not available.\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-heatmap"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Auxiliary Feature Distributions"
   ],
   "id": "cell-markdown-aux-dist",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for all auxiliary feature columns in a 3×3 grid\n",
    "# Use log-scale x-axis for columns spanning > 2 orders of magnitude\n",
    "\n",
    "# Identify numeric feature columns (exclude the planet ID column)\n",
    "# The first column is typically the planet ID\n",
    "id_col = df_aux.columns[0]\n",
    "feature_cols = [c for c in df_aux.columns if c != id_col and pd.api.types.is_numeric_dtype(df_aux[c])]\n",
    "\n",
    "print(f\"Auxiliary feature columns ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "n_cols = min(len(feature_cols), 9)\n",
    "nrows, ncols_grid = 3, 3\n",
    "fig, axes = plt.subplots(nrows, ncols_grid, figsize=(15, 11))\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(feature_cols[:n_cols]):\n",
    "    ax = axes_flat[idx]\n",
    "    vals = df_aux[col].dropna()\n",
    "\n",
    "    # Determine if log scale is appropriate\n",
    "    use_log = False\n",
    "    if vals.min() > 0:\n",
    "        ratio = vals.max() / vals.min()\n",
    "        if ratio > 100:  # > 2 orders of magnitude\n",
    "            use_log = True\n",
    "\n",
    "    if use_log:\n",
    "        ax.hist(vals, bins=50, color=\"steelblue\", edgecolor=\"white\", linewidth=0.3, log=False)\n",
    "        ax.set_xscale(\"log\")\n",
    "        scale_note = \"(log x-axis)\"\n",
    "    else:\n",
    "        ax.hist(vals, bins=50, color=\"steelblue\", edgecolor=\"white\", linewidth=0.3)\n",
    "        scale_note = \"\"\n",
    "\n",
    "    ax.set_title(f\"{col} {scale_note}\", fontsize=9, fontweight=\"bold\")\n",
    "    ax.set_xlabel(col, fontsize=8)\n",
    "    ax.set_ylabel(\"Count\", fontsize=8)\n",
    "    ax.tick_params(labelsize=7)\n",
    "\n",
    "    # Annotate with median\n",
    "    median_val = vals.median()\n",
    "    ax.axvline(median_val, color=\"red\", linestyle=\"--\", lw=1.2, alpha=0.8)\n",
    "    ax.text(0.97, 0.95, f\"median={median_val:.3g}\",\n",
    "            transform=ax.transAxes, ha=\"right\", va=\"top\", fontsize=7, color=\"red\")\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_cols, len(axes_flat)):\n",
    "    axes_flat[idx].set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Auxiliary Feature Distributions\", fontsize=14, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary stats\n",
    "print(\"\\nSummary statistics for auxiliary features:\")\n",
    "display(df_aux[feature_cols].describe().round(4))\n",
    "\n",
    "print(f\"\\n[Summary] Plotted {len(feature_cols)} auxiliary features. \"\n",
    "      f\"Log x-scale applied to columns spanning >2 orders of magnitude.\")"
   ],
   "id": "cell-aux-distributions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Transit Depth Distribution (Labelled Planets)"
   ],
   "id": "cell-markdown-transit-depth",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For labelled planets, compute median transit depth across 283 output wavelengths\n",
    "# Transit depth is represented by the q2 (median) columns in QuartilesTable\n",
    "# Column naming convention: expected to be something like 'q2_0', 'q2_1', ... or similar\n",
    "\n",
    "print(\"QuartilesTable column names (first 20):\")\n",
    "print(df_q.columns[:20].tolist())\n",
    "print(\"\\nQuartilesTable column names (last 10):\")\n",
    "print(df_q.columns[-10:].tolist())\n",
    "\n",
    "# Identify q2 columns (median quartile = transit depth proxy)\n",
    "# Try common naming patterns\n",
    "q2_cols = [c for c in df_q.columns if \"q2\" in c.lower() or \"median\" in c.lower()]\n",
    "\n",
    "if not q2_cols:\n",
    "    # Alternative: columns may be (planet_id, q1_0...q1_282, q2_0...q2_282, q3_0...q3_282)\n",
    "    # Try to split into thirds (excluding the ID column)\n",
    "    numeric_cols = [c for c in df_q.columns if pd.api.types.is_numeric_dtype(df_q[c])]\n",
    "    n_numeric = len(numeric_cols)\n",
    "    # If total numeric cols = 3 * 283 = 849, split into thirds\n",
    "    if n_numeric == 849:\n",
    "        q1_cols = numeric_cols[:283]\n",
    "        q2_cols = numeric_cols[283:566]\n",
    "        q3_cols = numeric_cols[566:]\n",
    "        print(f\"Auto-detected 849 numeric cols → split into q1/q2/q3 (283 each).\")\n",
    "    elif n_numeric % 3 == 0:\n",
    "        third = n_numeric // 3\n",
    "        q2_cols = numeric_cols[third:2 * third]\n",
    "        print(f\"Auto-detected {n_numeric} numeric cols → using middle third as q2 ({len(q2_cols)} cols).\")\n",
    "    else:\n",
    "        q2_cols = numeric_cols  # fallback: use all numeric\n",
    "        print(f\"Could not auto-detect q2 columns. Using all {len(q2_cols)} numeric columns.\")\n",
    "\n",
    "print(f\"\\nUsing {len(q2_cols)} q2 (median quartile) columns.\")\n",
    "\n",
    "# Compute median transit depth per planet (median across 283 wavelengths)\n",
    "q2_values = df_q[q2_cols].values.astype(float)\n",
    "median_depth_per_planet = np.nanmedian(q2_values, axis=1)\n",
    "\n",
    "overall_median = np.nanmedian(median_depth_per_planet)\n",
    "overall_std = np.nanstd(median_depth_per_planet)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(median_depth_per_planet, bins=60, color=\"steelblue\", edgecolor=\"white\",\n",
    "        linewidth=0.3, alpha=0.85, label=\"Median transit depth\")\n",
    "\n",
    "ax.axvline(overall_median, color=\"red\", linestyle=\"--\", lw=2, label=f\"Median = {overall_median:.4f}\")\n",
    "ax.axvline(overall_median - overall_std, color=\"orange\", linestyle=\":\", lw=1.5,\n",
    "           label=f\"±1 std = {overall_std:.4f}\")\n",
    "ax.axvline(overall_median + overall_std, color=\"orange\", linestyle=\":\", lw=1.5)\n",
    "\n",
    "# Shaded region\n",
    "ax.axvspan(overall_median - overall_std, overall_median + overall_std,\n",
    "           alpha=0.1, color=\"orange\")\n",
    "\n",
    "ax.set_xlabel(\"Median q2 (transit depth proxy) across 283 wavelengths\", fontsize=11)\n",
    "ax.set_ylabel(\"Number of planets\", fontsize=11)\n",
    "ax.set_title(\n",
    "    f\"Transit Depth Distribution — {len(median_depth_per_planet)} labelled planets\\n\"\n",
    "    f\"Annotated: median = {overall_median:.4f} ± std = {overall_std:.4f}\",\n",
    "    fontsize=12\n",
    ")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Transit depth statistics across {len(median_depth_per_planet)} labelled planets:\")\n",
    "print(f\"  Median  : {overall_median:.6f}\")\n",
    "print(f\"  Std     : {overall_std:.6f}\")\n",
    "print(f\"  Min     : {np.nanmin(median_depth_per_planet):.6f}\")\n",
    "print(f\"  Max     : {np.nanmax(median_depth_per_planet):.6f}\")\n",
    "print(f\"  p5      : {np.nanpercentile(median_depth_per_planet, 5):.6f}\")\n",
    "print(f\"  p95     : {np.nanpercentile(median_depth_per_planet, 95):.6f}\")\n",
    "print(f\"\\n[Summary] Median transit depth = {overall_median:.4f} ± {overall_std:.4f} across \"\n",
    "      f\"{len(median_depth_per_planet)} labelled planets; range \"\n",
    "      f\"[{np.nanmin(median_depth_per_planet):.4f}, {np.nanmax(median_depth_per_planet):.4f}].\")"
   ],
   "id": "cell-transit-depth"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Label Correlation Heatmap"
   ],
   "id": "cell-markdown-corr-heatmap",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation matrix of q2 columns (283 wavelengths)\n",
    "# Subsample every 10th wavelength for readability → ~28 channels\n",
    "\n",
    "SUBSAMPLE_STEP = 10  # use every 10th wavelength\n",
    "q2_sub_cols = q2_cols[::SUBSAMPLE_STEP]\n",
    "\n",
    "print(f\"Subsampling from {len(q2_cols)} to {len(q2_sub_cols)} wavelength channels \"\n",
    "      f\"(every {SUBSAMPLE_STEP}th channel).\")\n",
    "\n",
    "# Compute Pearson correlation\n",
    "df_q2_sub = df_q[q2_sub_cols].copy()\n",
    "\n",
    "# Create short labels: wavelength index\n",
    "short_labels = [str(i * SUBSAMPLE_STEP) for i in range(len(q2_sub_cols))]\n",
    "df_q2_sub.columns = short_labels\n",
    "\n",
    "corr_matrix = df_q2_sub.corr(method=\"pearson\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "# Use seaborn heatmap\n",
    "mask = None  # show full matrix\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    ax=ax,\n",
    "    cmap=\"RdBu_r\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"},\n",
    "    annot=False,  # too many cells for annotation\n",
    "    xticklabels=short_labels,\n",
    "    yticklabels=short_labels,\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    f\"Label Correlation Heatmap — q2 (median quartile) across subsampled wavelengths\\n\"\n",
    "    f\"Showing {len(q2_sub_cols)} of {len(q2_cols)} channels (every {SUBSAMPLE_STEP}th), \"\n",
    "    f\"{len(df_q)} labelled planets\",\n",
    "    fontsize=11\n",
    ")\n",
    "ax.set_xlabel(\"Wavelength channel index\", fontsize=10)\n",
    "ax.set_ylabel(\"Wavelength channel index\", fontsize=10)\n",
    "ax.tick_params(axis=\"x\", rotation=45, labelsize=7)\n",
    "ax.tick_params(axis=\"y\", rotation=0, labelsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find pairs with high correlation\n",
    "corr_vals = corr_matrix.values\n",
    "np.fill_diagonal(corr_vals, np.nan)  # exclude self-correlation\n",
    "max_corr = np.nanmax(np.abs(corr_vals))\n",
    "mean_corr = np.nanmean(np.abs(corr_vals))\n",
    "\n",
    "print(f\"Correlation statistics (off-diagonal):\")\n",
    "print(f\"  Max |Pearson r| : {max_corr:.4f}\")\n",
    "print(f\"  Mean |Pearson r|: {mean_corr:.4f}\")\n",
    "print(f\"  Highly correlated pairs (|r| > 0.9): \"\n",
    "      f\"{(np.abs(corr_vals) > 0.9).sum() // 2}\")\n",
    "\n",
    "print(f\"\\n[Summary] Q2 label correlation heatmap: mean |r| = {mean_corr:.3f}, \"\n",
    "      f\"max |r| = {max_corr:.3f} among {len(q2_sub_cols)} subsampled wavelength channels.\")"
   ],
   "id": "cell-corr-heatmap"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "### Key findings from this EDA:\n",
    "\n",
    "- **Dataset scale**: The competition dataset is ~180 GB, consisting of HDF5 files containing per-planet time-series photometry from two instruments (AIRS-CH0 and FGS1) plus two CSV tables.\n",
    "\n",
    "- **Labels**: Only ~24% of planets are labelled with quartile transit depths (q1/q2/q3) across 283 wavelength bins. The remaining ~76% form the test set where predictions must be submitted.\n",
    "\n",
    "- **Typical transit depth range**: The median transit depth (q2 column) across labelled planets falls in a characteristic range (see Section 8 histogram); the std is comparable in magnitude, indicating substantial planet-to-planet variation in atmospheric absorption signatures.\n",
    "\n",
    "- **Auxiliary features**: Nine stellar/planetary parameters are available (e.g., stellar radius, temperature, planet radius, orbital period). Some span several orders of magnitude and benefit from log-scale visualisation. No missing values are expected in the auxiliary table.\n",
    "\n",
    "- **AIRS-CH0 structure**: Each planet's light curve is a 2-D matrix of shape (time_steps × 356 wavelength channels), covering 1.95–3.90 µm. The white-light curve shows a clear transit dip. The 2-D heatmap reveals wavelength-dependent flux variations corresponding to atmospheric absorption lines.\n",
    "\n",
    "- **FGS1 structure**: Single-channel visible photometry provides a complementary broadband transit signal. It serves as a cross-check and may help normalise systematics.\n",
    "\n",
    "- **Label correlation**: Adjacent wavelength channels in q2 are highly correlated (Pearson r typically > 0.9 for neighbouring channels), consistent with smooth atmospheric spectra. Distant channels decorrelate, especially across molecular absorption band boundaries.\n",
    "\n",
    "- **Data quality**: No obvious NaN issues in the CSVs. HDF5 key names should be verified on Kaggle (see `TODO` comments). The flux matrices may contain instrument noise/systematics that will need careful preprocessing before model training.\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "1. Verify HDF5 key names by running `explore_data.py` or inspecting `list(f.keys())` on Kaggle.\n",
    "2. Compute per-planet baseline features: white-light transit depth, ingress/egress timing, stellar limb darkening.\n",
    "3. Extract AIRS-CH0 spectral features: per-channel transit depth, SNR per channel.\n",
    "4. Build a baseline LightGBM/XGBoost model using auxiliary + simple spectral features.\n",
    "5. Explore GP-based light-curve detrending and physics-informed feature engineering."
   ],
   "id": "cell-markdown-summary",
   "outputs": []
  }
 ]
}
