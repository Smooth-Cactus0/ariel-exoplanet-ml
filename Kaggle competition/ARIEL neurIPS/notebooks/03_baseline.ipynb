{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-title",
   "metadata": {},
   "source": [
    "# NeurIPS 2024 Ariel Data Challenge — Baseline Models\n",
    "\n",
    "**Goal**: Establish quantitative lower bounds on predictive performance using simple, interpretable baselines before training neural or tree-based models.\n",
    "\n",
    "**Baselines implemented**:\n",
    "1. **Constant predictor** — predict the training-set median for every planet and wavelength.\n",
    "2. **Per-wavelength Ridge regression** — fit 283 independent Ridge regressors on the 9 auxiliary features.\n",
    "3. **Sigma sensitivity analysis** — demonstrate how miscalibrated uncertainty degrades GLL even with perfect mean predictions.\n",
    "\n",
    "**Metric**: Gaussian Log-Likelihood (GLL).  Higher is better.  A perfect prediction scores 0.\n",
    "\n",
    "$$\\text{GLL}(y, \\mu, \\sigma) = -\\frac{1}{2} \\mathbb{E}\\left[\\log(2\\pi\\sigma^2) + \\left(\\frac{y - \\mu}{\\sigma}\\right)^2\\right]$$\n",
    "\n",
    "> **Note**: This notebook is Kaggle-ready. Run it as a Kaggle kernel with the `ariel-data-challenge-2024` dataset attached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-setup",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Kaggle: clone repo and add to sys.path ─────────────────────────────────\n",
    "# Uncomment the block below and replace YOUR_GITHUB_USERNAME once the repo\n",
    "# has been pushed to GitHub:\n",
    "#\n",
    "# import subprocess\n",
    "# subprocess.run(\n",
    "#     [\"git\", \"clone\",\n",
    "#      \"https://github.com/YOUR_GITHUB_USERNAME/ariel-exoplanet-ml.git\",\n",
    "#      \"/kaggle/working/ariel-exoplanet-ml\"],\n",
    "#     check=True\n",
    "# )\n",
    "\n",
    "sys.path.insert(0, \"/kaggle/working/ariel-exoplanet-ml\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ── Data root ──────────────────────────────────────────────────────────────\n",
    "DATA_ROOT = Path(\"/kaggle/input/ariel-data-challenge-2024\")\n",
    "\n",
    "# ── Plot style ─────────────────────────────────────────────────────────────\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 110,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "# ── Constants ──────────────────────────────────────────────────────────────\n",
    "N_WAVELENGTHS = 283   # competition output dimension\n",
    "RANDOM_STATE  = 42\n",
    "\n",
    "print(f\"Python      : {sys.version.split()[0]}\")\n",
    "print(f\"NumPy       : {np.__version__}\")\n",
    "print(f\"Pandas      : {pd.__version__}\")\n",
    "print(f\"Matplotlib  : {matplotlib.__version__}\")\n",
    "print(f\"DATA_ROOT   : {DATA_ROOT}\")\n",
    "print(f\"Exists      : {DATA_ROOT.exists()}\")\n",
    "print(\"[Done] Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-load",
   "metadata": {},
   "source": [
    "## 2. Load Labels and Auxiliary Features\n",
    "\n",
    "- `AuxillaryTable.csv` — 9 stellar/planetary parameters per planet (all planets, labelled + unlabelled).\n",
    "- `QuartilesTable.csv` — quartile labels `{i}_q1`, `{i}_q2`, `{i}_q3` for i in 0..282 (labelled planets only, ≈24%).\n",
    "\n",
    "We merge the two tables on planet ID and retain only the labelled planets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Attempt to load real data; fall back to synthetic if not found ──────────\n",
    "_USING_SYNTHETIC = False\n",
    "\n",
    "aux_path = DATA_ROOT / \"AuxillaryTable.csv\"\n",
    "q_path   = DATA_ROOT / \"QuartilesTable.csv\"\n",
    "\n",
    "if aux_path.exists() and q_path.exists():\n",
    "    print(\"Loading real CSV files...\")\n",
    "    df_aux = pd.read_csv(aux_path)\n",
    "    df_q   = pd.read_csv(q_path)\n",
    "    print(f\"  AuxillaryTable : {df_aux.shape}\")\n",
    "    print(f\"  QuartilesTable : {df_q.shape}\")\n",
    "else:\n",
    "    print(\"WARNING: CSV files not found. Generating SYNTHETIC fallback data.\")\n",
    "    _USING_SYNTHETIC = True\n",
    "\n",
    "    rng_synth = np.random.default_rng(RANDOM_STATE)\n",
    "    N_PLANETS_ALL      = 400\n",
    "    N_PLANETS_LABELLED = 100\n",
    "    N_AUX_FEATURES     = 9\n",
    "\n",
    "    # Synthetic planet IDs\n",
    "    all_planet_ids = np.arange(N_PLANETS_ALL)\n",
    "\n",
    "    # Auxiliary features: stellar radius, temperature, mass, log-g,\n",
    "    #                     planet radius, mass, period, semi-major axis, eccentricity\n",
    "    aux_feature_names = [\n",
    "        \"star_radius_rsun\", \"star_temp_k\", \"star_mass_msun\", \"star_logg\",\n",
    "        \"planet_radius_rearth\", \"planet_mass_mearth\", \"orbital_period_days\",\n",
    "        \"semi_major_axis_au\", \"eccentricity\"\n",
    "    ]\n",
    "    aux_data = rng_synth.normal(0, 1, size=(N_PLANETS_ALL, N_AUX_FEATURES))\n",
    "\n",
    "    df_aux = pd.DataFrame(aux_data, columns=aux_feature_names)\n",
    "    df_aux.insert(0, \"planet_id\", all_planet_ids)\n",
    "\n",
    "    # Synthetic quartiles for first N_PLANETS_LABELLED planets\n",
    "    labelled_ids = all_planet_ids[:N_PLANETS_LABELLED]\n",
    "    q2_base  = rng_synth.normal(0.01, 0.003, size=(N_PLANETS_LABELLED, N_WAVELENGTHS))\n",
    "    iqr_vals = np.abs(rng_synth.normal(0.002, 0.0005, size=(N_PLANETS_LABELLED, N_WAVELENGTHS)))\n",
    "\n",
    "    q_cols_dict = {\"planet_id\": labelled_ids}\n",
    "    for i in range(N_WAVELENGTHS):\n",
    "        q_cols_dict[f\"{i}_q1\"] = q2_base[:, i] - iqr_vals[:, i]\n",
    "        q_cols_dict[f\"{i}_q2\"] = q2_base[:, i]\n",
    "        q_cols_dict[f\"{i}_q3\"] = q2_base[:, i] + iqr_vals[:, i]\n",
    "\n",
    "    df_q = pd.DataFrame(q_cols_dict)\n",
    "    print(f\"  Synthetic AuxillaryTable : {df_aux.shape}\")\n",
    "    print(f\"  Synthetic QuartilesTable : {df_q.shape}\")\n",
    "\n",
    "# ── Identify planet ID column in each table ────────────────────────────────\n",
    "# The planet ID is the first column in both tables.\n",
    "aux_id_col = df_aux.columns[0]\n",
    "q_id_col   = df_q.columns[0]\n",
    "\n",
    "print(f\"\\nAux ID column    : '{aux_id_col}'\")\n",
    "print(f\"Quartile ID col  : '{q_id_col}'\")\n",
    "\n",
    "# Normalise planet ID types to strings for safe merging\n",
    "df_aux[aux_id_col] = df_aux[aux_id_col].astype(str)\n",
    "df_q[q_id_col]     = df_q[q_id_col].astype(str)\n",
    "\n",
    "# ── Merge: keep only labelled planets ─────────────────────────────────────\n",
    "df_merged = pd.merge(\n",
    "    df_q,\n",
    "    df_aux,\n",
    "    left_on=q_id_col,\n",
    "    right_on=aux_id_col,\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_q\", \"_aux\"),\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged labelled set shape : {df_merged.shape}\")\n",
    "print(f\"  Labelled planets  : {len(df_merged)}\")\n",
    "print(f\"  Total columns     : {df_merged.shape[1]}\")\n",
    "\n",
    "# ── Extract q1 / q2 / q3 column names ─────────────────────────────────────\n",
    "q2_cols = [f\"{i}_q2\" for i in range(N_WAVELENGTHS)]\n",
    "q1_cols = [f\"{i}_q1\" for i in range(N_WAVELENGTHS)]\n",
    "q3_cols = [f\"{i}_q3\" for i in range(N_WAVELENGTHS)]\n",
    "\n",
    "# Verify the columns exist; if the naming scheme differs, detect automatically\n",
    "missing_q2 = [c for c in q2_cols if c not in df_merged.columns]\n",
    "if missing_q2:\n",
    "    # Try alternate naming: look for any column containing 'q2'\n",
    "    q2_cols_alt = [c for c in df_merged.columns if \"_q2\" in c or \"q2_\" in c]\n",
    "    if len(q2_cols_alt) == N_WAVELENGTHS:\n",
    "        q2_cols = sorted(q2_cols_alt, key=lambda c: int(c.split(\"_q2\")[0].split(\"q2_\")[-1]))\n",
    "        print(f\"Auto-detected q2 columns via alternate naming: first={q2_cols[0]}, last={q2_cols[-1]}\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Could not find {N_WAVELENGTHS} q2 columns. Found {len(q2_cols_alt)}: {q2_cols_alt[:5]}...\"\n",
    "        )\n",
    "\n",
    "# ── Extract numpy arrays ───────────────────────────────────────────────────\n",
    "Y_q1 = df_merged[q1_cols].values.astype(np.float64)   # (n_planets, 283)\n",
    "Y_q2 = df_merged[q2_cols].values.astype(np.float64)   # (n_planets, 283)\n",
    "Y_q3 = df_merged[q3_cols].values.astype(np.float64)   # (n_planets, 283)\n",
    "\n",
    "# Auxiliary feature columns (numeric, excluding ID columns)\n",
    "id_cols_set = {aux_id_col, q_id_col,\n",
    "               aux_id_col + \"_aux\", q_id_col + \"_q\"}\n",
    "aux_feat_cols = [\n",
    "    c for c in df_merged.columns\n",
    "    if c not in id_cols_set\n",
    "    and c not in q1_cols and c not in q2_cols and c not in q3_cols\n",
    "    and pd.api.types.is_numeric_dtype(df_merged[c])\n",
    "]\n",
    "X_aux = df_merged[aux_feat_cols].values.astype(np.float64)  # (n_planets, 9)\n",
    "\n",
    "print(f\"\\nY_q2  shape : {Y_q2.shape}  (labelled planets x wavelengths)\")\n",
    "print(f\"X_aux shape : {X_aux.shape}  (labelled planets x aux features)\")\n",
    "print(f\"Aux features ({len(aux_feat_cols)}): {aux_feat_cols}\")\n",
    "print(\"[Done] Labels and auxiliary features loaded and merged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-gll",
   "metadata": {},
   "source": [
    "## Gaussian Log-Likelihood (GLL) — Implementation\n",
    "\n",
    "The competition metric is:\n",
    "\n",
    "$$\\text{GLL}(y, \\mu, \\sigma) = -\\frac{1}{2} \\operatorname{mean}\\!\\left[\\log(2\\pi\\sigma^2) + \\left(\\frac{y-\\mu}{\\sigma}\\right)^2\\right]$$\n",
    "\n",
    "- **Higher is better**; a perfect predictor (y = μ, σ → 0 correctly calibrated) gives 0.\n",
    "- Both **mean accuracy** (term 2) and **uncertainty calibration** (term 1) matter.\n",
    "- Overconfident predictions (σ too small) are penalised by the squared residual blowing up.\n",
    "- Underconfident predictions (σ too large) are penalised by the log(σ²) term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(\n",
    "    y: np.ndarray,\n",
    "    mu: np.ndarray,\n",
    "    sigma: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the competition Gaussian Log-Likelihood score.\n",
    "\n",
    "    GLL = -0.5 * mean( log(2*pi*sigma^2) + ((y - mu) / sigma)^2 )\n",
    "\n",
    "    Higher is better.  Perfect prediction = 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y     : (...,) ground truth values\n",
    "    mu    : (...,) predicted means (same shape as y)\n",
    "    sigma : (...,) predicted stds  (must be positive; clipped at 1e-9)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float  — GLL score\n",
    "    \"\"\"\n",
    "    sigma = np.clip(sigma, 1e-9, None)\n",
    "    term1 = np.log(2.0 * np.pi * sigma ** 2)\n",
    "    term2 = ((y - mu) / sigma) ** 2\n",
    "    return float(-0.5 * np.mean(term1 + term2))\n",
    "\n",
    "\n",
    "# ── Quick sanity checks ────────────────────────────────────────────────────\n",
    "y_test    = np.array([1.0, 2.0, 3.0])\n",
    "mu_test   = np.array([1.0, 2.0, 3.0])\n",
    "sig_test  = np.array([0.1, 0.1, 0.1])\n",
    "\n",
    "gll_perfect_mean = gaussian_log_likelihood(y_test, mu_test, sig_test)\n",
    "gll_scipy = float(np.mean(scipy.stats.norm.logpdf(y_test, loc=mu_test, scale=sig_test)))\n",
    "\n",
    "print(f\"GLL (perfect mean, sigma=0.1)  : {gll_perfect_mean:.6f}\")\n",
    "print(f\"scipy reference                : {gll_scipy:.6f}\")\n",
    "print(f\"Difference vs scipy            : {abs(gll_perfect_mean - gll_scipy):.2e}  (should be <1e-10)\")\n",
    "\n",
    "# Verify: small sigma → GLL near 0 when prediction is exact\n",
    "sigma_tiny = np.full_like(y_test, 1e-6)\n",
    "gll_tiny = gaussian_log_likelihood(y_test, mu_test, sigma_tiny)\n",
    "print(f\"GLL (perfect mean, sigma=1e-6) : {gll_tiny:.4f}  (near 0 expected)\")\n",
    "\n",
    "# Verify: large sigma → strongly negative GLL\n",
    "sigma_large = np.full_like(y_test, 100.0)\n",
    "gll_large = gaussian_log_likelihood(y_test, mu_test, sigma_large)\n",
    "print(f\"GLL (perfect mean, sigma=100)  : {gll_large:.4f}  (strongly negative expected)\")\n",
    "\n",
    "print(\"[Done] GLL function implemented and verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-baseline1",
   "metadata": {},
   "source": [
    "## 3. Baseline 1 — Constant Predictor (Training-Set Median)\n",
    "\n",
    "The simplest possible baseline: ignore auxiliary features entirely and predict the training-set\n",
    "median for every planet.\n",
    "\n",
    "- **μ[λ]** = median of q2 values over all training planets at wavelength λ\n",
    "- **σ[λ]** = 0.5 × median(q3 − q1) at wavelength λ  (IQR-based robust std estimate)\n",
    "\n",
    "This establishes the floor that *any* model must beat: if a model cannot outperform a global\n",
    "constant, it has learned nothing from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-baseline1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Train / held-out 80/20 split ───────────────────────────────────────────\n",
    "n_planets = Y_q2.shape[0]\n",
    "(\n",
    "    X_train, X_val,\n",
    "    Y_q1_train, Y_q1_val,\n",
    "    Y_q2_train, Y_q2_val,\n",
    "    Y_q3_train, Y_q3_val,\n",
    ") = train_test_split(\n",
    "    X_aux,\n",
    "    Y_q1, Y_q2, Y_q3,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(f\"Train split : {len(X_train)} planets\")\n",
    "print(f\"Val   split : {len(X_val)} planets\")\n",
    "\n",
    "# ── Compute constant predictions from training set ─────────────────────────\n",
    "# mu[λ]    = median of q2 over training planets\n",
    "mu_const    = np.median(Y_q2_train, axis=0)     # (283,)\n",
    "\n",
    "# sigma[λ] = 0.5 * median(q3 - q1) over training planets  (robust IQR-based std)\n",
    "iqr_train   = Y_q3_train - Y_q1_train           # (n_train, 283)\n",
    "sigma_const = 0.5 * np.median(iqr_train, axis=0)  # (283,)\n",
    "sigma_const = np.clip(sigma_const, 1e-9, None)   # guard against zero\n",
    "\n",
    "print(f\"\\nConstant predictor:\")\n",
    "print(f\"  mu    : mean={mu_const.mean():.6f},  std={mu_const.std():.6f}\")\n",
    "print(f\"  sigma : mean={sigma_const.mean():.6f}, std={sigma_const.std():.6f}\")\n",
    "\n",
    "# ── Evaluate on held-out validation set ───────────────────────────────────\n",
    "# Broadcast constant predictions to all validation planets\n",
    "n_val = len(Y_q2_val)\n",
    "mu_val_const    = np.tile(mu_const,    (n_val, 1))   # (n_val, 283)\n",
    "sigma_val_const = np.tile(sigma_const, (n_val, 1))   # (n_val, 283)\n",
    "\n",
    "gll_const = gaussian_log_likelihood(Y_q2_val, mu_val_const, sigma_val_const)\n",
    "\n",
    "print(f\"\\nBaseline (constant median) GLL = {gll_const:.4f}\")\n",
    "print(\"[Done] Baseline 1 (constant predictor) evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-baseline2",
   "metadata": {},
   "source": [
    "## 4. Baseline 2 — Per-Wavelength Ridge Regression on Aux Features\n",
    "\n",
    "Fit 283 independent Ridge regressors (one per wavelength channel), each using the 9 standardised\n",
    "auxiliary features to predict q2 at that wavelength.\n",
    "\n",
    "- **Features** (`X`): 9 aux parameters, standardised via `StandardScaler`.\n",
    "- **Target** (`y`): q2 at each wavelength.\n",
    "- **Sigma**: estimated from the standard deviation of in-fold training residuals.\n",
    "- **Evaluation**: 5-fold cross-validation → mean GLL across all folds and wavelengths.\n",
    "\n",
    "This tests whether the auxiliary features carry *any* predictive signal about the spectrum shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-baseline2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "ALPHA       = 1.0    # Ridge regularisation strength\n",
    "N_FOLDS     = 5\n",
    "\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Collect per-fold GLL scores\n",
    "fold_gll_scores = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_aux)):\n",
    "    X_tr, X_vl  = X_aux[train_idx], X_aux[val_idx]\n",
    "    Y_tr, Y_vl  = Y_q2[train_idx],  Y_q2[val_idx]\n",
    "\n",
    "    # Standardise features\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_s = scaler.fit_transform(X_tr)\n",
    "    X_vl_s = scaler.transform(X_vl)\n",
    "\n",
    "    # Fit 283 Ridge regressors in one shot using multi-output Ridge\n",
    "    # (Ridge natively supports multi-output when y is 2-D)\n",
    "    ridge = Ridge(alpha=ALPHA, fit_intercept=True)\n",
    "    ridge.fit(X_tr_s, Y_tr)\n",
    "\n",
    "    # Predict on validation fold\n",
    "    mu_pred  = ridge.predict(X_vl_s)   # (n_val, 283)\n",
    "\n",
    "    # Estimate sigma as std of in-fold TRAINING residuals (per wavelength)\n",
    "    resid_tr = Y_tr - ridge.predict(X_tr_s)   # (n_train, 283)\n",
    "    sigma_wl = resid_tr.std(axis=0)            # (283,)  — one sigma per wavelength\n",
    "    sigma_wl = np.clip(sigma_wl, 1e-9, None)\n",
    "\n",
    "    # Broadcast sigma to validation set\n",
    "    n_vl = len(X_vl)\n",
    "    sigma_pred = np.tile(sigma_wl, (n_vl, 1))   # (n_val, 283)\n",
    "\n",
    "    fold_gll = gaussian_log_likelihood(Y_vl, mu_pred, sigma_pred)\n",
    "    fold_gll_scores.append(fold_gll)\n",
    "    print(f\"  Fold {fold_idx + 1}/{N_FOLDS}:  val size={n_vl:3d}  GLL={fold_gll:.4f}\")\n",
    "\n",
    "gll_ridge_cv = float(np.mean(fold_gll_scores))\n",
    "gll_ridge_std = float(np.std(fold_gll_scores))\n",
    "\n",
    "print(f\"\\nBaseline (Ridge regression) GLL (5-fold CV) = {gll_ridge_cv:.4f}  \"\n",
    "      f\"± {gll_ridge_std:.4f} (std across folds)\")\n",
    "print(\"[Done] Baseline 2 (Ridge regression) evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-sigma-analysis",
   "metadata": {},
   "source": [
    "## 5. Baseline 3 — Constant Sigma Sensitivity Analysis\n",
    "\n",
    "This experiment holds the **mean prediction fixed** at the training median (Baseline 1) and\n",
    "varies only σ across a range of IQR multiples.\n",
    "\n",
    "Key insight: **GLL is maximised at a well-calibrated σ**. Too small (overconfident) or too large\n",
    "(underconfident) both hurt.  The optimal σ equals the true standard deviation of the target\n",
    "distribution around the mean, i.e. `std(y - mu)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sigma-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── IQR multipliers to test ────────────────────────────────────────────────\n",
    "sigma_multipliers = [0.1, 0.25, 0.5, 1.0, 1.5, 2.0, 3.0, 5.0, 10.0]\n",
    "\n",
    "# sigma_const is the 0.5× IQR baseline computed in Section 3\n",
    "# We use the same held-out val set (Y_q2_val, mu_val_const) from Baseline 1\n",
    "iqr_baseline_median = np.median(iqr_train, axis=0)   # (283,)  raw IQR median\n",
    "\n",
    "gll_by_mult = []\n",
    "for mult in sigma_multipliers:\n",
    "    sigma_test = np.clip(0.5 * mult * iqr_baseline_median, 1e-9, None)  # (283,)\n",
    "    sigma_val_test = np.tile(sigma_test, (n_val, 1))                     # (n_val, 283)\n",
    "    gll_score = gaussian_log_likelihood(Y_q2_val, mu_val_const, sigma_val_test)\n",
    "    gll_by_mult.append(gll_score)\n",
    "\n",
    "gll_by_mult = np.array(gll_by_mult)\n",
    "best_idx  = int(np.argmax(gll_by_mult))\n",
    "best_mult = sigma_multipliers[best_idx]\n",
    "best_gll  = gll_by_mult[best_idx]\n",
    "\n",
    "print(f\"{'Multiplier':>12}  {'Sigma (mean)':>14}  {'GLL':>10}\")\n",
    "print(\"-\" * 42)\n",
    "for mult, gll_s in zip(sigma_multipliers, gll_by_mult):\n",
    "    sigma_test = 0.5 * mult * iqr_baseline_median\n",
    "    mark = \" <<< BEST\" if mult == best_mult else \"\"\n",
    "    print(f\"{mult:>12.2f}  {sigma_test.mean():>14.6f}  {gll_s:>10.4f}{mark}\")\n",
    "\n",
    "# ── Plot GLL vs sigma multiplier ───────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(sigma_multipliers, gll_by_mult,\n",
    "        marker=\"o\", markersize=7, lw=2, color=\"steelblue\", label=\"GLL (constant mean)\")\n",
    "\n",
    "# Highlight optimal point\n",
    "ax.plot(best_mult, best_gll,\n",
    "        marker=\"*\", markersize=16, color=\"crimson\", zorder=5,\n",
    "        label=f\"Optimal: mult={best_mult:.2f}, GLL={best_gll:.4f}\")\n",
    "\n",
    "ax.annotate(\n",
    "    f\"Optimal σ multiplier = {best_mult:.2f}\\nGLL = {best_gll:.4f}\",\n",
    "    xy=(best_mult, best_gll),\n",
    "    xytext=(best_mult * 1.3 if best_mult < 5 else best_mult * 0.4,\n",
    "            best_gll - (gll_by_mult.max() - gll_by_mult.min()) * 0.25),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=1.5),\n",
    "    fontsize=10, color=\"crimson\",\n",
    "    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightyellow\",\n",
    "              edgecolor=\"gray\", alpha=0.9),\n",
    ")\n",
    "\n",
    "# Shade the overconfident / underconfident halves\n",
    "ax.axvspan(sigma_multipliers[0], best_mult,\n",
    "           alpha=0.06, color=\"tomato\",\n",
    "           label=\"Overconfident (σ too small)\")\n",
    "ax.axvspan(best_mult, sigma_multipliers[-1],\n",
    "           alpha=0.06, color=\"royalblue\",\n",
    "           label=\"Underconfident (σ too large)\")\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"σ multiplier  (σ = mult × 0.5 × IQR)\", fontsize=11)\n",
    "ax.set_ylabel(\"GLL score  (higher = better)\", fontsize=11)\n",
    "ax.set_title(\n",
    "    \"Uncertainty Sensitivity Analysis\\n\"\n",
    "    \"GLL vs σ multiplier with fixed constant mean (training median)\",\n",
    "    fontsize=12\n",
    ")\n",
    "ax.legend(fontsize=9, loc=\"lower right\")\n",
    "ax.set_xticks(sigma_multipliers)\n",
    "ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal σ multiplier : {best_mult:.2f}  (out of {sigma_multipliers})\")\n",
    "print(f\"Optimal GLL          : {best_gll:.4f}\")\n",
    "print(\"[Done] Sigma sensitivity analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-plot-preds",
   "metadata": {},
   "source": [
    "## 6. Plot Baseline Predictions\n",
    "\n",
    "For 3 representative held-out planets, compare the two baselines:\n",
    "- **Baseline 1** (constant median): flat spectrum, uncertainty from IQR.\n",
    "- **Baseline 2** (Ridge regression): per-wavelength prediction conditioned on aux features.\n",
    "\n",
    "Ground truth is shown as the q2 median spectrum with a shaded q1–q3 band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plot-preds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Retrain Ridge on full training set for illustration ────────────────────\n",
    "scaler_final = StandardScaler()\n",
    "X_train_s    = scaler_final.fit_transform(X_train)\n",
    "X_val_s      = scaler_final.transform(X_val)\n",
    "\n",
    "ridge_final  = Ridge(alpha=ALPHA, fit_intercept=True)\n",
    "ridge_final.fit(X_train_s, Y_q2_train)\n",
    "\n",
    "mu_ridge_val  = ridge_final.predict(X_val_s)               # (n_val, 283)\n",
    "resid_train   = Y_q2_train - ridge_final.predict(X_train_s)\n",
    "sigma_ridge_wl = np.clip(resid_train.std(axis=0), 1e-9, None)  # (283,)\n",
    "\n",
    "# ── Select 3 example planets from validation set ──────────────────────────\n",
    "N_EXAMPLE   = 3\n",
    "example_idx = np.linspace(0, len(Y_q2_val) - 1, N_EXAMPLE, dtype=int)\n",
    "\n",
    "wl_axis = np.arange(N_WAVELENGTHS)   # wavelength channel index (0..282)\n",
    "\n",
    "fig, axes = plt.subplots(N_EXAMPLE, 2, figsize=(16, 4 * N_EXAMPLE))\n",
    "fig.suptitle(\n",
    "    \"Baseline Predictions vs Ground Truth — 3 Example Validation Planets\",\n",
    "    fontsize=13, fontweight=\"bold\", y=1.01\n",
    ")\n",
    "\n",
    "for row, pid in enumerate(example_idx):\n",
    "    # Ground truth\n",
    "    gt_q1 = Y_q1_val[pid]\n",
    "    gt_q2 = Y_q2_val[pid]\n",
    "    gt_q3 = Y_q3_val[pid]\n",
    "\n",
    "    # ── Left: Baseline 1 (constant median) ────────────────────────────────\n",
    "    ax_left = axes[row, 0]\n",
    "\n",
    "    # Ground truth band\n",
    "    ax_left.fill_between(wl_axis, gt_q1, gt_q3,\n",
    "                         alpha=0.25, color=\"steelblue\", label=\"GT q1–q3 band\")\n",
    "    ax_left.plot(wl_axis, gt_q2, lw=1.2, color=\"steelblue\",\n",
    "                 label=\"GT q2 (median)\")\n",
    "\n",
    "    # Constant prediction\n",
    "    ax_left.plot(wl_axis, mu_const, lw=1.2, color=\"darkorange\", linestyle=\"--\",\n",
    "                 label=\"Pred μ (const median)\")\n",
    "    ax_left.fill_between(wl_axis,\n",
    "                         mu_const - sigma_const,\n",
    "                         mu_const + sigma_const,\n",
    "                         alpha=0.18, color=\"darkorange\", label=\"Pred μ±σ\")\n",
    "\n",
    "    ax_left.set_title(f\"Planet val[{pid}] — Baseline 1 (Constant Median)\", fontsize=10)\n",
    "    ax_left.set_xlabel(\"Wavelength channel\", fontsize=9)\n",
    "    ax_left.set_ylabel(\"Transit depth\", fontsize=9)\n",
    "    if row == 0:\n",
    "        ax_left.legend(fontsize=8, loc=\"upper right\")\n",
    "    ax_left.tick_params(labelsize=8)\n",
    "\n",
    "    # ── Right: Baseline 2 (Ridge regression) ──────────────────────────────\n",
    "    ax_right = axes[row, 1]\n",
    "\n",
    "    # Ground truth band\n",
    "    ax_right.fill_between(wl_axis, gt_q1, gt_q3,\n",
    "                          alpha=0.25, color=\"steelblue\", label=\"GT q1–q3 band\")\n",
    "    ax_right.plot(wl_axis, gt_q2, lw=1.2, color=\"steelblue\",\n",
    "                  label=\"GT q2 (median)\")\n",
    "\n",
    "    # Ridge prediction\n",
    "    mu_ridge_p = mu_ridge_val[pid]          # (283,)\n",
    "    ax_right.plot(wl_axis, mu_ridge_p, lw=1.2, color=\"crimson\", linestyle=\"--\",\n",
    "                  label=\"Pred μ (Ridge)\")\n",
    "    ax_right.fill_between(wl_axis,\n",
    "                          mu_ridge_p - sigma_ridge_wl,\n",
    "                          mu_ridge_p + sigma_ridge_wl,\n",
    "                          alpha=0.18, color=\"crimson\", label=\"Pred μ±σ\")\n",
    "\n",
    "    ax_right.set_title(f\"Planet val[{pid}] — Baseline 2 (Ridge Regression)\", fontsize=10)\n",
    "    ax_right.set_xlabel(\"Wavelength channel\", fontsize=9)\n",
    "    ax_right.set_ylabel(\"Transit depth\", fontsize=9)\n",
    "    if row == 0:\n",
    "        ax_right.legend(fontsize=8, loc=\"upper right\")\n",
    "    ax_right.tick_params(labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"[Done] Prediction plots for {N_EXAMPLE} example planets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-summary",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Results Table\n",
    "\n",
    "| Method | GLL | Notes |\n",
    "|---|---|---|\n",
    "| Baseline 1: Constant median | *see cell below* | Global median μ, IQR-based σ. No aux features used. |\n",
    "| Baseline 2: Ridge regression (5-fold CV) | *see cell below* | 283 independent Ridge models on 9 aux features. σ from training residuals. |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **GLL is sensitive to both mean accuracy and uncertainty calibration.**  \n",
    "   The sigma sensitivity analysis (Section 5) shows that even with perfect mean predictions,\n",
    "   mis-calibrated uncertainty halves or more the GLL score.  A model that outputs a good μ\n",
    "   but a bad σ will score poorly.\n",
    "\n",
    "2. **The constant predictor is a meaningful floor.**  \n",
    "   Any model that fails to beat Baseline 1 has not learned anything useful from the data.\n",
    "   Concretely, ~24% of planets are labelled, so the training-set median is computed on a\n",
    "   modest sample.  The model must generalise beyond this.\n",
    "\n",
    "3. **Auxiliary features carry some signal (Baseline 2 vs 1).**  \n",
    "   If Ridge regression outperforms the constant predictor, stellar/planetary parameters\n",
    "   (stellar temperature, planet radius, etc.) are correlated with the atmospheric spectrum.  \n",
    "   This is physically expected: hotter stars illuminate more, larger planets block more light.\n",
    "\n",
    "4. **What the full model needs to beat.**  \n",
    "   The competition-winning approaches use direct photometric extraction from AIRS-CH0 raw\n",
    "   light curves.  The HDF5 data provides per-channel flux time series that, once preprocessed\n",
    "   (see `02_preprocessing.ipynb`), yield a transit depth spectrum per planet.  This spectrum,\n",
    "   combined with aux features, is expected to drive GLL well above the baselines here.\n",
    "\n",
    "5. **GLL = 0 is unreachable in practice.**  \n",
    "   A score of 0 would require perfect mean prediction *and* σ equal to the irreducible noise.\n",
    "   Competition leaderboard scores are typically in the range −5 to −0.5; see the Kaggle\n",
    "   discussion forum for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Print final summary table ───────────────────────────────────────────────\n",
    "print(\"=\" * 65)\n",
    "print(\"BASELINE RESULTS SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Method':<42}  {'GLL':>8}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Baseline 1: Constant median (20% val split)':<42}  {gll_const:>8.4f}\")\n",
    "print(f\"{'Baseline 2: Ridge regression (5-fold CV)':<42}  {gll_ridge_cv:>8.4f}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "improvement = gll_ridge_cv - gll_const\n",
    "sign = \"+\" if improvement >= 0 else \"\"\n",
    "print(f\"Ridge vs Constant improvement : {sign}{improvement:.4f} GLL points\")\n",
    "print(f\"Using synthetic data           : {_USING_SYNTHETIC}\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(\"\\nBaseline (constant median) GLL =\", round(gll_const, 4))\n",
    "print(\"Baseline (Ridge regression) GLL (5-fold CV) =\", round(gll_ridge_cv, 4))\n",
    "\n",
    "print(\"\\n[Done] Baseline notebook complete.\")"
   ]
  }
 ]
}
