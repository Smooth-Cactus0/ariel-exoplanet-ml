{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-title",
   "metadata": {},
   "source": "# NeurIPS 2024 Ariel Data Challenge — Baseline Models\n\n**Goal**: Establish quantitative lower bounds on predictive performance using simple, interpretable baselines before training neural or tree-based models.\n\n**Baselines implemented**:\n1. **Constant predictor** — predict the training-set median for every planet and wavelength.\n2. **Per-wavelength Ridge regression** — fit 283 independent Ridge regressors on the 5 ADC calibration features.\n3. **Sigma sensitivity analysis** — demonstrate how miscalibrated uncertainty degrades GLL even with perfect mean predictions.\n\n**Metric**: Gaussian Log-Likelihood (GLL).  Higher is better.  A perfect prediction scores 0.\n\n$$\\text{GLL}(y, \\mu, \\sigma) = -\\frac{1}{2} \\mathbb{E}\\left[\\log(2\\pi\\sigma^2) + \\left(\\frac{y - \\mu}{\\sigma}\\right)^2\\right]$$\n\n> **Note**: This notebook is Kaggle-ready. Run it as a Kaggle kernel with the `ariel-data-challenge-2024` dataset attached."
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-setup",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys\nfrom pathlib import Path\n\n# ── Kaggle: clone repo and add to sys.path ─────────────────────────────────\nrepo_dir = \"/kaggle/working/ariel-exoplanet-ml\"\nproject_dir = repo_dir + \"/Kaggle competition/ARIEL neurIPS\"\n\nif not Path(repo_dir).exists():\n    subprocess.run(\n        [\"git\", \"clone\",\n         \"https://github.com/Smooth-Cactus0/ariel-exoplanet-ml.git\",\n         repo_dir],\n        check=True,\n    )\n    print(f\"Cloned repo to {repo_dir}\")\nelse:\n    print(f\"Repo already exists at {repo_dir}\")\n\nsys.path.insert(0, project_dir)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport scipy.stats\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.pipeline import Pipeline\n\n# ── Data root ──────────────────────────────────────────────────────────────\nDATA_ROOT = Path(\"/kaggle/input/ariel-data-challenge-2024\")\n\n# ── Plot style ─────────────────────────────────────────────────────────────\nplt.rcParams.update({\n    \"figure.dpi\": 110,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"savefig.dpi\": 150,\n    \"savefig.facecolor\": \"white\",\n})\n\n# ── Output directories ────────────────────────────────────────────────────\nOUT_DIR = Path(\"/kaggle/working/baseline_results\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nFIG_DIR = OUT_DIR / \"figures\"\nFIG_DIR.mkdir(parents=True, exist_ok=True)\n\n# ── Constants ──────────────────────────────────────────────────────────────\nN_WAVELENGTHS = 283   # competition output dimension\nRANDOM_STATE  = 42\n\nprint(f\"Python      : {sys.version.split()[0]}\")\nprint(f\"NumPy       : {np.__version__}\")\nprint(f\"Pandas      : {pd.__version__}\")\nprint(f\"Matplotlib  : {matplotlib.__version__}\")\nprint(f\"DATA_ROOT   : {DATA_ROOT}\")\nprint(f\"Exists      : {DATA_ROOT.exists()}\")\nprint(f\"OUT_DIR     : {OUT_DIR}\")\nprint(\"[Done] Setup complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-load",
   "metadata": {},
   "source": "## 2. Load Labels and Auxiliary Features\n\n- `train_adc_info.csv` — 5 ADC calibration features per planet: `FGS1_adc_offset`, `FGS1_adc_gain`, `AIRS-CH0_adc_offset`, `AIRS-CH0_adc_gain`, `star`.\n- `train_labels.csv` — mean transmission spectrum `wl_1` ... `wl_283` for labelled planets.\n\nWe merge the two tables on `planet_id` and retain only the labelled planets for training.  Labels provide only the **mean** spectrum (no quartiles, no sigma)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load",
   "metadata": {},
   "outputs": [],
   "source": "# ── Attempt to load real data; fall back to synthetic if not found ──────────\n_USING_SYNTHETIC = False\n\nadc_path   = DATA_ROOT / \"train_adc_info.csv\"\nlabel_path = DATA_ROOT / \"train_labels.csv\"\n\n# ADC feature columns (excluding planet_id)\nAUX_FEATURE_COLS = [\n    \"FGS1_adc_offset\", \"FGS1_adc_gain\",\n    \"AIRS-CH0_adc_offset\", \"AIRS-CH0_adc_gain\",\n    \"star\",\n]\nN_AUX_FEATURES = len(AUX_FEATURE_COLS)  # 5\n\n# Wavelength label columns\nWL_COLS = [f\"wl_{i}\" for i in range(1, N_WAVELENGTHS + 1)]\n\nif adc_path.exists() and label_path.exists():\n    print(\"Loading real CSV files...\")\n    df_adc    = pd.read_csv(adc_path)\n    df_labels = pd.read_csv(label_path)\n    print(f\"  train_adc_info : {df_adc.shape}\")\n    print(f\"  train_labels   : {df_labels.shape}\")\nelse:\n    print(\"WARNING: CSV files not found. Generating SYNTHETIC fallback data.\")\n    _USING_SYNTHETIC = True\n\n    rng_synth = np.random.default_rng(RANDOM_STATE)\n    N_PLANETS_ALL      = 400\n    N_PLANETS_LABELLED = 100\n\n    # Synthetic planet IDs\n    all_planet_ids = np.arange(N_PLANETS_ALL)\n\n    # Synthetic ADC features: 5 columns\n    aux_data = rng_synth.normal(0, 1, size=(N_PLANETS_ALL, N_AUX_FEATURES))\n    df_adc = pd.DataFrame(aux_data, columns=AUX_FEATURE_COLS)\n    df_adc.insert(0, \"planet_id\", all_planet_ids)\n\n    # Synthetic labels for first N_PLANETS_LABELLED planets (means only)\n    labelled_ids = all_planet_ids[:N_PLANETS_LABELLED]\n    wl_means = rng_synth.normal(0.01, 0.003, size=(N_PLANETS_LABELLED, N_WAVELENGTHS))\n\n    label_dict = {\"planet_id\": labelled_ids}\n    for i, col in enumerate(WL_COLS):\n        label_dict[col] = wl_means[:, i]\n\n    df_labels = pd.DataFrame(label_dict)\n    print(f\"  Synthetic train_adc_info : {df_adc.shape}\")\n    print(f\"  Synthetic train_labels   : {df_labels.shape}\")\n\n# ── Normalise planet_id types for safe merging ─────────────────────────────\ndf_adc[\"planet_id\"]    = df_adc[\"planet_id\"].astype(str)\ndf_labels[\"planet_id\"] = df_labels[\"planet_id\"].astype(str)\n\n# ── Merge: keep only labelled planets (inner join on planet_id) ────────────\ndf_merged = pd.merge(\n    df_labels,\n    df_adc,\n    on=\"planet_id\",\n    how=\"inner\",\n)\n\nprint(f\"\\nMerged labelled set shape : {df_merged.shape}\")\nprint(f\"  Labelled planets  : {len(df_merged)}\")\nprint(f\"  Total columns     : {df_merged.shape[1]}\")\n\n# ── Extract numpy arrays ───────────────────────────────────────────────────\nY_mean = df_merged[WL_COLS].values.astype(np.float64)   # (n_planets, 283)\nX_aux  = df_merged[AUX_FEATURE_COLS].values.astype(np.float64)  # (n_planets, 5)\n\nprint(f\"\\nY_mean shape : {Y_mean.shape}  (labelled planets x wavelengths)\")\nprint(f\"X_aux  shape : {X_aux.shape}  (labelled planets x aux features)\")\nprint(f\"Aux features ({N_AUX_FEATURES}): {AUX_FEATURE_COLS}\")\nprint(\"[Done] Labels and auxiliary features loaded and merged.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-gll",
   "metadata": {},
   "source": "## 3. Gaussian Log-Likelihood (GLL) — Implementation\n\nThe competition metric is:\n\n$$\\text{GLL}(y, \\mu, \\sigma) = -\\frac{1}{2} \\operatorname{mean}\\!\\left[\\log(2\\pi\\sigma^2) + \\left(\\frac{y-\\mu}{\\sigma}\\right)^2\\right]$$\n\n- **Higher is better**; a perfect predictor (y = mu, sigma calibrated) gives 0.\n- Both **mean accuracy** (term 2) and **uncertainty calibration** (term 1) matter.\n- Overconfident predictions (sigma too small) are penalised by the squared residual blowing up.\n- Underconfident predictions (sigma too large) are penalised by the log(sigma^2) term.\n\nSince `train_labels.csv` provides only the mean transmission spectrum (no quartiles or sigma), uncertainty is purely a model responsibility."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(\n",
    "    y: np.ndarray,\n",
    "    mu: np.ndarray,\n",
    "    sigma: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the competition Gaussian Log-Likelihood score.\n",
    "\n",
    "    GLL = -0.5 * mean( log(2*pi*sigma^2) + ((y - mu) / sigma)^2 )\n",
    "\n",
    "    Higher is better.  Perfect prediction = 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y     : (...,) ground truth values\n",
    "    mu    : (...,) predicted means (same shape as y)\n",
    "    sigma : (...,) predicted stds  (must be positive; clipped at 1e-9)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float  — GLL score\n",
    "    \"\"\"\n",
    "    sigma = np.clip(sigma, 1e-9, None)\n",
    "    term1 = np.log(2.0 * np.pi * sigma ** 2)\n",
    "    term2 = ((y - mu) / sigma) ** 2\n",
    "    return float(-0.5 * np.mean(term1 + term2))\n",
    "\n",
    "\n",
    "# ── Quick sanity checks ────────────────────────────────────────────────────\n",
    "y_test    = np.array([1.0, 2.0, 3.0])\n",
    "mu_test   = np.array([1.0, 2.0, 3.0])\n",
    "sig_test  = np.array([0.1, 0.1, 0.1])\n",
    "\n",
    "gll_perfect_mean = gaussian_log_likelihood(y_test, mu_test, sig_test)\n",
    "gll_scipy = float(np.mean(scipy.stats.norm.logpdf(y_test, loc=mu_test, scale=sig_test)))\n",
    "\n",
    "print(f\"GLL (perfect mean, sigma=0.1)  : {gll_perfect_mean:.6f}\")\n",
    "print(f\"scipy reference                : {gll_scipy:.6f}\")\n",
    "print(f\"Difference vs scipy            : {abs(gll_perfect_mean - gll_scipy):.2e}  (should be <1e-10)\")\n",
    "\n",
    "# Verify: small sigma → GLL near 0 when prediction is exact\n",
    "sigma_tiny = np.full_like(y_test, 1e-6)\n",
    "gll_tiny = gaussian_log_likelihood(y_test, mu_test, sigma_tiny)\n",
    "print(f\"GLL (perfect mean, sigma=1e-6) : {gll_tiny:.4f}  (near 0 expected)\")\n",
    "\n",
    "# Verify: large sigma → strongly negative GLL\n",
    "sigma_large = np.full_like(y_test, 100.0)\n",
    "gll_large = gaussian_log_likelihood(y_test, mu_test, sigma_large)\n",
    "print(f\"GLL (perfect mean, sigma=100)  : {gll_large:.4f}  (strongly negative expected)\")\n",
    "\n",
    "print(\"[Done] GLL function implemented and verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-baseline1",
   "metadata": {},
   "source": "## 4. Baseline 1 — Constant Predictor (Training-Set Median)\n\nThe simplest possible baseline: ignore auxiliary features entirely and predict the training-set\nmedian for every planet.\n\n- **mu[lambda]** = median of wl_* values over all training planets at wavelength lambda\n- **sigma[lambda]** = std of training residuals (Y_train - mu) at each wavelength\n\nSince labels provide only the mean spectrum (no quartiles/IQR), we estimate sigma from the\nstandard deviation of training residuals.\n\nThis establishes the floor that *any* model must beat: if a model cannot outperform a global\nconstant, it has learned nothing from the data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-baseline1",
   "metadata": {},
   "outputs": [],
   "source": "# ── Train / held-out 80/20 split ───────────────────────────────────────────\nn_planets = Y_mean.shape[0]\n(\n    X_train, X_val,\n    Y_train, Y_val,\n) = train_test_split(\n    X_aux,\n    Y_mean,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n\nprint(f\"Train split : {len(X_train)} planets\")\nprint(f\"Val   split : {len(X_val)} planets\")\n\n# ── Compute constant predictions from training set ─────────────────────────\n# mu[lambda] = median of wl_* values over training planets\nmu_const = np.median(Y_train, axis=0)     # (283,)\n\n# sigma[lambda] = std of training residuals (no IQR available — means only)\nresid_const_train = Y_train - mu_const[np.newaxis, :]   # (n_train, 283)\nsigma_const = resid_const_train.std(axis=0)              # (283,)\nsigma_const = np.clip(sigma_const, 1e-9, None)           # guard against zero\n\nprint(f\"\\nConstant predictor:\")\nprint(f\"  mu    : mean={mu_const.mean():.6f},  std={mu_const.std():.6f}\")\nprint(f\"  sigma : mean={sigma_const.mean():.6f}, std={sigma_const.std():.6f}\")\n\n# ── Evaluate on held-out validation set ───────────────────────────────────\n# Broadcast constant predictions to all validation planets\nn_val = len(Y_val)\nmu_val_const    = np.tile(mu_const,    (n_val, 1))   # (n_val, 283)\nsigma_val_const = np.tile(sigma_const, (n_val, 1))   # (n_val, 283)\n\ngll_const = gaussian_log_likelihood(Y_val, mu_val_const, sigma_val_const)\n\nprint(f\"\\nBaseline (constant median) GLL = {gll_const:.4f}\")\nprint(\"[Done] Baseline 1 (constant predictor) evaluated.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-baseline2",
   "metadata": {},
   "source": "## 5. Baseline 2 — Per-Wavelength Ridge Regression on ADC Features\n\nFit 283 independent Ridge regressors (one per wavelength channel), each using the 5 standardised\nADC calibration features to predict the mean transmission depth at that wavelength.\n\n- **Features** (`X`): 5 ADC parameters (`FGS1_adc_offset`, `FGS1_adc_gain`, `AIRS-CH0_adc_offset`, `AIRS-CH0_adc_gain`, `star`), standardised via `StandardScaler`.\n- **Target** (`y`): mean transmission depth at each wavelength (from `train_labels.csv`).\n- **Sigma**: estimated from the standard deviation of in-fold training residuals.\n- **Evaluation**: 5-fold cross-validation, mean GLL across all folds and wavelengths.\n\nThis tests whether the ADC calibration features carry *any* predictive signal about the spectrum shape."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-baseline2",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\n\nALPHA       = 1.0    # Ridge regularisation strength\nN_FOLDS     = 5\n\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n\n# Collect per-fold GLL scores\nfold_gll_scores = []\n\nfor fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_aux)):\n    X_tr, X_vl = X_aux[train_idx], X_aux[val_idx]\n    Y_tr, Y_vl = Y_mean[train_idx], Y_mean[val_idx]\n\n    # Standardise features\n    scaler = StandardScaler()\n    X_tr_s = scaler.fit_transform(X_tr)\n    X_vl_s = scaler.transform(X_vl)\n\n    # Fit 283 Ridge regressors in one shot using multi-output Ridge\n    # (Ridge natively supports multi-output when y is 2-D)\n    ridge = Ridge(alpha=ALPHA, fit_intercept=True)\n    ridge.fit(X_tr_s, Y_tr)\n\n    # Predict on validation fold\n    mu_pred = ridge.predict(X_vl_s)   # (n_val, 283)\n\n    # Estimate sigma as std of in-fold TRAINING residuals (per wavelength)\n    resid_tr = Y_tr - ridge.predict(X_tr_s)   # (n_train, 283)\n    sigma_wl = resid_tr.std(axis=0)            # (283,)  — one sigma per wavelength\n    sigma_wl = np.clip(sigma_wl, 1e-9, None)\n\n    # Broadcast sigma to validation set\n    n_vl = len(X_vl)\n    sigma_pred = np.tile(sigma_wl, (n_vl, 1))   # (n_val, 283)\n\n    fold_gll = gaussian_log_likelihood(Y_vl, mu_pred, sigma_pred)\n    fold_gll_scores.append(fold_gll)\n    print(f\"  Fold {fold_idx + 1}/{N_FOLDS}:  val size={n_vl:3d}  GLL={fold_gll:.4f}\")\n\ngll_ridge_cv = float(np.mean(fold_gll_scores))\ngll_ridge_std = float(np.std(fold_gll_scores))\n\nprint(f\"\\nBaseline (Ridge regression) GLL (5-fold CV) = {gll_ridge_cv:.4f}  \"\n      f\"+/- {gll_ridge_std:.4f} (std across folds)\")\nprint(\"[Done] Baseline 2 (Ridge regression) evaluated.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-sigma-analysis",
   "metadata": {},
   "source": "## 6. Sigma Sensitivity Analysis\n\nThis experiment holds the **mean prediction fixed** at the training median (Baseline 1) and\nvaries only sigma across a range of multipliers applied to the residual standard deviation.\n\nKey insight: **GLL is maximised at a well-calibrated sigma**. Too small (overconfident) or too large\n(underconfident) both hurt.  The optimal sigma equals the true standard deviation of the target\ndistribution around the mean, i.e. `std(y - mu)`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sigma-analysis",
   "metadata": {},
   "outputs": [],
   "source": "# ── Sigma multipliers to test ──────────────────────────────────────────────\nsigma_multipliers = [0.1, 0.25, 0.5, 1.0, 1.5, 2.0, 3.0, 5.0, 10.0]\n\n# sigma_const is the residual std baseline computed in Section 4\n# We use the same held-out val set (Y_val, mu_val_const) from Baseline 1\n\ngll_by_mult = []\nfor mult in sigma_multipliers:\n    sigma_test = np.clip(mult * sigma_const, 1e-9, None)     # (283,)\n    sigma_val_test = np.tile(sigma_test, (n_val, 1))          # (n_val, 283)\n    gll_score = gaussian_log_likelihood(Y_val, mu_val_const, sigma_val_test)\n    gll_by_mult.append(gll_score)\n\ngll_by_mult = np.array(gll_by_mult)\nbest_idx  = int(np.argmax(gll_by_mult))\nbest_mult = sigma_multipliers[best_idx]\nbest_gll  = gll_by_mult[best_idx]\n\nprint(f\"{'Multiplier':>12}  {'Sigma (mean)':>14}  {'GLL':>10}\")\nprint(\"-\" * 42)\nfor mult, gll_s in zip(sigma_multipliers, gll_by_mult):\n    sigma_test = mult * sigma_const\n    mark = \" <<< BEST\" if mult == best_mult else \"\"\n    print(f\"{mult:>12.2f}  {sigma_test.mean():>14.6f}  {gll_s:>10.4f}{mark}\")\n\n# ── Plot GLL vs sigma multiplier ───────────────────────────────────────────\nfig, ax = plt.subplots(figsize=(10, 5))\n\nax.plot(sigma_multipliers, gll_by_mult,\n        marker=\"o\", markersize=7, lw=2, color=\"steelblue\", label=\"GLL (constant mean)\")\n\n# Highlight optimal point\nax.plot(best_mult, best_gll,\n        marker=\"*\", markersize=16, color=\"crimson\", zorder=5,\n        label=f\"Optimal: mult={best_mult:.2f}, GLL={best_gll:.4f}\")\n\nax.annotate(\n    f\"Optimal sigma multiplier = {best_mult:.2f}\\nGLL = {best_gll:.4f}\",\n    xy=(best_mult, best_gll),\n    xytext=(best_mult * 1.3 if best_mult < 5 else best_mult * 0.4,\n            best_gll - (gll_by_mult.max() - gll_by_mult.min()) * 0.25),\n    arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=1.5),\n    fontsize=10, color=\"crimson\",\n    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightyellow\",\n              edgecolor=\"gray\", alpha=0.9),\n)\n\n# Shade the overconfident / underconfident halves\nax.axvspan(sigma_multipliers[0], best_mult,\n           alpha=0.06, color=\"tomato\",\n           label=\"Overconfident (sigma too small)\")\nax.axvspan(best_mult, sigma_multipliers[-1],\n           alpha=0.06, color=\"royalblue\",\n           label=\"Underconfident (sigma too large)\")\n\nax.set_xscale(\"log\")\nax.set_xlabel(\"sigma multiplier  (sigma = mult * residual_std)\", fontsize=11)\nax.set_ylabel(\"GLL score  (higher = better)\", fontsize=11)\nax.set_title(\n    \"Uncertainty Sensitivity Analysis\\n\"\n    \"GLL vs sigma multiplier with fixed constant mean (training median)\",\n    fontsize=12\n)\nax.legend(fontsize=9, loc=\"lower right\")\nax.set_xticks(sigma_multipliers)\nax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n\nplt.tight_layout()\nfig.savefig(FIG_DIR / \"sigma_sensitivity.png\", bbox_inches=\"tight\")\nplt.show()\n\nprint(f\"\\nOptimal sigma multiplier : {best_mult:.2f}  (out of {sigma_multipliers})\")\nprint(f\"Optimal GLL              : {best_gll:.4f}\")\nprint(\"[Done] Sigma sensitivity analysis complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-plot-preds",
   "metadata": {},
   "source": "## 7. Plot Baseline Predictions\n\nFor 3 representative held-out planets, compare the two baselines:\n- **Baseline 1** (constant median): flat spectrum, uncertainty from residual std.\n- **Baseline 2** (Ridge regression): per-wavelength prediction conditioned on ADC features.\n\nGround truth is shown as the mean transmission spectrum (points).  Predicted uncertainty is shown as a shaded mu +/- sigma band."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plot-preds",
   "metadata": {},
   "outputs": [],
   "source": "# ── Retrain Ridge on full training set for illustration ────────────────────\nscaler_final = StandardScaler()\nX_train_s    = scaler_final.fit_transform(X_train)\nX_val_s      = scaler_final.transform(X_val)\n\nridge_final  = Ridge(alpha=ALPHA, fit_intercept=True)\nridge_final.fit(X_train_s, Y_train)\n\nmu_ridge_val   = ridge_final.predict(X_val_s)               # (n_val, 283)\nresid_train    = Y_train - ridge_final.predict(X_train_s)\nsigma_ridge_wl = np.clip(resid_train.std(axis=0), 1e-9, None)  # (283,)\n\n# ── Select 3 example planets from validation set ──────────────────────────\nN_EXAMPLE   = 3\nexample_idx = np.linspace(0, len(Y_val) - 1, N_EXAMPLE, dtype=int)\n\nwl_axis = np.arange(N_WAVELENGTHS)   # wavelength channel index (0..282)\n\nfig, axes = plt.subplots(N_EXAMPLE, 2, figsize=(16, 4 * N_EXAMPLE))\nfig.suptitle(\n    \"Baseline Predictions vs Ground Truth — 3 Example Validation Planets\",\n    fontsize=13, fontweight=\"bold\", y=1.01\n)\n\nfor row, pid in enumerate(example_idx):\n    # Ground truth (mean spectrum only — no quartile bands)\n    gt_mean = Y_val[pid]\n\n    # ── Left: Baseline 1 (constant median) ────────────────────────────────\n    ax_left = axes[row, 0]\n\n    # Ground truth as scatter points\n    ax_left.scatter(wl_axis, gt_mean, s=4, color=\"steelblue\", alpha=0.7,\n                    label=\"GT mean spectrum\", zorder=3)\n\n    # Constant prediction\n    ax_left.plot(wl_axis, mu_const, lw=1.2, color=\"darkorange\", linestyle=\"--\",\n                 label=\"Pred mu (const median)\")\n    ax_left.fill_between(wl_axis,\n                         mu_const - sigma_const,\n                         mu_const + sigma_const,\n                         alpha=0.18, color=\"darkorange\", label=\"Pred mu +/- sigma\")\n\n    ax_left.set_title(f\"Planet val[{pid}] — Baseline 1 (Constant Median)\", fontsize=10)\n    ax_left.set_xlabel(\"Wavelength channel\", fontsize=9)\n    ax_left.set_ylabel(\"Transit depth\", fontsize=9)\n    if row == 0:\n        ax_left.legend(fontsize=8, loc=\"upper right\")\n    ax_left.tick_params(labelsize=8)\n\n    # ── Right: Baseline 2 (Ridge regression) ──────────────────────────────\n    ax_right = axes[row, 1]\n\n    # Ground truth as scatter points\n    ax_right.scatter(wl_axis, gt_mean, s=4, color=\"steelblue\", alpha=0.7,\n                     label=\"GT mean spectrum\", zorder=3)\n\n    # Ridge prediction\n    mu_ridge_p = mu_ridge_val[pid]          # (283,)\n    ax_right.plot(wl_axis, mu_ridge_p, lw=1.2, color=\"crimson\", linestyle=\"--\",\n                  label=\"Pred mu (Ridge)\")\n    ax_right.fill_between(wl_axis,\n                          mu_ridge_p - sigma_ridge_wl,\n                          mu_ridge_p + sigma_ridge_wl,\n                          alpha=0.18, color=\"crimson\", label=\"Pred mu +/- sigma\")\n\n    ax_right.set_title(f\"Planet val[{pid}] — Baseline 2 (Ridge Regression)\", fontsize=10)\n    ax_right.set_xlabel(\"Wavelength channel\", fontsize=9)\n    ax_right.set_ylabel(\"Transit depth\", fontsize=9)\n    if row == 0:\n        ax_right.legend(fontsize=8, loc=\"upper right\")\n    ax_right.tick_params(labelsize=8)\n\nplt.tight_layout()\nfig.savefig(FIG_DIR / \"baseline_predictions.png\", bbox_inches=\"tight\")\nplt.show()\n\nprint(f\"[Done] Prediction plots for {N_EXAMPLE} example planets.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-summary",
   "metadata": {},
   "source": "## 8. Summary\n\n### Results Table\n\n| Method | GLL | Notes |\n|---|---|---|\n| Baseline 1: Constant median | *see cell below* | Global median mu, residual-std sigma. No ADC features used. |\n| Baseline 2: Ridge regression (5-fold CV) | *see cell below* | 283 independent Ridge models on 5 ADC features. Sigma from training residuals. |\n\n### Key Takeaways\n\n1. **GLL is sensitive to both mean accuracy and uncertainty calibration.**  \n   The sigma sensitivity analysis (Section 6) shows that even with perfect mean predictions,\n   mis-calibrated uncertainty halves or more the GLL score.  A model that outputs a good mu\n   but a bad sigma will score poorly.\n\n2. **The constant predictor is a meaningful floor.**  \n   Any model that fails to beat Baseline 1 has not learned anything useful from the data.\n   The training-set median is computed on the labelled subset.  The model must generalise\n   beyond this.\n\n3. **ADC calibration features may carry some signal (Baseline 2 vs 1).**  \n   If Ridge regression outperforms the constant predictor, the ADC offset/gain parameters\n   and star type are correlated with the atmospheric spectrum.  The ADC features describe\n   the detector calibration (`FGS1_adc_offset`, `FGS1_adc_gain`, `AIRS-CH0_adc_offset`,\n   `AIRS-CH0_adc_gain`) and whether the star is in the training set (`star`).\n\n4. **Labels provide only the mean transmission spectrum.**  \n   Unlike a quartile-based setup, there are no q1/q3 bands to estimate uncertainty from\n   the labels.  Sigma is purely a model responsibility.  This makes uncertainty calibration\n   a key differentiator between models.\n\n5. **What the full model needs to beat.**  \n   The competition-winning approaches use direct photometric extraction from AIRS-CH0 raw\n   light curves.  The per-planet parquet data provides per-channel flux time series that,\n   once preprocessed (see `02_preprocessing.ipynb`), yield a transit depth spectrum per planet.\n   This spectrum, combined with ADC features, is expected to drive GLL well above the baselines here.\n\n6. **GLL = 0 is unreachable in practice.**  \n   A score of 0 would require perfect mean prediction *and* sigma equal to the irreducible noise.\n   Competition leaderboard scores are typically in the range -5 to -0.5; see the Kaggle\n   discussion forum for context."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary-print",
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n# ── Print final summary table ───────────────────────────────────────────────\nprint(\"=\" * 65)\nprint(\"BASELINE RESULTS SUMMARY\")\nprint(\"=\" * 65)\nprint(f\"{'Method':<42}  {'GLL':>8}\")\nprint(\"-\" * 65)\nprint(f\"{'Baseline 1: Constant median (20% val split)':<42}  {gll_const:>8.4f}\")\nprint(f\"{'Baseline 2: Ridge regression (5-fold CV)':<42}  {gll_ridge_cv:>8.4f}\")\nprint(\"-\" * 65)\n\nimprovement = gll_ridge_cv - gll_const\nsign = \"+\" if improvement >= 0 else \"\"\nprint(f\"Ridge vs Constant improvement : {sign}{improvement:.4f} GLL points\")\nprint(f\"Using synthetic data           : {_USING_SYNTHETIC}\")\nprint(\"=\" * 65)\n\nprint(\"\\nBaseline (constant median) GLL =\", round(gll_const, 4))\nprint(\"Baseline (Ridge regression) GLL (5-fold CV) =\", round(gll_ridge_cv, 4))\n\n# ── Save results as JSON ──────────────────────────────────────────────────\nbaseline_results = {\n    \"baseline_constant_median_gll\": round(gll_const, 4),\n    \"baseline_ridge_regression_gll_cv\": round(gll_ridge_cv, 4),\n    \"baseline_ridge_regression_gll_std\": round(gll_ridge_std, 4),\n    \"ridge_vs_constant_improvement\": round(improvement, 4),\n    \"optimal_sigma_multiplier\": round(best_mult, 2),\n    \"optimal_sigma_gll\": round(best_gll, 4),\n    \"using_synthetic_data\": _USING_SYNTHETIC,\n    \"n_labelled_planets\": len(Y_mean),\n    \"n_wavelengths\": N_WAVELENGTHS,\n}\nresults_path = OUT_DIR / \"baseline_results.json\"\nwith open(results_path, \"w\") as f:\n    json.dump(baseline_results, f, indent=2)\nprint(f\"\\nResults saved to {results_path}\")\n\nprint(\"\\n[Done] Baseline notebook complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "xnn5tlie55",
   "source": "## 9. Push Baseline Results to GitHub\n\nPush GLL scores (JSON), sigma sensitivity plot, and prediction comparison plot to the repo.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hhutzhun8la",
   "source": "import shutil\nimport subprocess\nfrom pathlib import Path\n\n# ── GitHub token (read from Kaggle Secrets — never hardcode!) ──────────────\n# Add your PAT as a Kaggle Secret named \"GH_TOKEN\":\n#   Notebook sidebar → Add-ons → Secrets → + Add a new secret\nimport os\ntry:\n    from kaggle_secrets import UserSecretsClient\n    GH_TOKEN = UserSecretsClient().get_secret(\"GH_TOKEN\")\n    print(\"GH_TOKEN loaded from Kaggle Secrets.\")\nexcept Exception:\n    GH_TOKEN = os.environ.get(\"GH_TOKEN\", \"\")\n    if GH_TOKEN:\n        print(\"GH_TOKEN loaded from environment variable.\")\n    else:\n        print(\"WARNING: GH_TOKEN not found — push to GitHub will be skipped.\")\n\n# ── Repo paths ────────────────────────────────────────────────────────────\nrepo_dir    = Path(\"/kaggle/working/ariel-exoplanet-ml\")\nproject_dir = repo_dir / \"Kaggle competition\" / \"ARIEL neurIPS\"\n\n# ── Ensure repo is up-to-date ─────────────────────────────────────────────\nif not repo_dir.exists():\n    subprocess.run(\n        [\"git\", \"clone\", \"https://github.com/Smooth-Cactus0/ariel-exoplanet-ml.git\",\n         str(repo_dir)],\n        check=True,\n    )\nelse:\n    subprocess.run([\"git\", \"-C\", str(repo_dir), \"pull\", \"--ff-only\"], check=False)\n\n# ── Configure git identity (required on Kaggle kernels) ───────────────────\nsubprocess.run([\"git\", \"-C\", str(repo_dir), \"config\", \"user.email\", \"alexy.louis@kaggle-notebook.local\"], check=True)\nsubprocess.run([\"git\", \"-C\", str(repo_dir), \"config\", \"user.name\", \"Alexy Louis (Kaggle)\"], check=True)\n\n# ── Copy results and figures to repo ───────────────────────────────────────\nrepo_results_dir = project_dir / \"results\"\nrepo_results_dir.mkdir(parents=True, exist_ok=True)\nrepo_fig_dir = project_dir / \"figures\"\nrepo_fig_dir.mkdir(parents=True, exist_ok=True)\n\n# Copy baseline_results.json → results/\nresults_json = OUT_DIR / \"baseline_results.json\"\nif results_json.exists():\n    shutil.copy2(results_json, repo_results_dir / \"baseline_results.json\")\n    print(f\"  baseline_results.json -> results/baseline_results.json\")\n\n# Copy figures → figures/\nfor fig_path in sorted(FIG_DIR.glob(\"*.png\")):\n    dest = repo_fig_dir / fig_path.name\n    shutil.copy2(fig_path, dest)\n    print(f\"  {fig_path.name} -> figures/{fig_path.name}\")\n\n# ── Git add, commit, push ─────────────────────────────────────────────────\nsubprocess.run(\n    [\"git\", \"-C\", str(repo_dir), \"add\",\n     \"Kaggle competition/ARIEL neurIPS/results/\",\n     \"Kaggle competition/ARIEL neurIPS/figures/\"],\n    check=True,\n)\n\nstatus = subprocess.run(\n    [\"git\", \"-C\", str(repo_dir), \"diff\", \"--cached\", \"--quiet\"],\n    capture_output=True,\n)\nif status.returncode != 0:\n    subprocess.run(\n        [\"git\", \"-C\", str(repo_dir), \"commit\", \"-m\",\n         \"data: update baseline results and figures from Kaggle notebook run\"],\n        check=True,\n    )\n    if GH_TOKEN:\n        subprocess.run(\n            [\"git\", \"-C\", str(repo_dir), \"remote\", \"set-url\", \"origin\",\n             f\"https://{GH_TOKEN}@github.com/Smooth-Cactus0/ariel-exoplanet-ml.git\"],\n            check=True,\n        )\n        subprocess.run(\n            [\"git\", \"-C\", str(repo_dir), \"push\", \"origin\", \"master\"],\n            check=True,\n        )\n        print(\"\\n[Done] Baseline results pushed to GitHub.\")\n    else:\n        print(\"\\n[Done] Results committed locally but NOT pushed (no GH_TOKEN).\")\nelse:\n    print(\"\\n[Done] No changes to push (results already up-to-date).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}