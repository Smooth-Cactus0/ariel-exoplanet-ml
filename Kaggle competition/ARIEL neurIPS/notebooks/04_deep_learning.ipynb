{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# NeurIPS 2024 Ariel Data Challenge — Deep Learning Training\n",
    "\n",
    "**Goal**: Train `TransitCNN` end-to-end on preprocessed Ariel light curves to predict\n",
    "283-wavelength atmospheric transmission spectra with calibrated uncertainty.\n",
    "\n",
    "**Scoring**: Gaussian Log-Likelihood (GLL). Higher is better; 0 = perfect.\n",
    "\n",
    "**Architecture**: `TransitCNN` — dual 1-D CNN encoders (AIRS-CH0 + FGS1) fused with\n",
    "auxiliary stellar/planetary parameters, producing `(mean, log_var)` per output wavelength.\n",
    "\n",
    "> **Note**: This notebook is Kaggle-ready and requires the `ariel-data-challenge-2024`\n",
    "> dataset attached to the kernel, plus the cloned repo at\n",
    "> `/kaggle/working/ariel-exoplanet-ml/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-sec1",
   "metadata": {},
   "source": [
    "## 1. Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-clone-repo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Clone the repo if running on a fresh Kaggle kernel.\n",
    "# Uncomment the line below the first time you run this notebook.\n",
    "# ------------------------------------------------------------------\n",
    "repo_dir = \"/kaggle/working/ariel-exoplanet-ml\"\n",
    "# subprocess.run(\n",
    "#     [\"git\", \"clone\", \"https://github.com/Smooth-Cactus0/ariel-exoplanet-ml.git\", repo_dir],\n",
    "#     check=True,\n",
    "# )\n",
    "\n",
    "# Add repo root to Python path so `src.*` imports resolve.\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.insert(0, repo_dir)\n",
    "\n",
    "print(f\"sys.path[0] = {sys.path[0]}\")\n",
    "print(\"[Done] repo path configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gpu-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ── Random seed (reproducibility) ─────────────────────────────────────────\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ── Device ────────────────────────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"Device          : {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name        : {torch.cuda.get_device_name(0)}\")\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU VRAM        : {vram_gb:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU found — training will be slow on CPU.\")\n",
    "\n",
    "# ── Paths ──────────────────────────────────────────────────────────────────\n",
    "DATA_ROOT = Path(\"/kaggle/input/ariel-data-challenge-2024\")\n",
    "OUT_DIR   = Path(\"/kaggle/working/outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nDATA_ROOT : {DATA_ROOT}\")\n",
    "print(f\"OUT_DIR   : {OUT_DIR}\")\n",
    "print(\"[Done] Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-sec2",
   "metadata": {},
   "source": [
    "## 2. Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from src.dataset import ArielDataset  # TODO: verify key names inside ArielDataset\n",
    "from src.train import make_labelled_subset\n",
    "\n",
    "# ── Full training dataset (all planets, labelled + unlabelled) ─────────────\n",
    "# HDF5 keys used internally: 'AIRS-CH0' and 'FGS1'  # TODO: verify key names\n",
    "full_train_ds = ArielDataset(\n",
    "    DATA_ROOT,\n",
    "    split=\"train\",\n",
    "    bin_size=5,\n",
    "    preprocess=True,\n",
    ")\n",
    "print(f\"Total planets in train HDF5 : {len(full_train_ds)}\")\n",
    "\n",
    "# ── Labelled subset (only planets with QuartilesTable ground truth) ─────────\n",
    "labelled_ds = make_labelled_subset(full_train_ds)\n",
    "n_labelled = len(labelled_ds)\n",
    "print(f\"Labelled planets            : {n_labelled}\")\n",
    "print(f\"  (~{100 * n_labelled / len(full_train_ds):.1f}% of total)\")\n",
    "\n",
    "# ── 80 / 20 train / val split ──────────────────────────────────────────────\n",
    "n_val   = max(1, int(n_labelled * 0.20))\n",
    "n_train = n_labelled - n_val\n",
    "\n",
    "train_ds, val_ds = random_split(\n",
    "    labelled_ds,\n",
    "    [n_train, n_val],\n",
    "    generator=torch.Generator().manual_seed(SEED),\n",
    ")\n",
    "print(f\"Train split                 : {n_train}\")\n",
    "print(f\"Val split                   : {n_val}\")\n",
    "\n",
    "# ── DataLoaders ────────────────────────────────────────────────────────────\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches : {len(train_loader)}\")\n",
    "print(f\"Val batches   : {len(val_loader)}\")\n",
    "\n",
    "# ── Inspect one batch ──────────────────────────────────────────────────────\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nSample batch shapes:\")\n",
    "for k, v in sample_batch.items():\n",
    "    if hasattr(v, 'shape'):\n",
    "        print(f\"  {k:15s}: {tuple(v.shape)}  dtype={v.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {k:15s}: {type(v).__name__} (len={len(v)})\")\n",
    "\n",
    "print(\"[Done] Datasets and loaders ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-sec3",
   "metadata": {},
   "source": [
    "## 3. Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import TransitCNN\n",
    "\n",
    "# ── Instantiate TransitCNN ────────────────────────────────────────────────\n",
    "model = TransitCNN(\n",
    "    embed_dim=128,\n",
    "    dropout=0.1,\n",
    "    n_aux=9,\n",
    "    n_output_wl=283,\n",
    ").to(device)\n",
    "\n",
    "# ── Parameter count ───────────────────────────────────────────────────────\n",
    "n_params       = sum(p.numel() for p in model.parameters())\n",
    "n_trainable    = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "n_non_trainable = n_params - n_trainable\n",
    "\n",
    "print(f\"Total parameters       : {n_params:>10,}\")\n",
    "print(f\"Trainable parameters   : {n_trainable:>10,}\")\n",
    "print(f\"Non-trainable params   : {n_non_trainable:>10,}\")\n",
    "\n",
    "# ── Architecture summary (top-level named children) ──────────────────────\n",
    "print(\"\\nTop-level modules:\")\n",
    "print(f\"{'Module name':<20} {'Class':<25} {'# params':>10}\")\n",
    "print(\"-\" * 58)\n",
    "for name, module in model.named_children():\n",
    "    n_mod = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"{name:<20} {type(module).__name__:<25} {n_mod:>10,}\")\n",
    "\n",
    "# ── Quick forward-pass smoke test ─────────────────────────────────────────\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_airs = sample_batch[\"airs\"].to(device)\n",
    "    test_fgs1 = sample_batch[\"fgs1\"].to(device)\n",
    "    test_aux  = sample_batch[\"aux\"].to(device)\n",
    "    test_mean, test_log_var = model(test_airs, test_fgs1, test_aux)\n",
    "\n",
    "print(f\"\\nSmoke-test output shapes:\")\n",
    "print(f\"  mean    : {tuple(test_mean.shape)}\")\n",
    "print(f\"  log_var : {tuple(test_log_var.shape)}\")\n",
    "\n",
    "print(\"[Done] Model instantiated and smoke test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-sec4",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# ── Hyperparameter config ─────────────────────────────────────────────────\n",
    "config = dict(\n",
    "    lr              = 3e-4,\n",
    "    weight_decay    = 1e-4,\n",
    "    epochs          = 50,\n",
    "    clip_grad       = 1.0,\n",
    "    scheduler       = \"cosine\",\n",
    "    checkpoint_dir  = OUT_DIR / \"checkpoints\",\n",
    ")\n",
    "config[\"checkpoint_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k:<20}: {v}\")\n",
    "\n",
    "# ── Optimizer ────────────────────────────────────────────────────────────\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"lr\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# ── LR scheduler: cosine annealing to lr*0.01 at T_max ───────────────────\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config[\"epochs\"],\n",
    "    eta_min=config[\"lr\"] * 0.01,\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimizer : {type(optimizer).__name__}\")\n",
    "print(f\"Scheduler : {type(scheduler).__name__}  (T_max={config['epochs']}, \"\n",
    "      f\"eta_min={config['lr'] * 0.01:.2e})\")\n",
    "print(\"[Done] Optimizer and scheduler configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-sec5",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.model import gaussian_nll_loss\n",
    "from src.train import train_epoch, eval_epoch\n",
    "\n",
    "# ── History containers ────────────────────────────────────────────────────\n",
    "train_losses: list[float] = []\n",
    "val_glls:     list[float] = []\n",
    "\n",
    "best_val_gll   = float(\"-inf\")   # GLL: higher is better\n",
    "best_ckpt_path = config[\"checkpoint_dir\"] / \"best_model.pt\"\n",
    "\n",
    "print(f\"Starting training for {config['epochs']} epochs ...\\n\")\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    # ── Train ──────────────────────────────────────────────────────────────\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "\n",
    "    # ── Validate ───────────────────────────────────────────────────────────\n",
    "    # eval_epoch returns the negative-NLL loss value;\n",
    "    # we negate it to get a GLL-style score (higher = better).\n",
    "    val_nll  = eval_epoch(model, val_loader, device)\n",
    "    val_gll  = -val_nll  # approximate GLL: higher is better\n",
    "\n",
    "    # ── LR scheduler step ─────────────────────────────────────────────────\n",
    "    scheduler.step()\n",
    "    lr_now = scheduler.get_last_lr()[0]\n",
    "\n",
    "    # ── Record history ────────────────────────────────────────────────────\n",
    "    train_losses.append(train_loss)\n",
    "    val_glls.append(val_gll)\n",
    "\n",
    "    # ── Save best checkpoint ───────────────────────────────────────────────\n",
    "    improved = \"\"\n",
    "    if val_gll > best_val_gll:\n",
    "        best_val_gll = val_gll\n",
    "        torch.save(model.state_dict(), best_ckpt_path)\n",
    "        improved = \"  <- best\"\n",
    "\n",
    "    # ── Log ───────────────────────────────────────────────────────────────\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d}/{config['epochs']}  \"\n",
    "        f\"train_loss={train_loss:.4f}  \"\n",
    "        f\"val_GLL={val_gll:.4f}  \"\n",
    "        f\"lr={lr_now:.2e}\"\n",
    "        f\"{improved}\"\n",
    "    )\n",
    "\n",
    "# ── Save final checkpoint and history ────────────────────────────────────\n",
    "last_ckpt_path = config[\"checkpoint_dir\"] / \"last_model.pt\"\n",
    "torch.save(model.state_dict(), last_ckpt_path)\n",
    "\n",
    "history = [\n",
    "    {\"epoch\": e + 1, \"train_loss\": tl, \"val_gll\": vg}\n",
    "    for e, (tl, vg) in enumerate(zip(train_losses, val_glls))\n",
    "]\n",
    "with open(config[\"checkpoint_dir\"] / \"history.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nTraining complete in {elapsed / 60:.1f} min.\")\n",
    "print(f\"Best val GLL : {best_val_gll:.4f}\")\n",
    "print(f\"Checkpoints  : {config['checkpoint_dir']}\")\n",
    "print(\"[Done] Training loop finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-training-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Plot training curves ──────────────────────────────────────────────────\n",
    "epochs_range = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "fig, (ax_loss, ax_gll) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(\"TransitCNN Training Curves\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Left: train loss\n",
    "ax_loss.plot(epochs_range, train_losses, color=\"steelblue\", lw=1.5, label=\"Train NLL loss\")\n",
    "ax_loss.set_xlabel(\"Epoch\", fontsize=11)\n",
    "ax_loss.set_ylabel(\"Gaussian NLL Loss\", fontsize=11)\n",
    "ax_loss.set_title(\"Training Loss (NLL)\", fontsize=11)\n",
    "ax_loss.legend(fontsize=9)\n",
    "ax_loss.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: val GLL\n",
    "ax_gll.plot(epochs_range, val_glls, color=\"darkorange\", lw=1.5, label=\"Val GLL\")\n",
    "best_epoch = int(np.argmax(val_glls)) + 1\n",
    "ax_gll.axvline(best_epoch, color=\"red\", linestyle=\"--\", lw=1.2,\n",
    "               label=f\"Best epoch={best_epoch} (GLL={best_val_gll:.4f})\")\n",
    "ax_gll.set_xlabel(\"Epoch\", fontsize=11)\n",
    "ax_gll.set_ylabel(\"Val GLL (higher is better)\", fontsize=11)\n",
    "ax_gll.set_title(\"Validation Gaussian Log-Likelihood\", fontsize=11)\n",
    "ax_gll.legend(fontsize=9)\n",
    "ax_gll.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"training_curves.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"[Done] Training curves saved to {OUT_DIR / 'training_curves.png'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-sec6",
   "metadata": {},
   "source": [
    "## 6. Load Best Checkpoint & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import run_inference, gaussian_log_likelihood\n",
    "\n",
    "# ── Load best checkpoint ──────────────────────────────────────────────────\n",
    "print(f\"Loading best checkpoint from: {best_ckpt_path}\")\n",
    "state_dict = torch.load(best_ckpt_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print(\"Checkpoint loaded successfully.\")\n",
    "\n",
    "# ── Run inference on the validation loader ────────────────────────────────\n",
    "planet_ids_val, means_val, stds_val = run_inference(model, val_loader, device)\n",
    "print(f\"\\nVal inference: {len(planet_ids_val)} planets  \"\n",
    "      f\"| means shape={means_val.shape}  | stds shape={stds_val.shape}\")\n",
    "\n",
    "# ── Gather ground-truth q2 medians for GLL computation ───────────────────\n",
    "gt_q2_list = []\n",
    "for pid in planet_ids_val:\n",
    "    row = full_train_ds.labels.loc[int(pid)]\n",
    "    q2  = row.filter(regex=r\"^\\d+_q2$\").values.astype(np.float32)\n",
    "    gt_q2_list.append(q2)\n",
    "gt_q2 = np.stack(gt_q2_list)   # (n_val, 283)\n",
    "\n",
    "# ── Final GLL ─────────────────────────────────────────────────────────────\n",
    "final_val_gll = gaussian_log_likelihood(gt_q2, means_val, stds_val)\n",
    "print(f\"\\nFinal val GLL (best model): {final_val_gll:.4f}\")\n",
    "print(\"  (GLL = 0 is perfect; more negative = worse)\")\n",
    "print(\"[Done] Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-spectrum-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Plot predicted vs ground-truth spectra for 3 example val planets ──────\n",
    "# Subplot grid: 3 rows x 2 columns\n",
    "# Left column : full-spectrum view\n",
    "# Right column: zoomed residual view\n",
    "\n",
    "N_EXAMPLES = 3\n",
    "wl_idx = np.arange(283)\n",
    "\n",
    "fig, axes = plt.subplots(N_EXAMPLES, 2, figsize=(16, 5 * N_EXAMPLES))\n",
    "fig.suptitle(\n",
    "    \"Predicted vs Ground-Truth Transmission Spectra\\n\"\n",
    "    \"(val set, best checkpoint)\\n\"\n",
    "    \"Left: full spectrum  |  Right: residual (pred - GT)\",\n",
    "    fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "for row_i in range(N_EXAMPLES):\n",
    "    pid  = planet_ids_val[row_i]\n",
    "    mu   = means_val[row_i]      # (283,)  predicted mean\n",
    "    sig  = stds_val[row_i]       # (283,)  predicted std\n",
    "\n",
    "    # Ground-truth q1 / q2 / q3\n",
    "    gt_row = full_train_ds.labels.loc[int(pid)]\n",
    "    q1 = gt_row.filter(regex=r\"^\\d+_q1$\").values.astype(np.float32)\n",
    "    q2 = gt_row.filter(regex=r\"^\\d+_q2$\").values.astype(np.float32)\n",
    "    q3 = gt_row.filter(regex=r\"^\\d+_q3$\").values.astype(np.float32)\n",
    "\n",
    "    # ── Left: full spectrum ───────────────────────────────────────────────\n",
    "    ax_l = axes[row_i, 0]\n",
    "    ax_l.plot(wl_idx, q2,  color=\"steelblue\",  lw=1.5, label=\"GT median (q2)\")\n",
    "    ax_l.fill_between(wl_idx, q1, q3, alpha=0.25, color=\"steelblue\",\n",
    "                      label=\"GT q1-q3 band\")\n",
    "    ax_l.plot(wl_idx, mu,  color=\"darkorange\", lw=1.5, linestyle=\"--\",\n",
    "              label=\"Predicted mean\")\n",
    "    ax_l.fill_between(wl_idx, mu - sig, mu + sig, alpha=0.25,\n",
    "                      color=\"darkorange\", label=\"Pred ±1σ band\")\n",
    "    ax_l.set_xlabel(\"Wavelength index\", fontsize=9)\n",
    "    ax_l.set_ylabel(\"Transmission depth\", fontsize=9)\n",
    "    ax_l.set_title(f\"Planet {pid} — spectrum\", fontsize=9, fontweight=\"bold\")\n",
    "    ax_l.legend(fontsize=7, loc=\"upper right\")\n",
    "    ax_l.grid(True, alpha=0.3)\n",
    "\n",
    "    # ── Right: residual ───────────────────────────────────────────────────\n",
    "    ax_r = axes[row_i, 1]\n",
    "    residual    = mu - q2\n",
    "    norm_resid  = residual / np.clip(sig, 1e-9, None)\n",
    "    ax_r.bar(wl_idx, norm_resid, width=1.0, color=\"purple\", alpha=0.5,\n",
    "             label=\"(pred - GT) / pred_std\")\n",
    "    ax_r.axhline(0,  color=\"black\",  lw=1.0)\n",
    "    ax_r.axhline(+1, color=\"red\",    lw=0.8, linestyle=\":\")\n",
    "    ax_r.axhline(-1, color=\"red\",    lw=0.8, linestyle=\":\")\n",
    "    ax_r.set_xlabel(\"Wavelength index\", fontsize=9)\n",
    "    ax_r.set_ylabel(\"Normalised residual (σ)\", fontsize=9)\n",
    "    ax_r.set_title(f\"Planet {pid} — residual\", fontsize=9, fontweight=\"bold\")\n",
    "    ax_r.legend(fontsize=7, loc=\"upper right\")\n",
    "    ax_r.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"val_spectra_examples.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"[Done] Spectrum plots saved to {OUT_DIR / 'val_spectra_examples.png'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-sec7",
   "metadata": {},
   "source": [
    "## 7. Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.evaluate import build_submission\n",
    "\n",
    "# ── Test dataset ──────────────────────────────────────────────────────────\n",
    "# HDF5 keys used internally: 'AIRS-CH0' and 'FGS1'  # TODO: verify key names\n",
    "test_ds = ArielDataset(\n",
    "    DATA_ROOT,\n",
    "    split=\"test\",\n",
    "    bin_size=5,\n",
    "    preprocess=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "print(f\"Test planets  : {len(test_ds)}\")\n",
    "print(f\"Test batches  : {len(test_loader)}\")\n",
    "\n",
    "# ── Run inference on test split ───────────────────────────────────────────\n",
    "planet_ids_test, means_test, stds_test = run_inference(model, test_loader, device)\n",
    "print(f\"\\nTest inference: {len(planet_ids_test)} planets  \"\n",
    "      f\"| means={means_test.shape}  | stds={stds_test.shape}\")\n",
    "\n",
    "# ── Build submission DataFrame ────────────────────────────────────────────\n",
    "# NOTE: column naming convention verified against sample_submission.csv\n",
    "# TODO: verify column format by running:\n",
    "#   ss = pd.read_csv('/kaggle/input/ariel-data-challenge-2024/sample_submission.csv', nrows=1)\n",
    "#   print(list(ss.columns[:6]))\n",
    "sub_df = build_submission(planet_ids_test, means_test, stds_test)\n",
    "\n",
    "# ── Save to disk ──────────────────────────────────────────────────────────\n",
    "submission_path = Path(\"/kaggle/working/submission.csv\")\n",
    "sub_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved: {submission_path}\")\n",
    "print(f\"Shape           : {sub_df.shape}\")\n",
    "print(f\"Columns (first 7): {list(sub_df.columns[:7])}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "display(sub_df.head(3))\n",
    "\n",
    "print(\"[Done] Submission file written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-sec8",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "- **TransitCNN**: dual-encoder design processing AIRS-CH0 (IR spectrometer, 356 channels)\n",
    "  and FGS1 (visible broadband, 1 channel) separately via 3-layer `TemporalEncoder` blocks\n",
    "  (Conv1d + GELU + GlobalAvgPool), then fusing both embeddings with 9 auxiliary stellar/\n",
    "  planetary parameters through a 3-layer MLP.\n",
    "- Two output heads: `mean (283,)` and `log_var (283,)` — one prediction per wavelength bin.\n",
    "- Loss: Gaussian NLL, which jointly minimises prediction error and trains calibrated\n",
    "  uncertainty (overconfident predictions are penalised).\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "| Hyperparameter   | Value          |\n",
    "|------------------|----------------|\n",
    "| `embed_dim`      | 128            |\n",
    "| `dropout`        | 0.1            |\n",
    "| `optimizer`      | AdamW          |\n",
    "| `lr`             | 3e-4           |\n",
    "| `weight_decay`   | 1e-4           |\n",
    "| `epochs`         | 50             |\n",
    "| `batch_size`     | 32             |\n",
    "| `scheduler`      | CosineAnnealing|\n",
    "| `clip_grad_norm` | 1.0            |\n",
    "| `bin_size`       | 5              |\n",
    "| `val_fraction`   | 20%            |\n",
    "| `seed`           | 42             |\n",
    "\n",
    "### Final Validation GLL\n",
    "\n",
    "See `final_val_gll` printed in Section 6.  \n",
    "(GLL = 0 is perfect; increasingly negative values indicate worse calibration.)\n",
    "\n",
    "### What to Try Next\n",
    "\n",
    "- **Ensembling**: train 5 models with different seeds and average their `(mean, log_var)` \n",
    "  predictions; ensemble uncertainty can be derived from prediction variance across members.\n",
    "- **More augmentation**: apply time-shift jitter, channel dropout, and Gaussian noise\n",
    "  injection during training to improve generalisation.\n",
    "- **Larger `embed_dim`**: try `embed_dim=256` or `embed_dim=512`; the current bottleneck\n",
    "  is the 128-dim AIRS embedding relative to the 356 input channels.\n",
    "- **Attention pooling**: replace `AdaptiveAvgPool1d` with a learnable attention-weighted\n",
    "  pool to focus on the transit window.\n",
    "- **Pseudo-labelling**: use the trained model to generate soft targets for the ~76%\n",
    "  unlabelled planets; re-train with a combined labelled + pseudo-labelled dataset.\n",
    "- **Physics priors**: add stellar limb-darkening coefficients or Bazin parametric\n",
    "  light-curve features as extra auxiliary inputs."
   ]
  }
 ]
}
