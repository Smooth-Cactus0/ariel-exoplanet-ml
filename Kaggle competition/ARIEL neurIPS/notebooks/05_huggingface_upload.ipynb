{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": "# NeurIPS 2024 Ariel Data Challenge — HuggingFace Dataset Upload\n\n**Purpose**: Preprocess every planet in the Ariel competition dataset and push  \nthe results to `Smooth-Cactus0/ariel-exoplanet-2024` on the HuggingFace Hub.\n\n**Data format**: The competition data uses **parquet files** organized as  \n`{split}/{planet_id}/AIRS-CH0_signal.parquet` (and calibration files).\n\n**Outputs**:\n- `data/preprocessed/{train,test}/{planet_id}.npz` — one compressed NumPy archive per planet\n- HuggingFace dataset repository with `ariel_dataset.py` loading script\n\n> **Note**: This notebook is Kaggle-ready and requires `ariel-data-challenge-2024` attached."
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup-install",
   "metadata": {},
   "outputs": [],
   "source": "# Install / verify required packages\nimport subprocess, sys\n\nREQUIRED_PACKAGES = [\n    \"pyarrow\",\n    \"tqdm\",\n    \"huggingface_hub\",\n    \"datasets\",\n]\n\nfor pkg in REQUIRED_PACKAGES:\n    try:\n        __import__(pkg.replace(\"-\", \"_\"))\n        print(f\"{pkg}: already installed\")\n    except ImportError:\n        print(f\"{pkg}: not found — installing...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n        print(f\"{pkg}: installed\")\n\nprint(\"[Done] Package check complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup-imports",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport subprocess\nimport sys\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\n# ---------------------------------------------------------------------------\n# Clone the repo on Kaggle and add to sys.path\n# ---------------------------------------------------------------------------\nREPO_DIR = \"/kaggle/working/ariel-exoplanet-ml\"\nPROJECT_DIR = REPO_DIR + \"/Kaggle competition/ARIEL neurIPS\"\n\nif not Path(REPO_DIR).exists():\n    subprocess.run(\n        [\"git\", \"clone\",\n         \"https://github.com/Smooth-Cactus0/ariel-exoplanet-ml.git\",\n         REPO_DIR],\n        check=True,\n    )\n    print(f\"Cloned repo to {REPO_DIR}\")\nelse:\n    print(f\"Repo already exists at {REPO_DIR}\")\n\nsys.path.insert(0, PROJECT_DIR)\n\n# ---------------------------------------------------------------------------\n# HuggingFace token — paste your token here or set HF_TOKEN env variable\n# ---------------------------------------------------------------------------\nos.environ[\"HF_TOKEN\"] = \"\"  # Set your HuggingFace token here\n\n# ---------------------------------------------------------------------------\n# Directories\n# ---------------------------------------------------------------------------\nDATA_ROOT = Path(\"/kaggle/input/ariel-data-challenge-2024\")\nOUT_DIR   = Path(\"/kaggle/working/preprocessed\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n(OUT_DIR / \"train\").mkdir(exist_ok=True)\n(OUT_DIR / \"test\").mkdir(exist_ok=True)\n\n# Figure output directory\nFIG_DIR = Path(\"/kaggle/working/figures_hf_upload\")\nFIG_DIR.mkdir(parents=True, exist_ok=True)\n\n# Plot style\nplt.rcParams.update({\n    \"savefig.dpi\": 150,\n    \"savefig.facecolor\": \"white\",\n})\n\n# ---------------------------------------------------------------------------\n# Instrument geometry constants (confirmed from competition data)\n# ---------------------------------------------------------------------------\nAIRS_N_ROWS = 32    # spatial rows in AIRS-CH0 detector\nAIRS_N_COLS = 356   # spectral channels in AIRS-CH0\nFGS1_N_ROWS = 32    # FGS1 detector rows\nFGS1_N_COLS = 32    # FGS1 detector columns\nFGS1_RATIO  = 12    # FGS1 cadence is 12x AIRS cadence\n\n# ---------------------------------------------------------------------------\n# Preprocessing hyper-parameters (must match training config)\n# ---------------------------------------------------------------------------\nINGRESS  = 0.20\nEGRESS   = 0.80\nBIN_SIZE = 5\n\nprint(f\"DATA_ROOT       : {DATA_ROOT}  (exists={DATA_ROOT.exists()})\")\nprint(f\"OUT_DIR         : {OUT_DIR}\")\nprint(f\"FIG_DIR         : {FIG_DIR}\")\nprint(f\"AIRS geometry   : {AIRS_N_ROWS} rows x {AIRS_N_COLS} spectral channels\")\nprint(f\"FGS1 geometry   : {FGS1_N_ROWS} x {FGS1_N_COLS}, cadence ratio={FGS1_RATIO}\")\nprint(f\"Ingress / Egress / BinSize : {INGRESS} / {EGRESS} / {BIN_SIZE}\")\n\nprint(\"[Done] Setup complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md-preprocess-train",
   "metadata": {},
   "source": [
    "## 2. Preprocess All Training Planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-tables",
   "metadata": {},
   "outputs": [],
   "source": "# Load ADC info table and labels\nadc_path    = DATA_ROOT / \"train_adc_info.csv\"\nlabels_path = DATA_ROOT / \"train_labels.csv\"\n\ndf_adc = pd.read_csv(adc_path)\nprint(f\"train_adc_info  : {df_adc.shape[0]} planets, {df_adc.shape[1]} columns\")\nprint(f\"  Columns: {list(df_adc.columns)}\")\n\n# Build a lookup dict: planet_id -> row of 5 ADC features\nadc_feature_cols = [c for c in df_adc.columns if c != \"planet_id\"]\nadc_lookup = {\n    str(row[\"planet_id\"]): row[adc_feature_cols].values.astype(np.float32)\n    for _, row in df_adc.iterrows()\n}\n\n# Labels: train_labels.csv with columns planet_id, wl_1, ..., wl_283\nif labels_path.exists():\n    df_labels = pd.read_csv(labels_path)\n    labelled_ids = set(df_labels[\"planet_id\"].astype(str))\n    # Build lookup: planet_id -> np.array of shape (283,)\n    wl_cols = [c for c in df_labels.columns if c.startswith(\"wl_\")]\n    labels_lookup = {\n        str(row[\"planet_id\"]): row[wl_cols].values.astype(np.float32)\n        for _, row in df_labels.iterrows()\n    }\n    print(f\"train_labels    : {len(labelled_ids)} labelled planets, {len(wl_cols)} wavelength bins\")\nelse:\n    df_labels     = None\n    labelled_ids  = set()\n    labels_lookup = {}\n    print(\"WARNING: train_labels.csv not found — no label extraction possible.\")\n\nprint(f\"\\n[Done] Tables loaded. {len(labelled_ids)} of {df_adc.shape[0]} planets are labelled.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-label-helper",
   "metadata": {},
   "outputs": [],
   "source": "def parse_label_row(labels_lookup: dict, planet_id: str):\n    \"\"\"\n    Extract target_mean from the labels lookup for a single planet.\n\n    Returns (target_mean,) where target_mean is a float32 array of shape (283,),\n    or (None,) if the planet has no labels.\n\n    Note: there is no target_std in the new label format — sigma is model-predicted.\n    \"\"\"\n    if planet_id not in labels_lookup:\n        return (None,)\n    return (labels_lookup[planet_id],)\n\n\ndef calibrate(signal_raw, dark, flat, dead, gain, offset):\n    \"\"\"\n    Apply detector calibration to raw ADC counts.\n\n    Parameters\n    ----------\n    signal_raw : np.ndarray, uint16 — raw detector signal\n    dark       : np.ndarray, float  — dark current frame\n    flat       : np.ndarray, float  — flat field frame\n    dead       : np.ndarray, float  — dead pixel mask (1 = dead)\n    gain       : float              — ADC gain\n    offset     : float              — ADC offset\n\n    Returns\n    -------\n    np.ndarray, float32 — calibrated signal\n    \"\"\"\n    # Convert from ADC counts to physical units\n    signal = signal_raw.astype(np.float32) * gain + offset\n    # Subtract dark current\n    signal = signal - dark\n    # Apply flat field correction (avoid division by zero)\n    flat_safe = np.where(flat == 0, 1.0, flat)\n    signal = signal / flat_safe\n    # Mask dead pixels (set to NaN, will be handled downstream)\n    signal = np.where(dead > 0.5, np.nan, signal)\n    return signal\n\n\ndef load_calibration(planet_dir, instrument):\n    \"\"\"\n    Load calibration frames for a given instrument from parquet files.\n\n    Returns dict with keys: dark, flat, dead, read, linear_corr\n    \"\"\"\n    cal_dir = planet_dir / f\"{instrument}_calibration\"\n    cal = {}\n    for name in [\"dark\", \"flat\", \"dead\", \"read\", \"linear_corr\"]:\n        fpath = cal_dir / f\"{name}.parquet\"\n        if fpath.exists():\n            df = pd.read_parquet(fpath)\n            cal[name] = df.values.astype(np.float32)\n        else:\n            cal[name] = None\n    return cal\n\n\ndef load_planet_parquet(planet_dir, adc_info):\n    \"\"\"\n    Load and calibrate AIRS-CH0 and FGS1 signals from parquet files.\n\n    Parameters\n    ----------\n    planet_dir : Path — directory for one planet (contains signal + calibration parquets)\n    adc_info   : np.ndarray, shape (5,) — [FGS1_adc_offset, FGS1_adc_gain,\n                                            AIRS-CH0_adc_offset, AIRS-CH0_adc_gain, star]\n\n    Returns\n    -------\n    airs_signal : np.ndarray, shape (n_time, 356), float32\n    fgs1_signal : np.ndarray, shape (n_time,), float32\n    \"\"\"\n    fgs1_offset, fgs1_gain, airs_offset, airs_gain, _star = adc_info\n\n    # --- AIRS-CH0 ---------------------------------------------------------\n    airs_path = planet_dir / \"AIRS-CH0_signal.parquet\"\n    airs_flat = pd.read_parquet(airs_path).values  # (n_time, 32*356) uint16\n    n_time_airs = airs_flat.shape[0]\n    airs_3d = airs_flat.reshape(n_time_airs, AIRS_N_ROWS, AIRS_N_COLS)  # (t, 32, 356)\n\n    airs_cal = load_calibration(planet_dir, \"AIRS-CH0\")\n    if airs_cal[\"dark\"] is not None:\n        dark_2d = airs_cal[\"dark\"].reshape(AIRS_N_ROWS, AIRS_N_COLS)\n        flat_2d = airs_cal[\"flat\"].reshape(AIRS_N_ROWS, AIRS_N_COLS) if airs_cal[\"flat\"] is not None else np.ones((AIRS_N_ROWS, AIRS_N_COLS), dtype=np.float32)\n        dead_2d = airs_cal[\"dead\"].reshape(AIRS_N_ROWS, AIRS_N_COLS) if airs_cal[\"dead\"] is not None else np.zeros((AIRS_N_ROWS, AIRS_N_COLS), dtype=np.float32)\n        airs_calibrated = calibrate(airs_3d, dark_2d, flat_2d, dead_2d, airs_gain, airs_offset)\n    else:\n        # No calibration available — just convert from ADC\n        airs_calibrated = airs_3d.astype(np.float32) * airs_gain + airs_offset\n\n    # Sum over spatial rows -> (n_time, 356)\n    airs_signal = np.nansum(airs_calibrated, axis=1)  # (n_time, 356)\n\n    # --- FGS1 -------------------------------------------------------------\n    fgs1_path = planet_dir / \"FGS1_signal.parquet\"\n    fgs1_flat = pd.read_parquet(fgs1_path).values  # (n_time_fgs1, 32*32) uint16\n    n_time_fgs1 = fgs1_flat.shape[0]\n    fgs1_3d = fgs1_flat.reshape(n_time_fgs1, FGS1_N_ROWS, FGS1_N_COLS)  # (t, 32, 32)\n\n    fgs1_cal = load_calibration(planet_dir, \"FGS1\")\n    if fgs1_cal[\"dark\"] is not None:\n        dark_f = fgs1_cal[\"dark\"].reshape(FGS1_N_ROWS, FGS1_N_COLS)\n        flat_f = fgs1_cal[\"flat\"].reshape(FGS1_N_ROWS, FGS1_N_COLS) if fgs1_cal[\"flat\"] is not None else np.ones((FGS1_N_ROWS, FGS1_N_COLS), dtype=np.float32)\n        dead_f = fgs1_cal[\"dead\"].reshape(FGS1_N_ROWS, FGS1_N_COLS) if fgs1_cal[\"dead\"] is not None else np.zeros((FGS1_N_ROWS, FGS1_N_COLS), dtype=np.float32)\n        fgs1_calibrated = calibrate(fgs1_3d, dark_f, flat_f, dead_f, fgs1_gain, fgs1_offset)\n    else:\n        fgs1_calibrated = fgs1_3d.astype(np.float32) * fgs1_gain + fgs1_offset\n\n    # Sum over spatial dims -> (n_time_fgs1,), then downsample 12:1 to match AIRS cadence\n    fgs1_summed = np.nansum(fgs1_calibrated, axis=(1, 2))  # (n_time_fgs1,)\n\n    # Downsample FGS1 by averaging groups of FGS1_RATIO frames\n    n_trim = (n_time_fgs1 // FGS1_RATIO) * FGS1_RATIO\n    fgs1_downsampled = fgs1_summed[:n_trim].reshape(-1, FGS1_RATIO).mean(axis=1)  # (n_time_airs,)\n\n    return airs_signal, fgs1_downsampled\n\n\nprint(\"[Done] Helper functions defined: parse_label_row, calibrate, load_calibration, load_planet_parquet.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-preprocess-train",
   "metadata": {},
   "outputs": [],
   "source": "# Import the preprocessing function from src/preprocessing.py\ntry:\n    from src.preprocessing import preprocess_planet\n    print(\"Imported preprocess_planet from src.preprocessing\")\nexcept ImportError as exc:\n    print(f\"WARNING: Could not import preprocess_planet ({exc}).\")\n    print(\"Ensure REPO_DIR is set correctly and the repo has been cloned.\")\n    raise\n\n# ---------------------------------------------------------------------------\n# Preprocess training planets\n# ---------------------------------------------------------------------------\nTRAIN_DIR = DATA_ROOT / \"train\"\ntrain_out_dir = OUT_DIR / \"train\"\n\n# Discover planet directories\nall_train_dirs = sorted([d for d in TRAIN_DIR.iterdir() if d.is_dir()])\nall_train_ids  = [d.name for d in all_train_dirs]\nn_total   = len(all_train_ids)\nn_success = 0\nn_labelled_saved = 0\nn_failed  = 0\ntotal_bytes = 0\n\nprint(f\"Found {n_total} planet directories in {TRAIN_DIR}\")\n\nwith tqdm(total=n_total, desc=\"Train planets\", unit=\"planet\") as pbar:\n    for planet_dir in all_train_dirs:\n        pid = planet_dir.name\n        out_path = train_out_dir / f\"{pid}.npz\"\n\n        # Skip if already done (allows resuming after interruption)\n        if out_path.exists():\n            n_success += 1\n            pbar.update(1)\n            pbar.set_postfix(skipped=\"(cached)\")\n            continue\n\n        # --- Auxiliary features (ADC info) --------------------------------\n        aux_row = adc_lookup.get(pid, np.zeros(len(adc_feature_cols), dtype=np.float32))\n        if pid not in adc_lookup:\n            warnings.warn(f\"ADC info not found for planet {pid} — using zeros.\")\n\n        # --- Load and calibrate from parquet ------------------------------\n        try:\n            airs_raw, fgs1_raw = load_planet_parquet(planet_dir, aux_row)\n        except FileNotFoundError as exc:\n            warnings.warn(f\"Missing parquet for planet {pid}: {exc} — skipping.\")\n            n_failed += 1\n            pbar.update(1)\n            continue\n        except Exception as exc:\n            warnings.warn(f\"Error loading planet {pid}: {exc} — skipping.\")\n            n_failed += 1\n            pbar.update(1)\n            continue\n\n        # --- Preprocessing pipeline ---------------------------------------\n        try:\n            result = preprocess_planet(\n                airs_raw, fgs1_raw,\n                ingress=INGRESS, egress=EGRESS, bin_size=BIN_SIZE,\n            )\n        except Exception as exc:\n            warnings.warn(f\"Preprocessing failed for planet {pid}: {exc} — skipping.\")\n            n_failed += 1\n            pbar.update(1)\n            continue\n\n        # --- Label extraction (optional) ----------------------------------\n        (target_mean,) = parse_label_row(labels_lookup, pid)\n\n        # --- Save to .npz ------------------------------------------------\n        save_kwargs = dict(\n            airs_norm        = result[\"airs_norm\"],          # (time_binned, 356)\n            fgs1_norm        = result[\"fgs1_norm\"],          # (time_binned,)\n            aux              = aux_row,                       # (5,)\n            transit_depth    = result[\"transit_depth\"],      # (356,)\n            transit_depth_err= result[\"transit_depth_err\"],  # (356,)\n            mask_oot         = result[\"mask_oot\"],           # (time_binned,)\n        )\n        if target_mean is not None:\n            save_kwargs[\"target_mean\"] = target_mean  # (283,)\n            n_labelled_saved += 1\n\n        np.savez_compressed(str(out_path), **save_kwargs)\n        total_bytes += out_path.stat().st_size\n        n_success += 1\n        pbar.update(1)\n        pbar.set_postfix(\n            success=n_success, labelled=n_labelled_saved, failed=n_failed\n        )\n\n# Summary\navg_kb = (total_bytes / max(n_success, 1)) / 1024\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Train preprocessing summary\")\nprint(\"=\" * 60)\nprint(f\"  Total planets     : {n_total}\")\nprint(f\"  Successfully saved: {n_success}\")\nprint(f\"  Labelled planets  : {n_labelled_saved}\")\nprint(f\"  Failed / skipped  : {n_failed}\")\nprint(f\"  Total size on disk: {total_bytes / 1_048_576:.1f} MB\")\nprint(f\"  Avg file size     : {avg_kb:.1f} KB\")\nprint(\"[Done] Train preprocessing complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md-preprocess-test",
   "metadata": {},
   "source": [
    "## 3. Preprocess All Test Planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-preprocess-test",
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Preprocess test planets (parquet directory layout)\n# ---------------------------------------------------------------------------\nTEST_DIR = DATA_ROOT / \"test\"\ntest_out_dir = OUT_DIR / \"test\"\n\n# Load test ADC info if available\ntest_adc_path = DATA_ROOT / \"test_adc_info.csv\"\nif test_adc_path.exists():\n    df_adc_test = pd.read_csv(test_adc_path)\n    test_adc_feature_cols = [c for c in df_adc_test.columns if c != \"planet_id\"]\n    test_adc_lookup = {\n        str(row[\"planet_id\"]): row[test_adc_feature_cols].values.astype(np.float32)\n        for _, row in df_adc_test.iterrows()\n    }\n    print(f\"test_adc_info   : {df_adc_test.shape[0]} planets, {df_adc_test.shape[1]} columns\")\nelse:\n    # Fallback: use train ADC lookup (planets may overlap) or zeros\n    test_adc_lookup = adc_lookup.copy()\n    test_adc_feature_cols = adc_feature_cols\n    print(\"WARNING: test_adc_info.csv not found — falling back to train ADC lookup.\")\n\nif not TEST_DIR.exists():\n    print(f\"WARNING: Test directory {TEST_DIR} does not exist — skipping test split.\")\n    all_test_dirs = []\nelse:\n    all_test_dirs = sorted([d for d in TEST_DIR.iterdir() if d.is_dir()])\n\nall_test_ids     = [d.name for d in all_test_dirs]\nn_total_test     = len(all_test_ids)\nn_success_test   = 0\nn_failed_test    = 0\ntotal_bytes_test = 0\n\nprint(f\"Test planets to process : {n_total_test}\")\n\nif n_total_test > 0:\n    with tqdm(total=n_total_test, desc=\"Test planets\", unit=\"planet\") as pbar:\n        for planet_dir in all_test_dirs:\n            pid = planet_dir.name\n            out_path = test_out_dir / f\"{pid}.npz\"\n\n            if out_path.exists():\n                n_success_test += 1\n                pbar.update(1)\n                pbar.set_postfix(skipped=\"(cached)\")\n                continue\n\n            # Auxiliary features\n            aux_row = test_adc_lookup.get(pid, np.zeros(len(test_adc_feature_cols), dtype=np.float32))\n            if pid not in test_adc_lookup:\n                warnings.warn(f\"ADC info not found for test planet {pid} — using zeros.\")\n\n            try:\n                airs_raw, fgs1_raw = load_planet_parquet(planet_dir, aux_row)\n            except FileNotFoundError as exc:\n                warnings.warn(f\"Missing parquet for test planet {pid}: {exc} — skipping.\")\n                n_failed_test += 1\n                pbar.update(1)\n                continue\n            except Exception as exc:\n                warnings.warn(f\"Error loading test planet {pid}: {exc} — skipping.\")\n                n_failed_test += 1\n                pbar.update(1)\n                continue\n\n            try:\n                result = preprocess_planet(\n                    airs_raw, fgs1_raw,\n                    ingress=INGRESS, egress=EGRESS, bin_size=BIN_SIZE,\n                )\n            except Exception as exc:\n                warnings.warn(f\"Preprocessing failed for test planet {pid}: {exc}.\")\n                n_failed_test += 1\n                pbar.update(1)\n                continue\n\n            np.savez_compressed(\n                str(out_path),\n                airs_norm        = result[\"airs_norm\"],\n                fgs1_norm        = result[\"fgs1_norm\"],\n                aux              = aux_row,\n                transit_depth    = result[\"transit_depth\"],\n                transit_depth_err= result[\"transit_depth_err\"],\n                mask_oot         = result[\"mask_oot\"],\n            )\n            total_bytes_test += out_path.stat().st_size\n            n_success_test += 1\n            pbar.update(1)\n            pbar.set_postfix(success=n_success_test, failed=n_failed_test)\n\navg_kb_test = (total_bytes_test / max(n_success_test, 1)) / 1024\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Test preprocessing summary\")\nprint(\"=\" * 60)\nprint(f\"  Total planets     : {n_total_test}\")\nprint(f\"  Successfully saved: {n_success_test}\")\nprint(f\"  Failed / skipped  : {n_failed_test}\")\nprint(f\"  Total size on disk: {total_bytes_test / 1_048_576:.1f} MB\")\nprint(f\"  Avg file size     : {avg_kb_test:.1f} KB\")\nprint(\"[Done] Test preprocessing complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md-validate",
   "metadata": {},
   "source": [
    "## 4. Validate One Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-validate",
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n# Load back one .npz from the train split to sanity-check contents and shapes\ntrain_npz_files = sorted((OUT_DIR / \"train\").glob(\"*.npz\"))\n\nif not train_npz_files:\n    print(\"WARNING: No .npz files found in train output dir — cannot validate.\")\nelse:\n    sample_path = train_npz_files[0]\n    sample = np.load(sample_path, allow_pickle=False)\n\n    planet_id = sample_path.stem\n    print(f\"Sample planet ID  : {planet_id}\")\n    print(f\"NPZ file path     : {sample_path}\")\n    print(f\"File size         : {sample_path.stat().st_size / 1024:.1f} KB\")\n    print()\n    print(f\"{'Key':<22} {'Shape':<25} {'dtype':<10} {'min':>10}  {'max':>10}\")\n    print(\"-\" * 80)\n    for key in sorted(sample.files):\n        arr = sample[key]\n        vmin = float(arr.min()) if arr.size > 0 else float('nan')\n        vmax = float(arr.max()) if arr.size > 0 else float('nan')\n        print(f\"  {key:<20} {str(arr.shape):<25} {str(arr.dtype):<10} {vmin:>10.4f}  {vmax:>10.4f}\")\n\n    # Validate expected shapes\n    assert sample[\"aux\"].shape == (len(adc_feature_cols),), \\\n        f\"Expected aux shape ({len(adc_feature_cols)},), got {sample['aux'].shape}\"\n    print(f\"\\n  aux shape confirmed: ({len(adc_feature_cols)},) — \"\n          f\"columns: {adc_feature_cols}\")\n\n    if \"target_mean\" in sample.files:\n        print(f\"  target_mean shape : {sample['target_mean'].shape} (labelled planet)\")\n        assert \"target_std\" not in sample.files, \\\n            \"target_std should NOT be present — sigma is model-predicted.\"\n    else:\n        print(\"  (unlabelled planet — no target_mean)\")\n\n    # --- Plot transit depth spectrum ----------------------------------------\n    transit_depth = sample[\"transit_depth\"]\n    transit_depth_err = sample[\"transit_depth_err\"]\n    n_channels = len(transit_depth)\n    wl_idx = np.arange(n_channels)\n\n    fig, ax = plt.subplots(figsize=(12, 4))\n    ax.plot(wl_idx, transit_depth, lw=0.9, color=\"steelblue\", label=\"Transit depth\")\n    ax.fill_between(\n        wl_idx,\n        transit_depth - transit_depth_err,\n        transit_depth + transit_depth_err,\n        alpha=0.25, color=\"steelblue\", label=\"+-1 sigma\",\n    )\n\n    # Overlay target_mean if available\n    if \"target_mean\" in sample.files and len(sample[\"target_mean\"]) == 283:\n        target_mean = sample[\"target_mean\"]\n        target_wl   = np.linspace(0, n_channels - 1, 283)\n        ax.plot(target_wl, target_mean, lw=1.2, color=\"darkorange\",\n                linestyle=\"--\", label=\"Target mean (283 channels)\")\n\n    ax.set_xlabel(\"AIRS-CH0 channel index\")\n    ax.set_ylabel(\"Transit depth (fractional)\")\n    ax.set_title(f\"Planet {planet_id} — extracted transit depth spectrum ({n_channels} channels)\")\n    ax.legend()\n    plt.tight_layout()\n    fig.savefig(FIG_DIR / \"hf_validation_spectrum.png\", bbox_inches=\"tight\")\n    plt.show()\n\n    # --- Save preprocessing summary as JSON --------------------------------\n    n_train_npz = len(train_npz_files)\n    n_test_npz  = len(sorted((OUT_DIR / \"test\").glob(\"*.npz\")))\n    preprocess_summary = {\n        \"train_planets_preprocessed\": n_train_npz,\n        \"test_planets_preprocessed\": n_test_npz,\n        \"sample_planet_id\": planet_id,\n        \"sample_keys\": sorted(sample.files),\n        \"airs_norm_shape\": list(sample[\"airs_norm\"].shape),\n        \"transit_depth_channels\": n_channels,\n        \"ingress\": INGRESS,\n        \"egress\": EGRESS,\n        \"bin_size\": BIN_SIZE,\n    }\n    summary_path = FIG_DIR / \"preprocessing_summary.json\"\n    with open(summary_path, \"w\") as f:\n        json.dump(preprocess_summary, f, indent=2)\n    print(f\"\\nPreprocessing summary saved to {summary_path}\")\n\n    print(f\"[Done] Validation complete for planet {planet_id}.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md-push-hub",
   "metadata": {},
   "source": [
    "## 5. Push to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-hf-login",
   "metadata": {},
   "outputs": [],
   "source": "from huggingface_hub import HfApi, login\n\nHF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\nif not HF_TOKEN:\n    raise ValueError(\n        \"HF_TOKEN is empty. Set os.environ['HF_TOKEN'] in the Setup cell \"\n        \"before running this section.\"\n    )\n\nlogin(token=HF_TOKEN, add_to_git_credential=False)\napi = HfApi()\n\nREPO_ID = \"Smooth-Cactus0/ariel-exoplanet-2024\"\n\nprint(f\"[Done] Logged in to HuggingFace Hub. Target repo: {REPO_ID}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-create-repo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset repository (no-op if it already exists)\n",
    "api.create_repo(\n",
    "    repo_id  = REPO_ID,\n",
    "    repo_type= \"dataset\",\n",
    "    exist_ok = True,\n",
    "    private  = False,\n",
    ")\n",
    "print(f\"Repository ready: https://huggingface.co/datasets/{REPO_ID}\")\n",
    "print(\"[Done] Repository created (or already exists).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-upload-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the entire preprocessed directory tree to data/preprocessed/ in the repo\n",
    "print(f\"Uploading {OUT_DIR} → {REPO_ID}:data/preprocessed/ ...\")\n",
    "print(\"(This may take several minutes depending on dataset size.)\")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path  = str(OUT_DIR),\n",
    "    repo_id      = REPO_ID,\n",
    "    repo_type    = \"dataset\",\n",
    "    path_in_repo = \"data/preprocessed\",\n",
    "    commit_message = \"Upload preprocessed .npz files (train + test)\",\n",
    ")\n",
    "\n",
    "print(\"[Done] Preprocessed data uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-upload-script",
   "metadata": {},
   "outputs": [],
   "source": "# Upload the HuggingFace Datasets loading script\nLOADING_SCRIPT = os.path.join(PROJECT_DIR, \"hf_dataset\", \"ariel_dataset.py\")\n\nif not os.path.exists(LOADING_SCRIPT):\n    print(f\"WARNING: Loading script not found at {LOADING_SCRIPT}.\")\n    print(\"Ensure the repository is cloned at REPO_DIR.\")\nelse:\n    api.upload_file(\n        path_or_fileobj= LOADING_SCRIPT,\n        path_in_repo   = \"ariel_dataset.py\",\n        repo_id        = REPO_ID,\n        repo_type      = \"dataset\",\n        commit_message = \"Add HuggingFace Datasets loading script\",\n    )\n    print(f\"Uploaded loading script: {LOADING_SCRIPT}\")\n\nprint(\"[Done] Upload complete!\")\nprint(f\"Dataset URL: https://huggingface.co/datasets/{REPO_ID}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md-verify",
   "metadata": {},
   "source": [
    "## 6. Verify Load from Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-verify-hub",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"Loading dataset from Hub: {REPO_ID}\")\n",
    "print(\"(First load will download and cache the data — may take a while.)\")\n",
    "\n",
    "ds = load_dataset(REPO_ID, split=\"train\")\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "print(ds)\n",
    "\n",
    "print(\"\\nFirst example keys:\")\n",
    "sample_hub = ds[0]\n",
    "print(list(sample_hub.keys()))\n",
    "\n",
    "print(\"\\nFirst example shapes / lengths:\")\n",
    "for key, val in sample_hub.items():\n",
    "    if hasattr(val, '__len__'):\n",
    "        if hasattr(val, 'shape'):\n",
    "            print(f\"  {key:<22}: shape={val.shape}\")\n",
    "        else:\n",
    "            print(f\"  {key:<22}: len={len(val)}\")\n",
    "    else:\n",
    "        print(f\"  {key:<22}: {val}\")\n",
    "\n",
    "print(\"[Done] Dataset verified from Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8va6ripvfr",
   "source": "## 8. Push Preprocessing Summary to GitHub\n\nPush the validation spectrum plot and preprocessing summary JSON to the repo.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jr8w7s793ds",
   "source": "import shutil\nimport subprocess\nfrom pathlib import Path\n\n# ── Repo paths ────────────────────────────────────────────────────────────\nrepo_dir    = Path(REPO_DIR)\nproject_dir = repo_dir / \"Kaggle competition\" / \"ARIEL neurIPS\"\n\n# ── Ensure repo is up-to-date ─────────────────────────────────────────────\nif not repo_dir.exists():\n    subprocess.run(\n        [\"git\", \"clone\", \"https://github.com/Smooth-Cactus0/ariel-exoplanet-ml.git\",\n         str(repo_dir)],\n        check=True,\n    )\nelse:\n    subprocess.run([\"git\", \"-C\", str(repo_dir), \"pull\", \"--ff-only\"], check=False)\n\n# ── Configure git identity (required on Kaggle kernels) ───────────────────\nsubprocess.run([\"git\", \"-C\", str(repo_dir), \"config\", \"user.email\", \"alexy.louis@kaggle-notebook.local\"], check=True)\nsubprocess.run([\"git\", \"-C\", str(repo_dir), \"config\", \"user.name\", \"Alexy Louis (Kaggle)\"], check=True)\n\n# ── Copy artifacts to repo ─────────────────────────────────────────────────\nrepo_results_dir = project_dir / \"results\"\nrepo_results_dir.mkdir(parents=True, exist_ok=True)\nrepo_fig_dir = project_dir / \"figures\"\nrepo_fig_dir.mkdir(parents=True, exist_ok=True)\n\n# Copy preprocessing_summary.json → results/\nsummary_json = FIG_DIR / \"preprocessing_summary.json\"\nif summary_json.exists():\n    shutil.copy2(summary_json, repo_results_dir / \"preprocessing_summary.json\")\n    print(f\"  preprocessing_summary.json -> results/preprocessing_summary.json\")\n\n# Copy validation figure → figures/\nval_fig = FIG_DIR / \"hf_validation_spectrum.png\"\nif val_fig.exists():\n    shutil.copy2(val_fig, repo_fig_dir / \"hf_validation_spectrum.png\")\n    print(f\"  hf_validation_spectrum.png -> figures/hf_validation_spectrum.png\")\n\n# ── Git add, commit, push ─────────────────────────────────────────────────\nsubprocess.run(\n    [\"git\", \"-C\", str(repo_dir), \"add\",\n     \"Kaggle competition/ARIEL neurIPS/results/\",\n     \"Kaggle competition/ARIEL neurIPS/figures/\"],\n    check=True,\n)\n\nstatus = subprocess.run(\n    [\"git\", \"-C\", str(repo_dir), \"diff\", \"--cached\", \"--quiet\"],\n    capture_output=True,\n)\nif status.returncode != 0:\n    subprocess.run(\n        [\"git\", \"-C\", str(repo_dir), \"commit\", \"-m\",\n         \"data: update HF preprocessing summary from Kaggle notebook run\"],\n        check=True,\n    )\n    subprocess.run(\n        [\"git\", \"-C\", str(repo_dir), \"push\", \"origin\", \"master\"],\n        check=True,\n    )\n    print(\"\\n[Done] Preprocessing summary pushed to GitHub.\")\nelse:\n    print(\"\\n[Done] No changes to push (summary already up-to-date).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "md-summary",
   "metadata": {},
   "source": "## 7. Summary\n\n### What was uploaded\n\n- **Train split**: one `.npz` file per planet under `data/preprocessed/train/`  \n  Each file contains: `airs_norm`, `fgs1_norm`, `aux`, `transit_depth`, `transit_depth_err`, `mask_oot`.  \n  Labelled planets additionally contain: `target_mean` (283 wavelength bins).  \n  Note: `target_std` is **not** included — sigma is model-predicted.\n\n- **Test split**: one `.npz` file per planet under `data/preprocessed/test/`  \n  Same structure, without label arrays.\n\n- **Loading script**: `ariel_dataset.py` — a `datasets.GeneratorBasedBuilder` enabling  \n  one-line loading via `load_dataset()`.\n\n### Data format\n\nRaw data is stored as **parquet files** (not HDF5) in a nested directory layout:\n```\n{split}/{planet_id}/\n    AIRS-CH0_signal.parquet      (n_time, 32*356) uint16\n    FGS1_signal.parquet          (n_time_fgs1, 32*32) uint16\n    AIRS-CH0_calibration/{dark,flat,dead,read,linear_corr}.parquet\n    FGS1_calibration/{dark,flat,dead,read,linear_corr}.parquet\n```\n\nAuxiliary features come from `train_adc_info.csv` (5 columns):\n`planet_id | FGS1_adc_offset | FGS1_adc_gain | AIRS-CH0_adc_offset | AIRS-CH0_adc_gain | star`\n\nLabels come from `train_labels.csv` (283 wavelength bins):\n`planet_id | wl_1 | ... | wl_283`\n\n### How to use the dataset\n\n```python\nfrom datasets import load_dataset\n\n# Load full train split\nds_train = load_dataset(\"Smooth-Cactus0/ariel-exoplanet-2024\", split=\"train\")\n\n# Load full test split\nds_test = load_dataset(\"Smooth-Cactus0/ariel-exoplanet-2024\", split=\"test\")\n\n# Access a single planet\nplanet = ds_train[0]\nprint(planet.keys())\n# dict_keys: planet_id, airs_norm, fgs1_norm, aux,\n#            transit_depth, transit_depth_err,\n#            target_mean (labelled only)\n\nimport numpy as np\nairs = np.array(planet[\"airs_norm\"])   # (time_binned, 356)\nfgs1 = np.array(planet[\"fgs1_norm\"])   # (time_binned,)\naux  = np.array(planet[\"aux\"])         # (5,)\ntd   = np.array(planet[\"transit_depth\"])  # (356,)\n```\n\n### Links\n\n- HuggingFace Dataset: https://huggingface.co/datasets/Smooth-Cactus0/ariel-exoplanet-2024\n- Kaggle Competition: https://www.kaggle.com/competitions/ariel-data-challenge-2024\n- GitHub Repository: https://github.com/Smooth-Cactus0/ariel-exoplanet-ml"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}