{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# NeurIPS 2024 Ariel Data Challenge — HuggingFace Dataset Upload\n",
    "\n",
    "**Purpose**: Preprocess every planet in the Ariel competition dataset and push  \n",
    "the results to `alexy-louis/ariel-exoplanet-2024` on the HuggingFace Hub.\n",
    "\n",
    "**Outputs**:\n",
    "- `data/preprocessed/{train,test}/{planet_id}.npz` — one compressed NumPy archive per planet\n",
    "- HuggingFace dataset repository with `ariel_dataset.py` loading script\n",
    "\n",
    "> **Note**: This notebook is Kaggle-ready and requires `ariel-data-challenge-2024` attached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install / verify required packages\n",
    "import subprocess, sys\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"h5py\",\n",
    "    \"tqdm\",\n",
    "    \"huggingface_hub\",\n",
    "    \"datasets\",\n",
    "]\n",
    "\n",
    "for pkg in REQUIRED_PACKAGES:\n",
    "    try:\n",
    "        __import__(pkg.replace(\"-\", \"_\"))\n",
    "        print(f\"{pkg}: already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"{pkg}: not found — installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
    "        print(f\"{pkg}: installed\")\n",
    "\n",
    "print(\"[Done] Package check complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Path configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Add the project source to the Python path so src.preprocessing is importable.\n",
    "# On Kaggle, clone the repo first (uncomment the subprocess call below).\n",
    "REPO_DIR = \"/kaggle/working/ariel-exoplanet-ml\"\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Uncomment to clone the repository on Kaggle:\n",
    "# subprocess.run(\n",
    "#     [\"git\", \"clone\",\n",
    "#      \"https://github.com/Smooth-Cactus0/ariel-exoplanet-ml.git\",\n",
    "#      REPO_DIR],\n",
    "#     check=True,\n",
    "# )\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HuggingFace token — paste your token here or set HF_TOKEN env variable\n",
    "# ---------------------------------------------------------------------------\n",
    "os.environ[\"HF_TOKEN\"] = \"\"  # Set your HuggingFace token here\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Directories\n",
    "# ---------------------------------------------------------------------------\n",
    "DATA_ROOT = Path(\"/kaggle/input/ariel-data-challenge-2024\")\n",
    "OUT_DIR   = Path(\"/kaggle/working/preprocessed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"train\").mkdir(exist_ok=True)\n",
    "(OUT_DIR / \"test\").mkdir(exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HDF5 key names\n",
    "# TODO: verify key names — confirmed as 'AIRS-CH0' and 'FGS1' in dataset.py\n",
    "# ---------------------------------------------------------------------------\n",
    "AIRS_KEY = \"AIRS-CH0\"  # TODO: verify key names\n",
    "FGS1_KEY = \"FGS1\"      # TODO: verify key names\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Preprocessing hyper-parameters (must match training config)\n",
    "# ---------------------------------------------------------------------------\n",
    "INGRESS  = 0.20\n",
    "EGRESS   = 0.80\n",
    "BIN_SIZE = 5\n",
    "\n",
    "print(f\"DATA_ROOT : {DATA_ROOT}  (exists={DATA_ROOT.exists()})\")\n",
    "print(f\"OUT_DIR   : {OUT_DIR}\")\n",
    "print(f\"AIRS key  : {AIRS_KEY}\")\n",
    "print(f\"FGS1 key  : {FGS1_KEY}\")\n",
    "print(f\"Ingress / Egress / BinSize : {INGRESS} / {EGRESS} / {BIN_SIZE}\")\n",
    "\n",
    "print(\"[Done] Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-preprocess-train",
   "metadata": {},
   "source": [
    "## 2. Preprocess All Training Planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-tables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load auxiliary table and quartiles table (labels)\n",
    "aux_path    = DATA_ROOT / \"AuxillaryTable.csv\"\n",
    "labels_path = DATA_ROOT / \"QuartilesTable.csv\"\n",
    "\n",
    "df_aux = pd.read_csv(aux_path, index_col=0)\n",
    "print(f\"AuxillaryTable  : {df_aux.shape[0]} planets, {df_aux.shape[1]} features\")\n",
    "\n",
    "# QuartilesTable has columns like '0_q1', '0_q2', '0_q3', '1_q1', ...\n",
    "if labels_path.exists():\n",
    "    df_labels = pd.read_csv(labels_path, index_col=0)\n",
    "    labelled_ids = set(df_labels.index.astype(str))\n",
    "    print(f\"QuartilesTable  : {len(labelled_ids)} labelled planets\")\n",
    "else:\n",
    "    df_labels    = None\n",
    "    labelled_ids = set()\n",
    "    print(\"WARNING: QuartilesTable.csv not found — no label extraction possible.\")\n",
    "\n",
    "print(f\"[Done] Tables loaded. {len(labelled_ids)} of {df_aux.shape[0]} planets are labelled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-label-helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_quartile_row(df_labels: pd.DataFrame, planet_id: str):\n",
    "    \"\"\"\n",
    "    Extract (target_mean, target_std) from QuartilesTable for a single planet.\n",
    "\n",
    "    Column naming convention: '{wavelength_idx}_q1', '{wavelength_idx}_q2', '{wavelength_idx}_q3'\n",
    "    Target mean = q2 (median)\n",
    "    Target std  = (q3 - q1) / 2  (half inter-quartile range ≈ 1-sigma)\n",
    "\n",
    "    Returns (None, None) if the planet is not in the table.\n",
    "    \"\"\"\n",
    "    if df_labels is None or planet_id not in df_labels.index.astype(str):\n",
    "        return None, None\n",
    "    try:\n",
    "        row = df_labels.loc[int(planet_id)]\n",
    "        q1 = row.filter(regex=r\"^\\d+_q1$\").values.astype(np.float32)\n",
    "        q2 = row.filter(regex=r\"^\\d+_q2$\").values.astype(np.float32)\n",
    "        q3 = row.filter(regex=r\"^\\d+_q3$\").values.astype(np.float32)\n",
    "        target_mean = q2\n",
    "        target_std  = (q3 - q1) / 2.0\n",
    "        return target_mean, target_std\n",
    "    except (KeyError, ValueError) as exc:\n",
    "        warnings.warn(f\"Could not parse labels for planet {planet_id}: {exc}\")\n",
    "        return None, None\n",
    "\n",
    "print(\"[Done] parse_quartile_row helper defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-preprocess-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the preprocessing function from src/preprocessing.py\n",
    "try:\n",
    "    from src.preprocessing import preprocess_planet\n",
    "    print(\"Imported preprocess_planet from src.preprocessing\")\n",
    "except ImportError as exc:\n",
    "    print(f\"WARNING: Could not import preprocess_planet ({exc}).\")\n",
    "    print(\"Ensure REPO_DIR is set correctly and the repo has been cloned.\")\n",
    "    raise\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Preprocess training planets\n",
    "# ---------------------------------------------------------------------------\n",
    "TRAIN_HDF5 = DATA_ROOT / \"train.hdf5\"  # TODO: verify key names inside this file\n",
    "train_out_dir = OUT_DIR / \"train\"\n",
    "\n",
    "all_train_ids = list(df_aux.index.astype(str))\n",
    "n_total   = len(all_train_ids)\n",
    "n_success = 0\n",
    "n_labelled_saved = 0\n",
    "n_failed  = 0\n",
    "total_bytes = 0\n",
    "\n",
    "print(f\"Opening {TRAIN_HDF5} ...\")\n",
    "\n",
    "try:\n",
    "    h5_train = h5py.File(TRAIN_HDF5, \"r\")\n",
    "except OSError as exc:\n",
    "    print(f\"ERROR: Cannot open {TRAIN_HDF5}: {exc}\")\n",
    "    print(\"Ensure the Kaggle dataset is attached to this notebook.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    with tqdm(total=n_total, desc=\"Train planets\", unit=\"planet\") as pbar:\n",
    "        for pid in all_train_ids:\n",
    "            out_path = train_out_dir / f\"{pid}.npz\"\n",
    "\n",
    "            # Skip if already done (allows resuming after interruption)\n",
    "            if out_path.exists():\n",
    "                n_success += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(skipped=\"(cached)\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # --- Load raw HDF5 data -----------------------------------\n",
    "                # TODO: verify key names — expected: planet group > AIRS-CH0, FGS1\n",
    "                planet_grp = h5_train[pid]\n",
    "                airs_raw = planet_grp[AIRS_KEY][()].astype(np.float32)  # (time, 356)\n",
    "                fgs1_raw = planet_grp[FGS1_KEY][()].astype(np.float32)  # (time,)\n",
    "            except KeyError as exc:\n",
    "                warnings.warn(f\"KeyError for planet {pid}: {exc} — skipping.\")\n",
    "                n_failed += 1\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            except Exception as exc:\n",
    "                warnings.warn(f\"Unexpected error loading planet {pid}: {exc} — skipping.\")\n",
    "                n_failed += 1\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            # --- Auxiliary features ---------------------------------------\n",
    "            try:\n",
    "                aux_row = df_aux.loc[int(pid)].values.astype(np.float32)  # (9,)\n",
    "            except (KeyError, ValueError):\n",
    "                aux_row = np.zeros(9, dtype=np.float32)\n",
    "                warnings.warn(f\"Auxiliary features not found for planet {pid} — using zeros.\")\n",
    "\n",
    "            # --- Preprocessing pipeline -----------------------------------\n",
    "            try:\n",
    "                result = preprocess_planet(\n",
    "                    airs_raw, fgs1_raw,\n",
    "                    ingress=INGRESS, egress=EGRESS, bin_size=BIN_SIZE,\n",
    "                )\n",
    "            except Exception as exc:\n",
    "                warnings.warn(f\"Preprocessing failed for planet {pid}: {exc} — skipping.\")\n",
    "                n_failed += 1\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            # --- Label extraction (optional) ------------------------------\n",
    "            target_mean, target_std = parse_quartile_row(df_labels, pid)\n",
    "\n",
    "            # --- Save to .npz ---------------------------------------------\n",
    "            save_kwargs = dict(\n",
    "                airs_norm        = result[\"airs_norm\"],          # (time_binned, 356)\n",
    "                fgs1_norm        = result[\"fgs1_norm\"],          # (time_binned,)\n",
    "                aux              = aux_row,                       # (9,)\n",
    "                transit_depth    = result[\"transit_depth\"],      # (356,)\n",
    "                transit_depth_err= result[\"transit_depth_err\"],  # (356,)\n",
    "                mask_oot         = result[\"mask_oot\"],           # (time_binned,)\n",
    "            )\n",
    "            if target_mean is not None:\n",
    "                save_kwargs[\"target_mean\"] = target_mean  # (283,)\n",
    "                save_kwargs[\"target_std\"]  = target_std   # (283,)\n",
    "                n_labelled_saved += 1\n",
    "\n",
    "            np.savez_compressed(str(out_path), **save_kwargs)\n",
    "            total_bytes += out_path.stat().st_size\n",
    "            n_success += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(\n",
    "                success=n_success, labelled=n_labelled_saved, failed=n_failed\n",
    "            )\n",
    "\n",
    "finally:\n",
    "    h5_train.close()\n",
    "\n",
    "# Summary\n",
    "avg_kb = (total_bytes / max(n_success, 1)) / 1024\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Train preprocessing summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total planets     : {n_total}\")\n",
    "print(f\"  Successfully saved: {n_success}\")\n",
    "print(f\"  Labelled planets  : {n_labelled_saved}\")\n",
    "print(f\"  Failed / skipped  : {n_failed}\")\n",
    "print(f\"  Total size on disk: {total_bytes / 1_048_576:.1f} MB\")\n",
    "print(f\"  Avg file size     : {avg_kb:.1f} KB\")\n",
    "print(\"[Done] Train preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-preprocess-test",
   "metadata": {},
   "source": [
    "## 3. Preprocess All Test Planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-preprocess-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test auxiliary table (planet IDs to iterate over)\n",
    "test_aux_path = DATA_ROOT / \"AuxillaryTable_test.csv\"\n",
    "if test_aux_path.exists():\n",
    "    df_aux_test = pd.read_csv(test_aux_path, index_col=0)\n",
    "else:\n",
    "    # Fallback: some competitions use a single AuxillaryTable for both splits\n",
    "    print(\"WARNING: AuxillaryTable_test.csv not found — attempting to use train table.\")\n",
    "    df_aux_test = df_aux.copy()\n",
    "\n",
    "TEST_HDF5 = DATA_ROOT / \"test.hdf5\"  # TODO: verify key names inside this file\n",
    "test_out_dir = OUT_DIR / \"test\"\n",
    "\n",
    "all_test_ids   = list(df_aux_test.index.astype(str))\n",
    "n_total_test   = len(all_test_ids)\n",
    "n_success_test = 0\n",
    "n_failed_test  = 0\n",
    "total_bytes_test = 0\n",
    "\n",
    "print(f\"Test planets to process : {n_total_test}\")\n",
    "print(f\"Opening {TEST_HDF5} ...\")\n",
    "\n",
    "try:\n",
    "    h5_test = h5py.File(TEST_HDF5, \"r\")\n",
    "except OSError as exc:\n",
    "    print(f\"ERROR: Cannot open {TEST_HDF5}: {exc}\")\n",
    "    print(\"Ensure the Kaggle dataset is attached. Skipping test split.\")\n",
    "    h5_test = None\n",
    "\n",
    "if h5_test is not None:\n",
    "    try:\n",
    "        with tqdm(total=n_total_test, desc=\"Test planets\", unit=\"planet\") as pbar:\n",
    "            for pid in all_test_ids:\n",
    "                out_path = test_out_dir / f\"{pid}.npz\"\n",
    "\n",
    "                if out_path.exists():\n",
    "                    n_success_test += 1\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix(skipped=\"(cached)\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # TODO: verify key names — expected: planet group > AIRS-CH0, FGS1\n",
    "                    planet_grp = h5_test[pid]\n",
    "                    airs_raw = planet_grp[AIRS_KEY][()].astype(np.float32)\n",
    "                    fgs1_raw = planet_grp[FGS1_KEY][()].astype(np.float32)\n",
    "                except KeyError as exc:\n",
    "                    warnings.warn(f\"KeyError for test planet {pid}: {exc} — skipping.\")\n",
    "                    n_failed_test += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                except Exception as exc:\n",
    "                    warnings.warn(f\"Error loading test planet {pid}: {exc} — skipping.\")\n",
    "                    n_failed_test += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    aux_row = df_aux_test.loc[int(pid)].values.astype(np.float32)\n",
    "                except (KeyError, ValueError):\n",
    "                    aux_row = np.zeros(9, dtype=np.float32)\n",
    "                    warnings.warn(f\"Aux features not found for test planet {pid}.\")\n",
    "\n",
    "                try:\n",
    "                    result = preprocess_planet(\n",
    "                        airs_raw, fgs1_raw,\n",
    "                        ingress=INGRESS, egress=EGRESS, bin_size=BIN_SIZE,\n",
    "                    )\n",
    "                except Exception as exc:\n",
    "                    warnings.warn(f\"Preprocessing failed for test planet {pid}: {exc}.\")\n",
    "                    n_failed_test += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                np.savez_compressed(\n",
    "                    str(out_path),\n",
    "                    airs_norm        = result[\"airs_norm\"],\n",
    "                    fgs1_norm        = result[\"fgs1_norm\"],\n",
    "                    aux              = aux_row,\n",
    "                    transit_depth    = result[\"transit_depth\"],\n",
    "                    transit_depth_err= result[\"transit_depth_err\"],\n",
    "                    mask_oot         = result[\"mask_oot\"],\n",
    "                )\n",
    "                total_bytes_test += out_path.stat().st_size\n",
    "                n_success_test += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(success=n_success_test, failed=n_failed_test)\n",
    "    finally:\n",
    "        h5_test.close()\n",
    "\n",
    "avg_kb_test = (total_bytes_test / max(n_success_test, 1)) / 1024\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test preprocessing summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total planets     : {n_total_test}\")\n",
    "print(f\"  Successfully saved: {n_success_test}\")\n",
    "print(f\"  Failed / skipped  : {n_failed_test}\")\n",
    "print(f\"  Total size on disk: {total_bytes_test / 1_048_576:.1f} MB\")\n",
    "print(f\"  Avg file size     : {avg_kb_test:.1f} KB\")\n",
    "print(\"[Done] Test preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-validate",
   "metadata": {},
   "source": [
    "## 4. Validate One Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-validate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load back one .npz from the train split to sanity-check contents and shapes\n",
    "train_npz_files = sorted((OUT_DIR / \"train\").glob(\"*.npz\"))\n",
    "\n",
    "if not train_npz_files:\n",
    "    print(\"WARNING: No .npz files found in train output dir — cannot validate.\")\n",
    "else:\n",
    "    sample_path = train_npz_files[0]\n",
    "    sample = np.load(sample_path, allow_pickle=False)\n",
    "\n",
    "    planet_id = sample_path.stem\n",
    "    print(f\"Sample planet ID  : {planet_id}\")\n",
    "    print(f\"NPZ file path     : {sample_path}\")\n",
    "    print(f\"File size         : {sample_path.stat().st_size / 1024:.1f} KB\")\n",
    "    print()\n",
    "    print(f\"{'Key':<22} {'Shape':<25} {'dtype':<10} {'min':>10}  {'max':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for key in sorted(sample.files):\n",
    "        arr = sample[key]\n",
    "        vmin = float(arr.min()) if arr.size > 0 else float('nan')\n",
    "        vmax = float(arr.max()) if arr.size > 0 else float('nan')\n",
    "        print(f\"  {key:<20} {str(arr.shape):<25} {str(arr.dtype):<10} {vmin:>10.4f}  {vmax:>10.4f}\")\n",
    "\n",
    "    # --- Plot transit depth spectrum ----------------------------------------\n",
    "    transit_depth = sample[\"transit_depth\"]\n",
    "    transit_depth_err = sample[\"transit_depth_err\"]\n",
    "    n_channels = len(transit_depth)\n",
    "    wl_idx = np.arange(n_channels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(wl_idx, transit_depth, lw=0.9, color=\"steelblue\", label=\"Transit depth\")\n",
    "    ax.fill_between(\n",
    "        wl_idx,\n",
    "        transit_depth - transit_depth_err,\n",
    "        transit_depth + transit_depth_err,\n",
    "        alpha=0.25, color=\"steelblue\", label=\"±1σ\",\n",
    "    )\n",
    "\n",
    "    # Overlay target_mean if available\n",
    "    if \"target_mean\" in sample.files and len(sample[\"target_mean\"]) == 283:\n",
    "        target_mean = sample[\"target_mean\"]\n",
    "        target_std  = sample[\"target_std\"]\n",
    "        target_wl   = np.linspace(0, n_channels - 1, 283)\n",
    "        ax.plot(target_wl, target_mean, lw=1.2, color=\"darkorange\",\n",
    "                linestyle=\"--\", label=\"Target mean (q2, 283 channels)\")\n",
    "        ax.fill_between(\n",
    "            target_wl,\n",
    "            target_mean - target_std,\n",
    "            target_mean + target_std,\n",
    "            alpha=0.20, color=\"darkorange\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"AIRS-CH0 channel index\")\n",
    "    ax.set_ylabel(\"Transit depth (fractional)\")\n",
    "    ax.set_title(f\"Planet {planet_id} — extracted transit depth spectrum ({n_channels} channels)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"[Done] Validation complete for planet {planet_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-push-hub",
   "metadata": {},
   "source": [
    "## 5. Push to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-hf-login",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\n",
    "        \"HF_TOKEN is empty. Set os.environ['HF_TOKEN'] in the Setup cell \"\n",
    "        \"before running this section.\"\n",
    "    )\n",
    "\n",
    "login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "api = HfApi()\n",
    "\n",
    "REPO_ID = \"alexy-louis/ariel-exoplanet-2024\"\n",
    "\n",
    "print(f\"[Done] Logged in to HuggingFace Hub as alexy-louis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-create-repo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset repository (no-op if it already exists)\n",
    "api.create_repo(\n",
    "    repo_id  = REPO_ID,\n",
    "    repo_type= \"dataset\",\n",
    "    exist_ok = True,\n",
    "    private  = False,\n",
    ")\n",
    "print(f\"Repository ready: https://huggingface.co/datasets/{REPO_ID}\")\n",
    "print(\"[Done] Repository created (or already exists).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-upload-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the entire preprocessed directory tree to data/preprocessed/ in the repo\n",
    "print(f\"Uploading {OUT_DIR} → {REPO_ID}:data/preprocessed/ ...\")\n",
    "print(\"(This may take several minutes depending on dataset size.)\")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path  = str(OUT_DIR),\n",
    "    repo_id      = REPO_ID,\n",
    "    repo_type    = \"dataset\",\n",
    "    path_in_repo = \"data/preprocessed\",\n",
    "    commit_message = \"Upload preprocessed .npz files (train + test)\",\n",
    ")\n",
    "\n",
    "print(\"[Done] Preprocessed data uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-upload-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the HuggingFace Datasets loading script\n",
    "LOADING_SCRIPT = os.path.join(REPO_DIR, \"hf_dataset\", \"ariel_dataset.py\")\n",
    "\n",
    "if not os.path.exists(LOADING_SCRIPT):\n",
    "    print(f\"WARNING: Loading script not found at {LOADING_SCRIPT}.\")\n",
    "    print(\"Ensure the repository is cloned at REPO_DIR.\")\n",
    "else:\n",
    "    api.upload_file(\n",
    "        path_or_fileobj= LOADING_SCRIPT,\n",
    "        path_in_repo   = \"ariel_dataset.py\",\n",
    "        repo_id        = REPO_ID,\n",
    "        repo_type      = \"dataset\",\n",
    "        commit_message = \"Add HuggingFace Datasets loading script\",\n",
    "    )\n",
    "    print(f\"Uploaded loading script: {LOADING_SCRIPT}\")\n",
    "\n",
    "print(\"[Done] Upload complete!\")\n",
    "print(f\"Dataset URL: https://huggingface.co/datasets/{REPO_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-verify",
   "metadata": {},
   "source": [
    "## 6. Verify Load from Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-verify-hub",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"Loading dataset from Hub: {REPO_ID}\")\n",
    "print(\"(First load will download and cache the data — may take a while.)\")\n",
    "\n",
    "ds = load_dataset(REPO_ID, split=\"train\")\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "print(ds)\n",
    "\n",
    "print(\"\\nFirst example keys:\")\n",
    "sample_hub = ds[0]\n",
    "print(list(sample_hub.keys()))\n",
    "\n",
    "print(\"\\nFirst example shapes / lengths:\")\n",
    "for key, val in sample_hub.items():\n",
    "    if hasattr(val, '__len__'):\n",
    "        if hasattr(val, 'shape'):\n",
    "            print(f\"  {key:<22}: shape={val.shape}\")\n",
    "        else:\n",
    "            print(f\"  {key:<22}: len={len(val)}\")\n",
    "    else:\n",
    "        print(f\"  {key:<22}: {val}\")\n",
    "\n",
    "print(\"[Done] Dataset verified from Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-summary",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### What was uploaded\n",
    "\n",
    "- **Train split**: one `.npz` file per planet under `data/preprocessed/train/`  \n",
    "  Each file contains: `airs_norm`, `fgs1_norm`, `aux`, `transit_depth`, `transit_depth_err`, `mask_oot`.  \n",
    "  Labelled planets additionally contain: `target_mean`, `target_std`.\n",
    "\n",
    "- **Test split**: one `.npz` file per planet under `data/preprocessed/test/`  \n",
    "  Same structure, without label arrays.\n",
    "\n",
    "- **Loading script**: `ariel_dataset.py` — a `datasets.GeneratorBasedBuilder` enabling  \n",
    "  one-line loading via `load_dataset()`.\n",
    "\n",
    "### How to use the dataset\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load full train split\n",
    "ds_train = load_dataset(\"alexy-louis/ariel-exoplanet-2024\", split=\"train\")\n",
    "\n",
    "# Load full test split\n",
    "ds_test = load_dataset(\"alexy-louis/ariel-exoplanet-2024\", split=\"test\")\n",
    "\n",
    "# Access a single planet\n",
    "planet = ds_train[0]\n",
    "print(planet.keys())\n",
    "# dict_keys: planet_id, airs_norm, fgs1_norm, aux,\n",
    "#            transit_depth, transit_depth_err,\n",
    "#            target_mean (labelled only), target_std (labelled only)\n",
    "\n",
    "import numpy as np\n",
    "airs = np.array(planet[\"airs_norm\"])   # (time_binned, 356)\n",
    "fgs1 = np.array(planet[\"fgs1_norm\"])   # (time_binned,)\n",
    "td   = np.array(planet[\"transit_depth\"])  # (356,)\n",
    "```\n",
    "\n",
    "### Links\n",
    "\n",
    "- HuggingFace Dataset: https://huggingface.co/datasets/alexy-louis/ariel-exoplanet-2024\n",
    "- Kaggle Competition: https://www.kaggle.com/competitions/ariel-data-challenge-2024\n",
    "- GitHub Repository: https://github.com/Smooth-Cactus0/ariel-exoplanet-ml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
